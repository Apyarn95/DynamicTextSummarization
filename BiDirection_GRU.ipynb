{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiDirection_GRU.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4BjqOk5fyvC"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from string import digits\n",
        "import re,os,time,io\n",
        "import tensorflow as tf\n",
        "from keras.layers import Add , Concatenate \n",
        "from numpy import asarray , zeros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2hlG3xuf_Gx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80dcf7ae-bd3b-4057-e965-39fadc5146d2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yib4_1vkU7Ef"
      },
      "source": [
        "#defining contraction mapping to change all the occurences of short form to their original meaning \n",
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP0FnzlxU840"
      },
      "source": [
        "#defining preprocessing stuff\n",
        "def preprocess(w):\n",
        "  w = w.lower().strip()\n",
        "  w = ' '.join(contraction_mapping[t] if t in contraction_mapping else t for t in w.split(\" \"))\n",
        "  \n",
        "  #removing everything within brackets\n",
        "  w = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\", w)\n",
        "  #creating space between word and punctuation following\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "  #replace everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "  w = ' '.join(t for t in w.split(\" \") if t != \" \")\n",
        "  w = w.strip()\n",
        "  \n",
        "  #inclusind <sostoken> and <eostoken> to identify starting and ending of the sentences\n",
        "  w = '<sostoken> ' + w + ' <eostoken>'\n",
        "  return w\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdyBf7sTgLdd"
      },
      "source": [
        "#start from here\n",
        "data = pd.read_csv('/content/gdrive/My Drive/proc_reviews.csv',nrows = 20000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0HpgxLIgQLs"
      },
      "source": [
        "#removing null values or duplicates dataset\n",
        "data = data[~pd.isnull(data['text'])]\n",
        "data.drop_duplicates(inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgJedVP_gQwq"
      },
      "source": [
        "#converting pandas core series datastructrure to list for further calculations as tensors in tensorflow\n",
        "text_sentences = data['text'].tolist()\n",
        "summary_sentences = data['summary'].tolist() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVGq4Lx9gSWz"
      },
      "source": [
        "#tokenizing the sentences (changing each sentence to a vector of numbers describing the sentence)\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aKGIxpwgWb6"
      },
      "source": [
        "input_tensor , inp_lang_tokenizer = tokenize(text_sentences)\n",
        "target_tensor , targ_lang_tokenize = tokenize(summary_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBJRAoCbgYEw"
      },
      "source": [
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZyFvUYAgYw3"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_test, target_tensor_train, target_tensor_test = train_test_split(input_tensor, target_tensor, test_size=0.015)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoZj4YF8gamS"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor_train, target_tensor_train, test_size=0.038)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR3yyzDOgc-v"
      },
      "source": [
        "#defining the model runtime attributes\n",
        "#units -> number of gated GRU cells\n",
        "#vocab_inp_size is the total size of the english vocab (number of unique words in the corpus) \n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "validation_steps = len(input_tensor_val)//BATCH_SIZE\n",
        "embedding_dim = 300\n",
        "units = 512\n",
        "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(targ_lang_tokenize.word_index)+1\n",
        "\n",
        "Validation_buffer= len(input_tensor_val)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "hold_out_set = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).shuffle(Validation_buffer)\n",
        "hold_out_set = hold_out_set.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmZ8ObcE7aIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a015e1c-1dbe-45b9-f0df-9995ce4bbaf0"
      },
      "source": [
        "vocab_tar_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7817"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPvmGUutgfFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4102fdf2-83ec-4a28-814f-bfdb2b2df49e"
      },
      "source": [
        "#example_input_batch and example_target batch are the batch iterators that would feed data = batch size from the training to the corpus\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 150]), TensorShape([64, 30]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTMwFbqvgzAn"
      },
      "source": [
        "#defining the encoder class\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.gru_rev = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state = True,\n",
        "                                       recurrent_initializer='glorot_uniform',go_backwards = True)\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    output_rev , state_rev = self.gru_rev(x,initial_state = hidden)\n",
        "    # print(\"gru shape :{}\".format(self.gru))\n",
        "    # print(\"forward output_shape : {} , hidden_state : {}\".format(output.shape,state.shape))\n",
        "    # print(\"backward output_shape : {} , hidden_state : {}\".format(output_rev.shape,state_rev.shape))\n",
        "    Encoder_state = Concatenate(axis=-1,name='concat-layer')([state,state_rev])\n",
        "    Encoder_output = Concatenate(axis=-1,name='output')([output,output_rev])\n",
        "    # print(\"encoder state output_shape : {} , hidden_state : {}\".format(Encoder_output.shape,Encoder_state.shape))\n",
        "    \n",
        "    \n",
        "    \n",
        "    return Encoder_output, Encoder_state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbjmqOyH-5KB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5abaa43c-8e3e-4261-99e4-1b729091262b"
      },
      "source": [
        "from keras.layers import Add , Concatenate\n",
        "ar1 = tf.convert_to_tensor([2,3])\n",
        "ar2 = tf.convert_to_tensor([5,6])\n",
        "ar3 = Concatenate(axis=-1, name='concat_layer')([ar1,ar2])\n",
        "print(ar3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([2 3 5 6], shape=(4,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qwz5oJz_g3Yg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df467f1-8c50-4bd0-fdeb-3fbdd6fe1dc0"
      },
      "source": [
        "#definine the encoder layer with the above defined params\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden= encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 150, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj_Jk9jMg5WU"
      },
      "source": [
        "#for attention we would use Bahdanau attention layer for more information read readme.md file\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWIVltOZg67v"
      },
      "source": [
        "#decoder definition\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIRph6v8g88Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "644e3e0d-9138-4840-fa59-619f9c9036cc"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, 2*units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 7817)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjmjselqg-qq"
      },
      "source": [
        "#defining optimizers and loss function\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peBJwCNehAZb"
      },
      "source": [
        "\n",
        "checkpoint_dir = \"/content/gdrive/My Drive/Bi_GRU_test.ckpt\"\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kHkuiGwhaB9"
      },
      "source": [
        "#training the model\n",
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang_tokenize.word_index['<sostoken>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATmgH4sd7PM1"
      },
      "source": [
        "@tf.function\n",
        "def validation_loss(inp,targ,enc_hidden):\n",
        "  print(\"hello\")\n",
        "  loss = 0\n",
        "  enc_output , enc_hidden = encoder(inp,enc_hidden)\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input =  tf.expand_dims([targ_lang_tokenize.word_index['<sostoken>']] * BATCH_SIZE ,1)\n",
        "\n",
        "  for t in range(1,targ.shape[1]):\n",
        "    #passing encoder output to the decoder\n",
        "    predictions , dec_hidden ,_ = decoder(dec_input,dec_hidden,enc_output)\n",
        "    loss += loss_function(targ[:,t],predictions)\n",
        "    predicted_id = tf.argmax(predictions , axis=1)\n",
        "    dec_input = tf.expand_dims(predicted_id,1)\n",
        "\n",
        "  batch_loss = loss/int(targ.shape[1])\n",
        "  return batch_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z67iymphhS1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41bdd596-49c4-443d-d8a0-3833c63c1348"
      },
      "source": [
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  val_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  for (batch,(inp,targ)) in enumerate(hold_out_set.take(validation_steps)):\n",
        "    batch_loss = validation_loss(inp,targ,enc_hidden)\n",
        "    val_loss += batch_loss  # saving (checkpoint) the model every 2 epochs\n",
        "  checkpoint.save(file_prefix = checkpoint_dir)\n",
        "\n",
        "  print('Epoch {} Training_Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Epoch {} Validation_Loss {:.4f}'.format(epoch + 1,\n",
        "                                      val_loss / validation_steps))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.9444\n",
            "Epoch 1 Batch 100 Loss 0.9363\n",
            "Epoch 1 Batch 200 Loss 0.9433\n",
            "hello\n",
            "Epoch 1 Training_Loss 0.9217\n",
            "Epoch 1 Validation_Loss 1.2303\n",
            "Time taken for 1 epoch 252.3096363544464 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.8631\n",
            "Epoch 2 Batch 100 Loss 0.8173\n",
            "Epoch 2 Batch 200 Loss 0.9396\n",
            "Epoch 2 Training_Loss 0.8443\n",
            "Epoch 2 Validation_Loss 1.2604\n",
            "Time taken for 1 epoch 243.87487411499023 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.7299\n",
            "Epoch 3 Batch 100 Loss 0.8111\n",
            "Epoch 3 Batch 200 Loss 0.6616\n",
            "Epoch 3 Training_Loss 0.7776\n",
            "Epoch 3 Validation_Loss 1.3438\n",
            "Time taken for 1 epoch 244.0375165939331 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.6751\n",
            "Epoch 4 Batch 100 Loss 0.6962\n",
            "Epoch 4 Batch 200 Loss 0.7098\n",
            "Epoch 4 Training_Loss 0.7148\n",
            "Epoch 4 Validation_Loss 1.4390\n",
            "Time taken for 1 epoch 243.9841206073761 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.6516\n",
            "Epoch 5 Batch 100 Loss 0.7409\n",
            "Epoch 5 Batch 200 Loss 0.7277\n",
            "Epoch 5 Training_Loss 0.6501\n",
            "Epoch 5 Validation_Loss 1.4698\n",
            "Time taken for 1 epoch 244.52276515960693 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.5310\n",
            "Epoch 6 Batch 100 Loss 0.5224\n",
            "Epoch 6 Batch 200 Loss 0.5490\n",
            "Epoch 6 Training_Loss 0.5812\n",
            "Epoch 6 Validation_Loss 1.5689\n",
            "Time taken for 1 epoch 244.31735682487488 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.6421\n",
            "Epoch 7 Batch 100 Loss 0.5265\n",
            "Epoch 7 Batch 200 Loss 0.4447\n",
            "Epoch 7 Training_Loss 0.5091\n",
            "Epoch 7 Validation_Loss 1.6853\n",
            "Time taken for 1 epoch 244.44489312171936 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.4004\n",
            "Epoch 8 Batch 100 Loss 0.4089\n",
            "Epoch 8 Batch 200 Loss 0.6000\n",
            "Epoch 8 Training_Loss 0.4348\n",
            "Epoch 8 Validation_Loss 1.8341\n",
            "Time taken for 1 epoch 243.93545198440552 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.3343\n",
            "Epoch 9 Batch 100 Loss 0.3672\n",
            "Epoch 9 Batch 200 Loss 0.3747\n",
            "Epoch 9 Training_Loss 0.3599\n",
            "Epoch 9 Validation_Loss 1.8788\n",
            "Time taken for 1 epoch 244.26011538505554 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.2569\n",
            "Epoch 10 Batch 100 Loss 0.3194\n",
            "Epoch 10 Batch 200 Loss 0.3242\n",
            "Epoch 10 Training_Loss 0.2895\n",
            "Epoch 10 Validation_Loss 2.0156\n",
            "Time taken for 1 epoch 243.75213980674744 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.1936\n",
            "Epoch 11 Batch 100 Loss 0.2467\n",
            "Epoch 11 Batch 200 Loss 0.2388\n",
            "Epoch 11 Training_Loss 0.2262\n",
            "Epoch 11 Validation_Loss 2.1561\n",
            "Time taken for 1 epoch 244.1464512348175 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.1409\n",
            "Epoch 12 Batch 100 Loss 0.1769\n",
            "Epoch 12 Batch 200 Loss 0.1922\n",
            "Epoch 12 Training_Loss 0.1732\n",
            "Epoch 12 Validation_Loss 2.2608\n",
            "Time taken for 1 epoch 243.794371843338 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.1178\n",
            "Epoch 13 Batch 100 Loss 0.1076\n",
            "Epoch 13 Batch 200 Loss 0.1289\n",
            "Epoch 13 Training_Loss 0.1304\n",
            "Epoch 13 Validation_Loss 2.4003\n",
            "Time taken for 1 epoch 244.71182656288147 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.1016\n",
            "Epoch 14 Batch 100 Loss 0.0803\n",
            "Epoch 14 Batch 200 Loss 0.0886\n",
            "Epoch 14 Training_Loss 0.0992\n",
            "Epoch 14 Validation_Loss 2.5109\n",
            "Time taken for 1 epoch 244.01922225952148 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0815\n",
            "Epoch 15 Batch 100 Loss 0.0841\n",
            "Epoch 15 Batch 200 Loss 0.0824\n",
            "Epoch 15 Training_Loss 0.0739\n",
            "Epoch 15 Validation_Loss 2.5358\n",
            "Time taken for 1 epoch 243.95547723770142 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.0679\n",
            "Epoch 16 Batch 100 Loss 0.0522\n",
            "Epoch 16 Batch 200 Loss 0.0704\n",
            "Epoch 16 Training_Loss 0.0552\n",
            "Epoch 16 Validation_Loss 2.6792\n",
            "Time taken for 1 epoch 243.4604630470276 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0409\n",
            "Epoch 17 Batch 100 Loss 0.0513\n",
            "Epoch 17 Batch 200 Loss 0.0389\n",
            "Epoch 17 Training_Loss 0.0421\n",
            "Epoch 17 Validation_Loss 2.7374\n",
            "Time taken for 1 epoch 243.52671551704407 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0307\n",
            "Epoch 18 Batch 100 Loss 0.0310\n",
            "Epoch 18 Batch 200 Loss 0.0381\n",
            "Epoch 18 Training_Loss 0.0342\n",
            "Epoch 18 Validation_Loss 2.8499\n",
            "Time taken for 1 epoch 243.31870460510254 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0218\n",
            "Epoch 19 Batch 100 Loss 0.0238\n",
            "Epoch 19 Batch 200 Loss 0.0301\n",
            "Epoch 19 Training_Loss 0.0289\n",
            "Epoch 19 Validation_Loss 2.9468\n",
            "Time taken for 1 epoch 243.53481698036194 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0198\n",
            "Epoch 20 Batch 100 Loss 0.0290\n",
            "Epoch 20 Batch 200 Loss 0.0293\n",
            "Epoch 20 Training_Loss 0.0250\n",
            "Epoch 20 Validation_Loss 2.8859\n",
            "Time taken for 1 epoch 243.6574568748474 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuGiQWguNX98"
      },
      "source": [
        "#cconvert the english sentence to hindi using the model\n",
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess(sentence)\n",
        "\n",
        "  inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang_tokenize.word_index['<sostoken>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang_tokenize.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang_tokenize.index_word[predicted_id] == '<eostoken>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY680WSQNcSx"
      },
      "source": [
        "def translate(sentence):\n",
        "  print(\"this is the result\")\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "  print('Predicted translation: {}'.format(result))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEEnxmanUlkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1843a10a-05cb-4bef-ccd5-b59d6e674aad"
      },
      "source": [
        "print(data['summary'][10])\n",
        "print(translate(data['text'][10][11:-11]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sostoken> classic movie <eostoken>\n",
            "this is the result\n",
            "Predicted translation: love this movie <eostoken> \n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOimyl9uNN4X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}