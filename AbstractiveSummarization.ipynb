{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LanguageTranslation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlAogaNvim7X",
        "outputId": "c7965de4-82e6-4e6b-fc4c-3555cdbaf48e"
      },
      "source": [
        "!pip install -q tfds-nightly\n",
        "!pip install matplotlib==3.2.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.7MB 13.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib==3.2.2 in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.2.2) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.2.2) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.2.2) (1.19.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.2.2) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.2.2) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib==3.2.2) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqgI9Vl-jZxh"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVk_x5L65LwT",
        "outputId": "42ffc195-48b1-416f-f0ea-fb4295abcae2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxLEFYtK9ipX"
      },
      "source": [
        "#start from here\n",
        "data = pd.read_csv('/content/gdrive/My Drive/proc_reviews.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "8BA23i1oYlAJ",
        "outputId": "63fff223-a722-4db3-c876-ff95ca36a580"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "      <th>text_len</th>\n",
              "      <th>summary_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>&lt;sostoken&gt; this was made in canada but has som...</td>\n",
              "      <td>&lt;sostoken&gt; love this movie &lt;eostoken&gt;</td>\n",
              "      <td>47</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>&lt;sostoken&gt; i love this movie , i used to watch...</td>\n",
              "      <td>&lt;sostoken&gt; great ! &lt;eostoken&gt;</td>\n",
              "      <td>70</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>&lt;sostoken&gt; i was looking forward to this movie...</td>\n",
              "      <td>&lt;sostoken&gt; love it &lt;eostoken&gt;</td>\n",
              "      <td>29</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>&lt;sostoken&gt; a take off of scrooge , but with a ...</td>\n",
              "      <td>&lt;sostoken&gt; christmas classic &lt;eostoken&gt;</td>\n",
              "      <td>29</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>&lt;sostoken&gt; i agree with others who say this is...</td>\n",
              "      <td>&lt;sostoken&gt; best christmas movie . . . except t...</td>\n",
              "      <td>142</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... summary_len\n",
              "0           0  ...           5\n",
              "1           4  ...           4\n",
              "2           5  ...           4\n",
              "3           6  ...           4\n",
              "4           7  ...          12\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkLLVO1mChuL",
        "outputId": "53f0454f-7975-48a3-e8bd-c3154a7d9e56"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1059562"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwcncGBJOfMU"
      },
      "source": [
        "#defining contraction mapping to change all the occurences of short form to their original meaning \r\n",
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\r\n",
        "\r\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\r\n",
        "\r\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\r\n",
        "\r\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\r\n",
        "\r\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\r\n",
        "\r\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\r\n",
        "\r\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\r\n",
        "\r\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\r\n",
        "\r\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\r\n",
        "\r\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\r\n",
        "\r\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\r\n",
        "\r\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\r\n",
        "\r\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\r\n",
        "\r\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\r\n",
        "\r\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\r\n",
        "\r\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\r\n",
        "\r\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\r\n",
        "\r\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\r\n",
        "\r\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\r\n",
        "\r\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\r\n",
        "\r\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\r\n",
        "\r\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\r\n",
        "\r\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O2K83xCOfzY"
      },
      "source": [
        "#defining preprocessing stuff\r\n",
        "def preprocess(w):\r\n",
        "  w = w.lower().strip()\r\n",
        "  w = ' '.join(contraction_mapping[t] if t in contraction_mapping else t for t in w.split(\" \"))\r\n",
        "  \r\n",
        "  #removing everything within brackets\r\n",
        "  w = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\", w)\r\n",
        "  #creating space between word and punctuation following\r\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\r\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\r\n",
        "  #replace everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\r\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\r\n",
        "  w = ' '.join(t for t in w.split(\" \") if t != \" \")\r\n",
        "  w = w.strip()\r\n",
        "  \r\n",
        "  #inclusind <sostoken> and <eostoken> to identify starting and ending of the sentences\r\n",
        "  return w\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhnjCC0-V9Nr"
      },
      "source": [
        "data['text'] = data['text'].apply(lambda x : tf.convert_to_tensor(preprocess(\" \".join(x.split(\" \")[1:-1]))))\r\n",
        "data['summary'] = data['summary'].apply(lambda x : tf.convert_to_tensor(preprocess(\" \".join(x.split(\" \")[1:-1]))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYsdStzQWC-K",
        "outputId": "3887a084-3b8c-43d9-dc86-ee65342a1332"
      },
      "source": [
        "for i in range(10):\n",
        "  print(data['text'][i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'this was made in canada but has some well know actors in it . i really like this really different version on a kind of scrouge who goes through a lot to find out the magic of christmas is all about our attitude about it', shape=(), dtype=string)\n",
            "tf.Tensor(b'i love this movie , i used to watch it on tv all the time when it was on disney . the story is about a poor family in canada and the mother does not have any christmas spirit and is sent one day ahead and realizes that she has more than what money can buy . this is a great christmas movie , i recommend it .', shape=(), dtype=string)\n",
            "tf.Tensor(b'i was looking forward to this movie , and got it super fast ! ! ! love the expedient service , and look forward to ordering again', shape=(), dtype=string)\n",
            "tf.Tensor(b'a take off of scrooge , but with a twist . sad but wonderful . ranks with white christmas and wonderful life according to my wife !', shape=(), dtype=string)\n",
            "tf.Tensor(b'i agree with others who say this is their favorite christmas movie . it is also mine . unfortunately , disney has never seemed to pay much attention to it . it took quite a while for it even to be released in a widescreen version . this movie needs to add the usual extra features most movies have today . as with other movies today , i would like to see it brought to blu ray . it seems the fans of this movie think more of it than disney does . i hope that changes . for those who have not seen it , marking this , as so many others have , as my favorite christmas movie stands as its own recommendation . get with it , disney . give this film the care it deserves .', shape=(), dtype=string)\n",
            "tf.Tensor(b'one magic christmas is an excellent christmas classic . no home should be without it .', shape=(), dtype=string)\n",
            "tf.Tensor(b'i watch this movie every christmas . this movie has a quite a magical feel to it . mary has a way of being in the best movies . really nice message to this one . harry is really good in this one . look at those coke bottles . this movie takes me back when life was quite different . i wish christmas was like this . i love this movie .', shape=(), dtype=string)\n",
            "tf.Tensor(b'when a woman falls on seriously hard times that destroy her sense of hope for the future , an angel is sent to renew her faith her belief in things hoped for but yet unseen . despite the christmas setting , this dvd is relavent any time you need a lift to your spirits . i am sure you will watch it over and over again . one magic christmas has the emotional impact of it is a wonderful life , but the story stands completely on it is own for originality and drama . excellent !', shape=(), dtype=string)\n",
            "tf.Tensor(b'if you have any empathy or compassion or imagination , this movie will make you cry , as well as laugh , and you may just find yourself feeling a little bit of the magic long after it is over . it is one of my favorites . we watch it every christmas !', shape=(), dtype=string)\n",
            "tf.Tensor(b'this is a great movie about finding holiday spirit . i wish it had more exposure . it was a favorite growing up . it was delivered very quickly and is a great value . i wish it was available in blu ray .', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6atU15gDWUHA"
      },
      "source": [
        "source_sent = tf.convert_to_tensor(data['text'].tolist())\n",
        "summary_sent = tf.convert_to_tensor(data['summary'].tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-BD7FWykp1g",
        "outputId": "c0f5bead-c193-4c1d-bc8f-e9c2f71b299d"
      },
      "source": [
        "source_sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(20000,), dtype=string, numpy=\n",
              "array([b'this was made in canada but has some well know actors in it . i really like this really different version on a kind of scrouge who goes through a lot to find out the magic of christmas is all about our attitude about it',\n",
              "       b'i love this movie , i used to watch it on tv all the time when it was on disney . the story is about a poor family in canada and the mother does not have any christmas spirit and is sent one day ahead and realizes that she has more than what money can buy . this is a great christmas movie , i recommend it .',\n",
              "       b'i was looking forward to this movie , and got it super fast ! ! ! love the expedient service , and look forward to ordering again',\n",
              "       ...,\n",
              "       b'this is tied with quot freddy s revenge quot as the worst sequel . nothing made sense , it followed quot the dream master quot in that alice appeared in this , she and dan were together and she becomes pregnant shortly before high school graduation , but does not find out until dan is killed in an accident , but that is about it . there are not enough words to describe how awful this was . the story behind freddy was explained but it did not matter anyway because he was brought back . this is not freddy at his best .',\n",
              "       b'some laughssome quite horrific scenesglad i got this on dvdgive it a tryfreddy is just as terrifying as the wishmaster , and other sick bad guys in horror movies',\n",
              "       b'this one is just too gorey , it s stupid . the worst acting that i have ever seen . the only thing good was that it was a little funny cause freddy always makes some funny jokes .'],\n",
              "      dtype=object)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "070LOXbLkQt2"
      },
      "source": [
        "#tokenise sentences into isolated words\n",
        "tokenizer_text = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus((sent.numpy() for sent in source_sent) , target_vocab_size = 2**13)\n",
        "tokenizer_summary = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus((summ.numpy() for summ in summary_sent) , target_vocab_size = 2**13)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3reu1EIOR16Z",
        "outputId": "d7361277-67cd-4fbc-c07c-c6bc3be173d4"
      },
      "source": [
        "print(tokenizer_summary.vocab_size)\r\n",
        "print(tokenizer_text.vocab_size)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8118\n",
            "8239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hQqzKCnmdw6"
      },
      "source": [
        "BUFFER_SIZE_ = 20000\n",
        "BATCH_SIZE_ = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqk4bld4vRkt"
      },
      "source": [
        "def encode(lang1 , lang2):\n",
        "  lang1 = [tokenizer_text.vocab_size] + tokenizer_text.encode( lang1.numpy() ) + [tokenizer_text.vocab_size + 1]\n",
        "  lang2 = [tokenizer_summary.vocab_size] + tokenizer_summary.encode( lang2.numpy()) + [tokenizer_summary.vocab_size + 1]\n",
        "  return lang1 , lang2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8n-RJetv5om"
      },
      "source": [
        "def tf_encode(sent,summ):\n",
        "  result_sent , result_summ = tf.py_function(encode ,[sent,summ] ,[tf.int64 , tf.int64])\n",
        "  result_sent.set_shape([None])\n",
        "  result_summ.set_shape([None])\n",
        "\n",
        "  return result_sent,result_summ\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4g5Y0qIyMvq"
      },
      "source": [
        "MAX_LENGTH_SENT = 150\n",
        "MAX_LENGTH_SUM = 30\n",
        "def filter_max_length(x,y,max_length_sent = MAX_LENGTH_SENT , max_length_sum = MAX_LENGTH_SUM):\n",
        "  return tf.logical_and(tf.size(x) <= max_length_sent , tf.size(y) <= max_length_sum)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vnPry9CkMlM"
      },
      "source": [
        "train_examples = tf.data.Dataset.from_tensor_slices((source_sent, summary_sent))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5uuNujRzRVR"
      },
      "source": [
        "train_dataset = train_examples.map(tf_encode)\n",
        "train_dataset = train_dataset.filter(filter_max_length)\n",
        "# cache the dataset to memory to get a speedup while reading from it.\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE_).padded_batch(BATCH_SIZE_)\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "# val_dataset = val_examples.map(tf_encode)\n",
        "# val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8yFrW5nKEnM",
        "outputId": "cca42788-44d2-4a36-e25a-a6fc69a8101f"
      },
      "source": [
        "i=0\n",
        "for en,su in train_dataset:\n",
        "  i+=1\n",
        "print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meyao_vwDuGe"
      },
      "source": [
        "# sent_batch , summ_batch = next(iter(val_dataset))\n",
        "# sent_batch , summ_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah_t6uXAEKZl"
      },
      "source": [
        "#possitonal encoding \n",
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5LGLLBpHdG4"
      },
      "source": [
        "#Masking\n",
        "#seq is of shape => (batch_size,seq_lenght) \n",
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSDaGtxYJ3kZ",
        "outputId": "46d6c97c-5c59-4dd0-9ac9-5750e4b056fa"
      },
      "source": [
        "x = tf.constant([[7,6,0,0,1] , [1,2,3,0,0] , [0,0,0,4,5]])\n",
        "create_padding_mask(x)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
              "array([[[[0., 0., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ey2mywUKVk2"
      },
      "source": [
        "Look Ahead Mask \n",
        "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
        "\n",
        "This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uLa0OnbKBhj"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAejWGpwLl4p",
        "outputId": "137b06c4-86b4-41b3-e9db-71fc2a4b282d"
      },
      "source": [
        "x = tf.random.uniform((1,3))\n",
        "temp = create_look_ahead_mask(x.shape[1])\n",
        "print(temp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0. 1. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 0.]], shape=(3, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdNNLExcTyM7"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6sAQZECVUNs"
      },
      "source": [
        "def print_out(q, k, v):\n",
        "  temp_out, temp_attn = scaled_dot_product_attention(\n",
        "      q, k, v, None)\n",
        "  print ('Attention weights are:')\n",
        "  print (temp_attn)\n",
        "  print ('Output is:')\n",
        "  print (temp_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-Lzt8nPVk2N",
        "outputId": "b8bfc242-e8f4-4e32-b187-8be76a5c3de1"
      },
      "source": [
        "#just for testing purpose\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "temp_k = tf.constant([[10,0,0],\n",
        "                      [0,10,0],\n",
        "                      [0,0,10],\n",
        "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
        "\n",
        "temp_v = tf.constant([[   1,0],\n",
        "                      [  10,0],\n",
        "                      [ 100,5],\n",
        "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
        "\n",
        "# This `query` aligns with the second `key`,\n",
        "# so the second `value` is returned.\n",
        "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W6sPZKmYdG1"
      },
      "source": [
        "# Multiheaded Attention\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCBYdVmyY3b0"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ph3ZxbYcrf3",
        "outputId": "16444eee-b9cf-402c-d066-5c4760a66007"
      },
      "source": [
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
        "out.shape, attn.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWMFH0wzq_Ka"
      },
      "source": [
        "# Pointwise feed forward network\n",
        "Pointwise feed forward network consists of two fully connected layers with Relu activation in between "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TcF3-1lqQiM"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1WL_tcXGHek"
      },
      "source": [
        "# Encoder-Decoder Architecture\n",
        "Defining the encoder-decoder architecture using the above defined components.\n",
        "\n",
        "## Encoder Layer\n",
        "Each encoder= layer consists of sublayers:\n",
        "1. Multi-Head attention( with padding mask)\n",
        "2. Point-wise feed forward networks\n",
        "\n",
        "\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
        "\n",
        "The output of each sublayer is LayerNorm(x + Sublayer(x)). The normalization is done on the d_model (last) axis. There are N encoder layers in the transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qrFuXTIG4wA"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbA9Gjn6btSQ",
        "outputId": "f499de51-c1ea-4cbf-8164-69e4f43e3b8f"
      },
      "source": [
        "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_encoder_layer_output = sample_encoder_layer(\n",
        "    tf.random.uniform((64, 43, 512)), False, None)\n",
        "\n",
        "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 43, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSid1jr1Maf_"
      },
      "source": [
        "# Decoder Layer\n",
        "\n",
        "Each decoder layer consists of sublayers:\n",
        "\n",
        "1. Masked multi-head attention\n",
        "2. Multi-head attention (with padding mask) . V and k receive the encoder output as inputs . Q receives the output from the masked multi-head attention sublayer.\n",
        "3. Pointwise feed forward networks\n",
        "\n",
        "residual layers are added to minimize the problem of vanishing gradient\n",
        "\n",
        "LayerNorm of the form Layernorm( x+sublayer(x)) are added and normalization is done across d_model dimension.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tfYEFz6MZFz"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        " \n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "    \n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3ES1BD6aNEQ",
        "outputId": "016cc702-74e9-4065-a1fe-3775b83c6d43"
      },
      "source": [
        "\n",
        "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
        "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
        "    False, None, None)\n",
        "\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 50, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGUtzM45S-9S"
      },
      "source": [
        "input_vocab_size = tokenizer_text.vocab_size + 2\r\n",
        "target_vocab_size = tokenizer_summary.vocab_size + 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTeAOTYGV17_"
      },
      "source": [
        "#Encoder architecture\n",
        "\n",
        "1. Input Embedding\n",
        "2. Positional Encoding\n",
        "3. N encoder layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICPhaznlUNP0"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                            self.d_model)\n",
        "    \n",
        "    \n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "  \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    \n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdx-JjIkYxPR",
        "outputId": "262e75db-86d9-463f-f462-19bd49fbdb5c"
      },
      "source": [
        "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
        "                         dff=2048, input_vocab_size=input_vocab_size,\n",
        "                         maximum_position_encoding=10000)\n",
        "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
        "\n",
        "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 62, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCzZqYc2Y8C4"
      },
      "source": [
        "# Decoder \n",
        "\n",
        "1. Output Embedding\n",
        "2. Positional Encoding\n",
        "3. N decoder Layers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OR5FvoCY6sZ"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "    \n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "    \n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "    \n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "      \n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX0pnuL6fGbf",
        "outputId": "f0c7aa0c-8fa7-4290-b852-a4c6a5e2b515"
      },
      "source": [
        "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
        "                         dff=2048, target_vocab_size=target_vocab_size,\n",
        "                         maximum_position_encoding=5000)\n",
        "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "output, attn = sample_decoder(temp_input, \n",
        "                              enc_output=sample_encoder_output, \n",
        "                              training=False,\n",
        "                              look_ahead_mask=None, \n",
        "                              padding_mask=None)\n",
        "\n",
        "output.shape, attn['decoder_layer2_block2'].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuo80lLCiFaR"
      },
      "source": [
        "# Transformer Arhitecture\n",
        "\n",
        "1. Encoder Layer\n",
        "2. Decoder Layer\n",
        "3. Final Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v7ho1pXiEHj"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                           input_vocab_size, pe_input, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                           target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "  def call(self, inp, tar, training, enc_padding_mask, \n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "    \n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    \n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    \n",
        "    return final_output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_iQOG9nkJ4L",
        "outputId": "0ade6c64-683e-406c-e2df-3b36d4b1d62a"
      },
      "source": [
        "sample_transformer = Transformer(\n",
        "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
        "    input_vocab_size=input_vocab_size, target_vocab_size=target_vocab_size, \n",
        "    pe_input=10000, pe_target=6000)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
        "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
        "                               enc_padding_mask=None, \n",
        "                               look_ahead_mask=None,\n",
        "                               dec_padding_mask=None)\n",
        "\n",
        "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 36, 8120])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIB3tPSHktQ5"
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "input_vocab_size = tokenizer_text.vocab_size + 2\n",
        "target_vocab_size = tokenizer_summary.vocab_size + 2\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHEJtkdT4TP4",
        "outputId": "f7bfef8b-2782-44c7-8955-5d7b0e2b0d02"
      },
      "source": [
        "input_vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8241"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niYvnbu9lIz6"
      },
      "source": [
        "# Optimizer\n",
        "\n",
        "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the paper \"Attention is all you need\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iti5EFhblSKA"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUoTpPlPlW83"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkjCYFW0yqOz"
      },
      "source": [
        "# Loss Metrics\n",
        "\n",
        "Since the target sequences are padded , it is important to apply a padding mask when calculating the loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYnM94VIy7Ik"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "  \n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS4sbxUv14AR"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVh_1u_qzgvf"
      },
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHWu2Gt275wF",
        "outputId": "8c7acb99-aaee-4085-f425-b513ac5b4559"
      },
      "source": [
        "target_vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8120"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EELpCtqC2kyb"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "  # Encoder padding mask\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 2nd attention block in the decoder.\n",
        "  # This padding mask is used to mask the encoder outputs.\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 1st attention block in the decoder.\n",
        "  # It is used to pad and mask future tokens in the input received by \n",
        "  # the decoder.\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oEEUQfu5Cio"
      },
      "source": [
        "checkpoint_path = \"/content/gdrive/My Drive/checkpoints\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ4i5nYR858Z",
        "outputId": "052ea94d-e9b5-4f4d-925c-483634d46cd9"
      },
      "source": [
        "ckpt_path = tf.train.latest_checkpoint(checkpoint_path)\r\n",
        "ckpt.restore(ckpt_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f580393f320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7qaJVu75pDr"
      },
      "source": [
        "EPOCHS = 20\n",
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  \n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcuWKl0DzyS7",
        "outputId": "268862a0-ceb7-445a-d87e-adbe07cdd594"
      },
      "source": [
        "print(len(source_sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1059562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3Ezb2Do55-N",
        "outputId": "b9176f5b-46e4-4f23-dd6a-275bd8c9faba"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  \n",
        "  # inp -> portuguese, tar -> english\n",
        "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "    train_step(inp, tar)\n",
        "    \n",
        "    if batch % 50 == 0:\n",
        "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "      \n",
        "  if (epoch + 1) % 3 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                         ckpt_save_path))\n",
        "    \n",
        "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))\n",
        "\n",
        "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.2112 Accuracy 0.3436\n",
            "Epoch 1 Batch 50 Loss 4.0831 Accuracy 0.3636\n",
            "Epoch 1 Batch 100 Loss 4.0510 Accuracy 0.3607\n",
            "Epoch 1 Batch 150 Loss 3.9931 Accuracy 0.3639\n",
            "Epoch 1 Batch 200 Loss 3.9533 Accuracy 0.3674\n",
            "Epoch 1 Batch 250 Loss 3.9247 Accuracy 0.3689\n",
            "Epoch 1 Batch 300 Loss 3.9043 Accuracy 0.3697\n",
            "Epoch 1 Batch 350 Loss 3.8929 Accuracy 0.3701\n",
            "Epoch 1 Batch 400 Loss 3.8851 Accuracy 0.3704\n",
            "Epoch 1 Batch 450 Loss 3.8689 Accuracy 0.3714\n",
            "Epoch 1 Batch 500 Loss 3.8457 Accuracy 0.3730\n",
            "Epoch 1 Batch 550 Loss 3.8318 Accuracy 0.3742\n",
            "Epoch 1 Batch 600 Loss 3.8234 Accuracy 0.3748\n",
            "Epoch 1 Batch 650 Loss 3.8146 Accuracy 0.3753\n",
            "Epoch 1 Batch 700 Loss 3.8067 Accuracy 0.3758\n",
            "Epoch 1 Batch 750 Loss 3.7992 Accuracy 0.3765\n",
            "Epoch 1 Batch 800 Loss 3.7930 Accuracy 0.3771\n",
            "Epoch 1 Batch 850 Loss 3.7873 Accuracy 0.3775\n",
            "Epoch 1 Batch 900 Loss 3.7795 Accuracy 0.3781\n",
            "Epoch 1 Batch 950 Loss 3.7759 Accuracy 0.3785\n",
            "Epoch 1 Batch 1000 Loss 3.7710 Accuracy 0.3789\n",
            "Epoch 1 Batch 1050 Loss 3.7671 Accuracy 0.3792\n",
            "Epoch 1 Batch 1100 Loss 3.7646 Accuracy 0.3794\n",
            "Epoch 1 Batch 1150 Loss 3.7630 Accuracy 0.3795\n",
            "Epoch 1 Batch 1200 Loss 3.7621 Accuracy 0.3795\n",
            "Epoch 1 Batch 1250 Loss 3.7614 Accuracy 0.3796\n",
            "Epoch 1 Batch 1300 Loss 3.7604 Accuracy 0.3796\n",
            "Epoch 1 Batch 1350 Loss 3.7618 Accuracy 0.3794\n",
            "Epoch 1 Batch 1400 Loss 3.7623 Accuracy 0.3794\n",
            "Epoch 1 Batch 1450 Loss 3.7622 Accuracy 0.3793\n",
            "Epoch 1 Batch 1500 Loss 3.7640 Accuracy 0.3790\n",
            "Epoch 1 Batch 1550 Loss 3.7651 Accuracy 0.3788\n",
            "Epoch 1 Batch 1600 Loss 3.7638 Accuracy 0.3788\n",
            "Epoch 1 Batch 1650 Loss 3.7628 Accuracy 0.3786\n",
            "Epoch 1 Batch 1700 Loss 3.7614 Accuracy 0.3787\n",
            "Epoch 1 Batch 1750 Loss 3.7598 Accuracy 0.3788\n",
            "Epoch 1 Batch 1800 Loss 3.7587 Accuracy 0.3788\n",
            "Epoch 1 Batch 1850 Loss 3.7585 Accuracy 0.3787\n",
            "Epoch 1 Batch 1900 Loss 3.7584 Accuracy 0.3787\n",
            "Epoch 1 Batch 1950 Loss 3.7574 Accuracy 0.3787\n",
            "Epoch 1 Batch 2000 Loss 3.7571 Accuracy 0.3786\n",
            "Epoch 1 Batch 2050 Loss 3.7562 Accuracy 0.3786\n",
            "Epoch 1 Batch 2100 Loss 3.7555 Accuracy 0.3786\n",
            "Epoch 1 Batch 2150 Loss 3.7548 Accuracy 0.3787\n",
            "Epoch 1 Batch 2200 Loss 3.7554 Accuracy 0.3787\n",
            "Epoch 1 Batch 2250 Loss 3.7555 Accuracy 0.3786\n",
            "Epoch 1 Batch 2300 Loss 3.7567 Accuracy 0.3785\n",
            "Epoch 1 Batch 2350 Loss 3.7570 Accuracy 0.3785\n",
            "Epoch 1 Batch 2400 Loss 3.7574 Accuracy 0.3785\n",
            "Epoch 1 Batch 2450 Loss 3.7574 Accuracy 0.3785\n",
            "Epoch 1 Batch 2500 Loss 3.7575 Accuracy 0.3784\n",
            "Epoch 1 Batch 2550 Loss 3.7567 Accuracy 0.3785\n",
            "Epoch 1 Batch 2600 Loss 3.7566 Accuracy 0.3786\n",
            "Epoch 1 Batch 2650 Loss 3.7568 Accuracy 0.3785\n",
            "Epoch 1 Batch 2700 Loss 3.7565 Accuracy 0.3785\n",
            "Epoch 1 Batch 2750 Loss 3.7567 Accuracy 0.3785\n",
            "Epoch 1 Batch 2800 Loss 3.7580 Accuracy 0.3784\n",
            "Epoch 1 Batch 2850 Loss 3.7582 Accuracy 0.3784\n",
            "Epoch 1 Batch 2900 Loss 3.7583 Accuracy 0.3784\n",
            "Epoch 1 Batch 2950 Loss 3.7584 Accuracy 0.3783\n",
            "Epoch 1 Batch 3000 Loss 3.7590 Accuracy 0.3782\n",
            "Epoch 1 Batch 3050 Loss 3.7585 Accuracy 0.3782\n",
            "Epoch 1 Batch 3100 Loss 3.7583 Accuracy 0.3782\n",
            "Epoch 1 Batch 3150 Loss 3.7579 Accuracy 0.3782\n",
            "Epoch 1 Batch 3200 Loss 3.7584 Accuracy 0.3782\n",
            "Epoch 1 Batch 3250 Loss 3.7578 Accuracy 0.3782\n",
            "Epoch 1 Batch 3300 Loss 3.7576 Accuracy 0.3782\n",
            "Epoch 1 Batch 3350 Loss 3.7580 Accuracy 0.3781\n",
            "Epoch 1 Batch 3400 Loss 3.7577 Accuracy 0.3781\n",
            "Epoch 1 Batch 3450 Loss 3.7577 Accuracy 0.3780\n",
            "Epoch 1 Batch 3500 Loss 3.7576 Accuracy 0.3780\n",
            "Epoch 1 Batch 3550 Loss 3.7575 Accuracy 0.3779\n",
            "Epoch 1 Batch 3600 Loss 3.7570 Accuracy 0.3780\n",
            "Epoch 1 Batch 3650 Loss 3.7575 Accuracy 0.3779\n",
            "Epoch 1 Batch 3700 Loss 3.7578 Accuracy 0.3778\n",
            "Epoch 1 Batch 3750 Loss 3.7574 Accuracy 0.3778\n",
            "Epoch 1 Batch 3800 Loss 3.7569 Accuracy 0.3778\n",
            "Epoch 1 Batch 3850 Loss 3.7573 Accuracy 0.3777\n",
            "Epoch 1 Batch 3900 Loss 3.7578 Accuracy 0.3776\n",
            "Epoch 1 Batch 3950 Loss 3.7573 Accuracy 0.3776\n",
            "Epoch 1 Batch 4000 Loss 3.7578 Accuracy 0.3775\n",
            "Epoch 1 Batch 4050 Loss 3.7585 Accuracy 0.3775\n",
            "Epoch 1 Batch 4100 Loss 3.7589 Accuracy 0.3774\n",
            "Epoch 1 Batch 4150 Loss 3.7598 Accuracy 0.3773\n",
            "Epoch 1 Batch 4200 Loss 3.7603 Accuracy 0.3772\n",
            "Epoch 1 Batch 4250 Loss 3.7610 Accuracy 0.3771\n",
            "Epoch 1 Batch 4300 Loss 3.7612 Accuracy 0.3771\n",
            "Epoch 1 Batch 4350 Loss 3.7616 Accuracy 0.3770\n",
            "Epoch 1 Batch 4400 Loss 3.7622 Accuracy 0.3768\n",
            "Epoch 1 Batch 4450 Loss 3.7632 Accuracy 0.3767\n",
            "Epoch 1 Batch 4500 Loss 3.7634 Accuracy 0.3767\n",
            "Epoch 1 Batch 4550 Loss 3.7641 Accuracy 0.3766\n",
            "Epoch 1 Batch 4600 Loss 3.7648 Accuracy 0.3764\n",
            "Epoch 1 Batch 4650 Loss 3.7656 Accuracy 0.3764\n",
            "Epoch 1 Batch 4700 Loss 3.7666 Accuracy 0.3763\n",
            "Epoch 1 Batch 4750 Loss 3.7670 Accuracy 0.3762\n",
            "Epoch 1 Batch 4800 Loss 3.7676 Accuracy 0.3761\n",
            "Epoch 1 Batch 4850 Loss 3.7677 Accuracy 0.3761\n",
            "Epoch 1 Batch 4900 Loss 3.7679 Accuracy 0.3761\n",
            "Epoch 1 Batch 4950 Loss 3.7683 Accuracy 0.3760\n",
            "Epoch 1 Batch 5000 Loss 3.7687 Accuracy 0.3759\n",
            "Epoch 1 Batch 5050 Loss 3.7689 Accuracy 0.3758\n",
            "Epoch 1 Batch 5100 Loss 3.7684 Accuracy 0.3758\n",
            "Epoch 1 Batch 5150 Loss 3.7685 Accuracy 0.3758\n",
            "Epoch 1 Batch 5200 Loss 3.7681 Accuracy 0.3758\n",
            "Epoch 1 Batch 5250 Loss 3.7678 Accuracy 0.3758\n",
            "Epoch 1 Batch 5300 Loss 3.7677 Accuracy 0.3758\n",
            "Epoch 1 Batch 5350 Loss 3.7678 Accuracy 0.3758\n",
            "Epoch 1 Batch 5400 Loss 3.7684 Accuracy 0.3757\n",
            "Epoch 1 Batch 5450 Loss 3.7686 Accuracy 0.3757\n",
            "Epoch 1 Batch 5500 Loss 3.7689 Accuracy 0.3756\n",
            "Epoch 1 Batch 5550 Loss 3.7694 Accuracy 0.3756\n",
            "Epoch 1 Batch 5600 Loss 3.7697 Accuracy 0.3755\n",
            "Epoch 1 Batch 5650 Loss 3.7702 Accuracy 0.3754\n",
            "Epoch 1 Batch 5700 Loss 3.7704 Accuracy 0.3754\n",
            "Epoch 1 Batch 5750 Loss 3.7707 Accuracy 0.3753\n",
            "Epoch 1 Batch 5800 Loss 3.7714 Accuracy 0.3752\n",
            "Epoch 1 Batch 5850 Loss 3.7719 Accuracy 0.3751\n",
            "Epoch 1 Batch 5900 Loss 3.7729 Accuracy 0.3750\n",
            "Epoch 1 Batch 5950 Loss 3.7735 Accuracy 0.3749\n",
            "Epoch 1 Batch 6000 Loss 3.7743 Accuracy 0.3748\n",
            "Epoch 1 Batch 6050 Loss 3.7750 Accuracy 0.3747\n",
            "Epoch 1 Batch 6100 Loss 3.7760 Accuracy 0.3746\n",
            "Epoch 1 Batch 6150 Loss 3.7767 Accuracy 0.3745\n",
            "Epoch 1 Batch 6200 Loss 3.7772 Accuracy 0.3744\n",
            "Epoch 1 Batch 6250 Loss 3.7775 Accuracy 0.3743\n",
            "Epoch 1 Batch 6300 Loss 3.7781 Accuracy 0.3742\n",
            "Epoch 1 Batch 6350 Loss 3.7785 Accuracy 0.3741\n",
            "Epoch 1 Batch 6400 Loss 3.7789 Accuracy 0.3740\n",
            "Epoch 1 Batch 6450 Loss 3.7795 Accuracy 0.3739\n",
            "Epoch 1 Batch 6500 Loss 3.7793 Accuracy 0.3738\n",
            "Epoch 1 Batch 6550 Loss 3.7796 Accuracy 0.3738\n",
            "Epoch 1 Batch 6600 Loss 3.7803 Accuracy 0.3737\n",
            "Epoch 1 Batch 6650 Loss 3.7803 Accuracy 0.3736\n",
            "Epoch 1 Batch 6700 Loss 3.7804 Accuracy 0.3735\n",
            "Epoch 1 Batch 6750 Loss 3.7805 Accuracy 0.3735\n",
            "Epoch 1 Batch 6800 Loss 3.7809 Accuracy 0.3734\n",
            "Epoch 1 Batch 6850 Loss 3.7809 Accuracy 0.3734\n",
            "Epoch 1 Batch 6900 Loss 3.7812 Accuracy 0.3733\n",
            "Epoch 1 Batch 6950 Loss 3.7817 Accuracy 0.3732\n",
            "Epoch 1 Batch 7000 Loss 3.7821 Accuracy 0.3731\n",
            "Epoch 1 Batch 7050 Loss 3.7823 Accuracy 0.3731\n",
            "Epoch 1 Batch 7100 Loss 3.7826 Accuracy 0.3731\n",
            "Epoch 1 Batch 7150 Loss 3.7829 Accuracy 0.3730\n",
            "Epoch 1 Batch 7200 Loss 3.7833 Accuracy 0.3729\n",
            "Epoch 1 Batch 7250 Loss 3.7835 Accuracy 0.3728\n",
            "Epoch 1 Batch 7300 Loss 3.7840 Accuracy 0.3728\n",
            "Epoch 1 Batch 7350 Loss 3.7844 Accuracy 0.3727\n",
            "Epoch 1 Batch 7400 Loss 3.7847 Accuracy 0.3726\n",
            "Epoch 1 Batch 7450 Loss 3.7849 Accuracy 0.3726\n",
            "Epoch 1 Batch 7500 Loss 3.7851 Accuracy 0.3725\n",
            "Epoch 1 Batch 7550 Loss 3.7853 Accuracy 0.3725\n",
            "Epoch 1 Batch 7600 Loss 3.7858 Accuracy 0.3724\n",
            "Epoch 1 Batch 7650 Loss 3.7860 Accuracy 0.3724\n",
            "Epoch 1 Batch 7700 Loss 3.7863 Accuracy 0.3723\n",
            "Epoch 1 Batch 7750 Loss 3.7866 Accuracy 0.3723\n",
            "Epoch 1 Batch 7800 Loss 3.7871 Accuracy 0.3722\n",
            "Epoch 1 Batch 7850 Loss 3.7871 Accuracy 0.3722\n",
            "Epoch 1 Batch 7900 Loss 3.7873 Accuracy 0.3722\n",
            "Epoch 1 Batch 7950 Loss 3.7872 Accuracy 0.3721\n",
            "Epoch 1 Batch 8000 Loss 3.7872 Accuracy 0.3721\n",
            "Epoch 1 Batch 8050 Loss 3.7874 Accuracy 0.3721\n",
            "Epoch 1 Batch 8100 Loss 3.7873 Accuracy 0.3721\n",
            "Epoch 1 Batch 8150 Loss 3.7872 Accuracy 0.3721\n",
            "Epoch 1 Batch 8200 Loss 3.7874 Accuracy 0.3721\n",
            "Epoch 1 Batch 8250 Loss 3.7875 Accuracy 0.3720\n",
            "Epoch 1 Batch 8300 Loss 3.7877 Accuracy 0.3720\n",
            "Epoch 1 Batch 8350 Loss 3.7877 Accuracy 0.3720\n",
            "Epoch 1 Batch 8400 Loss 3.7878 Accuracy 0.3719\n",
            "Epoch 1 Batch 8450 Loss 3.7877 Accuracy 0.3719\n",
            "Epoch 1 Batch 8500 Loss 3.7879 Accuracy 0.3719\n",
            "Epoch 1 Batch 8550 Loss 3.7880 Accuracy 0.3718\n",
            "Epoch 1 Batch 8600 Loss 3.7882 Accuracy 0.3718\n",
            "Epoch 1 Batch 8650 Loss 3.7882 Accuracy 0.3718\n",
            "Epoch 1 Batch 8700 Loss 3.7883 Accuracy 0.3717\n",
            "Epoch 1 Batch 8750 Loss 3.7881 Accuracy 0.3717\n",
            "Epoch 1 Batch 8800 Loss 3.7883 Accuracy 0.3717\n",
            "Epoch 1 Batch 8850 Loss 3.7882 Accuracy 0.3717\n",
            "Epoch 1 Batch 8900 Loss 3.7881 Accuracy 0.3717\n",
            "Epoch 1 Batch 8950 Loss 3.7882 Accuracy 0.3717\n",
            "Epoch 1 Batch 9000 Loss 3.7882 Accuracy 0.3717\n",
            "Epoch 1 Batch 9050 Loss 3.7881 Accuracy 0.3717\n",
            "Epoch 1 Batch 9100 Loss 3.7882 Accuracy 0.3716\n",
            "Epoch 1 Batch 9150 Loss 3.7881 Accuracy 0.3716\n",
            "Epoch 1 Batch 9200 Loss 3.7881 Accuracy 0.3716\n",
            "Epoch 1 Batch 9250 Loss 3.7881 Accuracy 0.3716\n",
            "Epoch 1 Batch 9300 Loss 3.7882 Accuracy 0.3716\n",
            "Epoch 1 Batch 9350 Loss 3.7883 Accuracy 0.3716\n",
            "Epoch 1 Batch 9400 Loss 3.7883 Accuracy 0.3716\n",
            "Epoch 1 Batch 9450 Loss 3.7880 Accuracy 0.3716\n",
            "Epoch 1 Batch 9500 Loss 3.7881 Accuracy 0.3716\n",
            "Epoch 1 Batch 9550 Loss 3.7880 Accuracy 0.3716\n",
            "Epoch 1 Batch 9600 Loss 3.7882 Accuracy 0.3716\n",
            "Epoch 1 Batch 9650 Loss 3.7881 Accuracy 0.3715\n",
            "Epoch 1 Batch 9700 Loss 3.7879 Accuracy 0.3715\n",
            "Epoch 1 Batch 9750 Loss 3.7878 Accuracy 0.3715\n",
            "Epoch 1 Batch 9800 Loss 3.7877 Accuracy 0.3715\n",
            "Epoch 1 Batch 9850 Loss 3.7875 Accuracy 0.3715\n",
            "Epoch 1 Batch 9900 Loss 3.7871 Accuracy 0.3716\n",
            "Epoch 1 Batch 9950 Loss 3.7869 Accuracy 0.3716\n",
            "Epoch 1 Batch 10000 Loss 3.7867 Accuracy 0.3716\n",
            "Epoch 1 Batch 10050 Loss 3.7864 Accuracy 0.3716\n",
            "Epoch 1 Batch 10100 Loss 3.7863 Accuracy 0.3716\n",
            "Epoch 1 Batch 10150 Loss 3.7860 Accuracy 0.3716\n",
            "Epoch 1 Batch 10200 Loss 3.7856 Accuracy 0.3716\n",
            "Epoch 1 Batch 10250 Loss 3.7855 Accuracy 0.3716\n",
            "Epoch 1 Batch 10300 Loss 3.7850 Accuracy 0.3717\n",
            "Epoch 1 Batch 10350 Loss 3.7847 Accuracy 0.3717\n",
            "Epoch 1 Batch 10400 Loss 3.7844 Accuracy 0.3717\n",
            "Epoch 1 Batch 10450 Loss 3.7842 Accuracy 0.3718\n",
            "Epoch 1 Batch 10500 Loss 3.7838 Accuracy 0.3718\n",
            "Epoch 1 Batch 10550 Loss 3.7835 Accuracy 0.3718\n",
            "Epoch 1 Batch 10600 Loss 3.7834 Accuracy 0.3718\n",
            "Epoch 1 Batch 10650 Loss 3.7831 Accuracy 0.3718\n",
            "Epoch 1 Batch 10700 Loss 3.7829 Accuracy 0.3718\n",
            "Epoch 1 Batch 10750 Loss 3.7826 Accuracy 0.3719\n",
            "Epoch 1 Batch 10800 Loss 3.7822 Accuracy 0.3719\n",
            "Epoch 1 Batch 10850 Loss 3.7816 Accuracy 0.3719\n",
            "Epoch 1 Batch 10900 Loss 3.7810 Accuracy 0.3720\n",
            "Epoch 1 Batch 10950 Loss 3.7805 Accuracy 0.3720\n",
            "Epoch 1 Batch 11000 Loss 3.7800 Accuracy 0.3721\n",
            "Epoch 1 Batch 11050 Loss 3.7797 Accuracy 0.3721\n",
            "Epoch 1 Batch 11100 Loss 3.7793 Accuracy 0.3722\n",
            "Epoch 1 Batch 11150 Loss 3.7790 Accuracy 0.3722\n",
            "Epoch 1 Batch 11200 Loss 3.7786 Accuracy 0.3723\n",
            "Epoch 1 Batch 11250 Loss 3.7783 Accuracy 0.3723\n",
            "Epoch 1 Batch 11300 Loss 3.7776 Accuracy 0.3723\n",
            "Epoch 1 Batch 11350 Loss 3.7770 Accuracy 0.3724\n",
            "Epoch 1 Batch 11400 Loss 3.7765 Accuracy 0.3724\n",
            "Epoch 1 Batch 11450 Loss 3.7759 Accuracy 0.3725\n",
            "Epoch 1 Batch 11500 Loss 3.7756 Accuracy 0.3725\n",
            "Epoch 1 Batch 11550 Loss 3.7750 Accuracy 0.3725\n",
            "Epoch 1 Batch 11600 Loss 3.7747 Accuracy 0.3726\n",
            "Epoch 1 Batch 11650 Loss 3.7743 Accuracy 0.3726\n",
            "Epoch 1 Batch 11700 Loss 3.7743 Accuracy 0.3726\n",
            "Epoch 1 Batch 11750 Loss 3.7742 Accuracy 0.3726\n",
            "Epoch 1 Batch 11800 Loss 3.7739 Accuracy 0.3727\n",
            "Epoch 1 Batch 11850 Loss 3.7737 Accuracy 0.3727\n",
            "Epoch 1 Batch 11900 Loss 3.7732 Accuracy 0.3728\n",
            "Epoch 1 Batch 11950 Loss 3.7728 Accuracy 0.3728\n",
            "Epoch 1 Batch 12000 Loss 3.7722 Accuracy 0.3729\n",
            "Epoch 1 Batch 12050 Loss 3.7719 Accuracy 0.3729\n",
            "Epoch 1 Batch 12100 Loss 3.7714 Accuracy 0.3730\n",
            "Epoch 1 Batch 12150 Loss 3.7709 Accuracy 0.3730\n",
            "Epoch 1 Batch 12200 Loss 3.7706 Accuracy 0.3730\n",
            "Epoch 1 Batch 12250 Loss 3.7703 Accuracy 0.3731\n",
            "Epoch 1 Batch 12300 Loss 3.7700 Accuracy 0.3731\n",
            "Epoch 1 Batch 12350 Loss 3.7695 Accuracy 0.3731\n",
            "Epoch 1 Batch 12400 Loss 3.7691 Accuracy 0.3732\n",
            "Epoch 1 Batch 12450 Loss 3.7688 Accuracy 0.3732\n",
            "Epoch 1 Batch 12500 Loss 3.7686 Accuracy 0.3732\n",
            "Epoch 1 Batch 12550 Loss 3.7682 Accuracy 0.3732\n",
            "Epoch 1 Batch 12600 Loss 3.7677 Accuracy 0.3733\n",
            "Epoch 1 Batch 12650 Loss 3.7671 Accuracy 0.3734\n",
            "Epoch 1 Batch 12700 Loss 3.7664 Accuracy 0.3735\n",
            "Epoch 1 Batch 12750 Loss 3.7655 Accuracy 0.3736\n",
            "Epoch 1 Batch 12800 Loss 3.7646 Accuracy 0.3736\n",
            "Epoch 1 Batch 12850 Loss 3.7639 Accuracy 0.3737\n",
            "Epoch 1 Batch 12900 Loss 3.7630 Accuracy 0.3738\n",
            "Epoch 1 Batch 12950 Loss 3.7622 Accuracy 0.3739\n",
            "Epoch 1 Batch 13000 Loss 3.7614 Accuracy 0.3740\n",
            "Epoch 1 Batch 13050 Loss 3.7609 Accuracy 0.3741\n",
            "Epoch 1 Batch 13100 Loss 3.7603 Accuracy 0.3741\n",
            "Epoch 1 Batch 13150 Loss 3.7599 Accuracy 0.3741\n",
            "Epoch 1 Batch 13200 Loss 3.7595 Accuracy 0.3742\n",
            "Epoch 1 Batch 13250 Loss 3.7592 Accuracy 0.3742\n",
            "Epoch 1 Batch 13300 Loss 3.7589 Accuracy 0.3743\n",
            "Epoch 1 Batch 13350 Loss 3.7587 Accuracy 0.3743\n",
            "Epoch 1 Batch 13400 Loss 3.7587 Accuracy 0.3743\n",
            "Epoch 1 Batch 13450 Loss 3.7584 Accuracy 0.3743\n",
            "Epoch 1 Batch 13500 Loss 3.7583 Accuracy 0.3743\n",
            "Epoch 1 Batch 13550 Loss 3.7580 Accuracy 0.3744\n",
            "Epoch 1 Batch 13600 Loss 3.7579 Accuracy 0.3744\n",
            "Epoch 1 Batch 13650 Loss 3.7575 Accuracy 0.3744\n",
            "Epoch 1 Batch 13700 Loss 3.7570 Accuracy 0.3745\n",
            "Epoch 1 Batch 13750 Loss 3.7563 Accuracy 0.3746\n",
            "Epoch 1 Batch 13800 Loss 3.7557 Accuracy 0.3746\n",
            "Epoch 1 Batch 13850 Loss 3.7552 Accuracy 0.3747\n",
            "Epoch 1 Batch 13900 Loss 3.7546 Accuracy 0.3747\n",
            "Epoch 1 Batch 13950 Loss 3.7539 Accuracy 0.3748\n",
            "Epoch 1 Batch 14000 Loss 3.7533 Accuracy 0.3749\n",
            "Epoch 1 Batch 14050 Loss 3.7527 Accuracy 0.3749\n",
            "Epoch 1 Batch 14100 Loss 3.7520 Accuracy 0.3750\n",
            "Epoch 1 Batch 14150 Loss 3.7517 Accuracy 0.3750\n",
            "Epoch 1 Batch 14200 Loss 3.7511 Accuracy 0.3751\n",
            "Epoch 1 Batch 14250 Loss 3.7507 Accuracy 0.3751\n",
            "Epoch 1 Batch 14300 Loss 3.7503 Accuracy 0.3751\n",
            "Epoch 1 Batch 14350 Loss 3.7499 Accuracy 0.3752\n",
            "Epoch 1 Batch 14400 Loss 3.7496 Accuracy 0.3752\n",
            "Epoch 1 Batch 14450 Loss 3.7493 Accuracy 0.3753\n",
            "Epoch 1 Batch 14500 Loss 3.7489 Accuracy 0.3753\n",
            "Epoch 1 Batch 14550 Loss 3.7486 Accuracy 0.3753\n",
            "Epoch 1 Batch 14600 Loss 3.7483 Accuracy 0.3753\n",
            "Epoch 1 Batch 14650 Loss 3.7480 Accuracy 0.3753\n",
            "Epoch 1 Batch 14700 Loss 3.7477 Accuracy 0.3754\n",
            "Epoch 1 Batch 14750 Loss 3.7473 Accuracy 0.3754\n",
            "Epoch 1 Batch 14800 Loss 3.7471 Accuracy 0.3754\n",
            "Epoch 1 Batch 14850 Loss 3.7469 Accuracy 0.3754\n",
            "Epoch 1 Batch 14900 Loss 3.7468 Accuracy 0.3754\n",
            "Epoch 1 Batch 14950 Loss 3.7467 Accuracy 0.3754\n",
            "Epoch 1 Batch 15000 Loss 3.7466 Accuracy 0.3754\n",
            "Epoch 1 Batch 15050 Loss 3.7462 Accuracy 0.3754\n",
            "Epoch 1 Batch 15100 Loss 3.7461 Accuracy 0.3754\n",
            "Epoch 1 Batch 15150 Loss 3.7457 Accuracy 0.3755\n",
            "Epoch 1 Batch 15200 Loss 3.7454 Accuracy 0.3755\n",
            "Epoch 1 Batch 15250 Loss 3.7452 Accuracy 0.3755\n",
            "Epoch 1 Batch 15300 Loss 3.7451 Accuracy 0.3755\n",
            "Epoch 1 Batch 15350 Loss 3.7448 Accuracy 0.3755\n",
            "Epoch 1 Batch 15400 Loss 3.7447 Accuracy 0.3756\n",
            "Epoch 1 Batch 15450 Loss 3.7442 Accuracy 0.3756\n",
            "Epoch 1 Batch 15500 Loss 3.7438 Accuracy 0.3756\n",
            "Epoch 1 Batch 15550 Loss 3.7433 Accuracy 0.3756\n",
            "Epoch 1 Loss 3.7433 Accuracy 0.3756\n",
            "Time taken for 1 epoch: 3924.5945448875427 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 4.0232 Accuracy 0.3739\n",
            "Epoch 2 Batch 50 Loss 4.0622 Accuracy 0.3606\n",
            "Epoch 2 Batch 100 Loss 4.0137 Accuracy 0.3645\n",
            "Epoch 2 Batch 150 Loss 3.9608 Accuracy 0.3681\n",
            "Epoch 2 Batch 200 Loss 3.9299 Accuracy 0.3698\n",
            "Epoch 2 Batch 250 Loss 3.9164 Accuracy 0.3697\n",
            "Epoch 2 Batch 300 Loss 3.8974 Accuracy 0.3709\n",
            "Epoch 2 Batch 350 Loss 3.8844 Accuracy 0.3713\n",
            "Epoch 2 Batch 400 Loss 3.8702 Accuracy 0.3719\n",
            "Epoch 2 Batch 450 Loss 3.8558 Accuracy 0.3733\n",
            "Epoch 2 Batch 500 Loss 3.8455 Accuracy 0.3736\n",
            "Epoch 2 Batch 550 Loss 3.8343 Accuracy 0.3744\n",
            "Epoch 2 Batch 600 Loss 3.8238 Accuracy 0.3750\n",
            "Epoch 2 Batch 650 Loss 3.8125 Accuracy 0.3758\n",
            "Epoch 2 Batch 700 Loss 3.8004 Accuracy 0.3767\n",
            "Epoch 2 Batch 750 Loss 3.7962 Accuracy 0.3768\n",
            "Epoch 2 Batch 800 Loss 3.7912 Accuracy 0.3770\n",
            "Epoch 2 Batch 850 Loss 3.7861 Accuracy 0.3773\n",
            "Epoch 2 Batch 900 Loss 3.7831 Accuracy 0.3774\n",
            "Epoch 2 Batch 950 Loss 3.7769 Accuracy 0.3779\n",
            "Epoch 2 Batch 1000 Loss 3.7699 Accuracy 0.3786\n",
            "Epoch 2 Batch 1050 Loss 3.7674 Accuracy 0.3789\n",
            "Epoch 2 Batch 1100 Loss 3.7625 Accuracy 0.3793\n",
            "Epoch 2 Batch 1150 Loss 3.7622 Accuracy 0.3792\n",
            "Epoch 2 Batch 1200 Loss 3.7608 Accuracy 0.3796\n",
            "Epoch 2 Batch 1250 Loss 3.7599 Accuracy 0.3796\n",
            "Epoch 2 Batch 1300 Loss 3.7582 Accuracy 0.3796\n",
            "Epoch 2 Batch 1350 Loss 3.7589 Accuracy 0.3795\n",
            "Epoch 2 Batch 1400 Loss 3.7609 Accuracy 0.3791\n",
            "Epoch 2 Batch 1450 Loss 3.7604 Accuracy 0.3790\n",
            "Epoch 2 Batch 1500 Loss 3.7622 Accuracy 0.3787\n",
            "Epoch 2 Batch 1550 Loss 3.7634 Accuracy 0.3785\n",
            "Epoch 2 Batch 1600 Loss 3.7633 Accuracy 0.3785\n",
            "Epoch 2 Batch 1650 Loss 3.7617 Accuracy 0.3785\n",
            "Epoch 2 Batch 1700 Loss 3.7608 Accuracy 0.3786\n",
            "Epoch 2 Batch 1750 Loss 3.7584 Accuracy 0.3787\n",
            "Epoch 2 Batch 1800 Loss 3.7581 Accuracy 0.3787\n",
            "Epoch 2 Batch 1850 Loss 3.7578 Accuracy 0.3787\n",
            "Epoch 2 Batch 1900 Loss 3.7579 Accuracy 0.3787\n",
            "Epoch 2 Batch 1950 Loss 3.7573 Accuracy 0.3787\n",
            "Epoch 2 Batch 2000 Loss 3.7562 Accuracy 0.3789\n",
            "Epoch 2 Batch 2050 Loss 3.7550 Accuracy 0.3789\n",
            "Epoch 2 Batch 2100 Loss 3.7547 Accuracy 0.3790\n",
            "Epoch 2 Batch 2150 Loss 3.7540 Accuracy 0.3790\n",
            "Epoch 2 Batch 2200 Loss 3.7549 Accuracy 0.3789\n",
            "Epoch 2 Batch 2250 Loss 3.7554 Accuracy 0.3789\n",
            "Epoch 2 Batch 2300 Loss 3.7563 Accuracy 0.3787\n",
            "Epoch 2 Batch 2350 Loss 3.7564 Accuracy 0.3788\n",
            "Epoch 2 Batch 2400 Loss 3.7564 Accuracy 0.3787\n",
            "Epoch 2 Batch 2450 Loss 3.7558 Accuracy 0.3788\n",
            "Epoch 2 Batch 2500 Loss 3.7557 Accuracy 0.3787\n",
            "Epoch 2 Batch 2550 Loss 3.7556 Accuracy 0.3787\n",
            "Epoch 2 Batch 2600 Loss 3.7548 Accuracy 0.3788\n",
            "Epoch 2 Batch 2650 Loss 3.7560 Accuracy 0.3787\n",
            "Epoch 2 Batch 2700 Loss 3.7564 Accuracy 0.3786\n",
            "Epoch 2 Batch 2750 Loss 3.7564 Accuracy 0.3786\n",
            "Epoch 2 Batch 2800 Loss 3.7564 Accuracy 0.3786\n",
            "Epoch 2 Batch 2850 Loss 3.7564 Accuracy 0.3785\n",
            "Epoch 2 Batch 2900 Loss 3.7559 Accuracy 0.3785\n",
            "Epoch 2 Batch 2950 Loss 3.7564 Accuracy 0.3784\n",
            "Epoch 2 Batch 3000 Loss 3.7567 Accuracy 0.3784\n",
            "Epoch 2 Batch 3050 Loss 3.7566 Accuracy 0.3784\n",
            "Epoch 2 Batch 3100 Loss 3.7572 Accuracy 0.3783\n",
            "Epoch 2 Batch 3150 Loss 3.7576 Accuracy 0.3783\n",
            "Epoch 2 Batch 3200 Loss 3.7575 Accuracy 0.3783\n",
            "Epoch 2 Batch 3250 Loss 3.7571 Accuracy 0.3783\n",
            "Epoch 2 Batch 3300 Loss 3.7565 Accuracy 0.3784\n",
            "Epoch 2 Batch 3350 Loss 3.7561 Accuracy 0.3784\n",
            "Epoch 2 Batch 3400 Loss 3.7556 Accuracy 0.3784\n",
            "Epoch 2 Batch 3450 Loss 3.7560 Accuracy 0.3784\n",
            "Epoch 2 Batch 3500 Loss 3.7557 Accuracy 0.3783\n",
            "Epoch 2 Batch 3550 Loss 3.7556 Accuracy 0.3783\n",
            "Epoch 2 Batch 3600 Loss 3.7555 Accuracy 0.3782\n",
            "Epoch 2 Batch 3650 Loss 3.7564 Accuracy 0.3781\n",
            "Epoch 2 Batch 3700 Loss 3.7563 Accuracy 0.3781\n",
            "Epoch 2 Batch 3750 Loss 3.7564 Accuracy 0.3780\n",
            "Epoch 2 Batch 3800 Loss 3.7568 Accuracy 0.3779\n",
            "Epoch 2 Batch 3850 Loss 3.7568 Accuracy 0.3778\n",
            "Epoch 2 Batch 3900 Loss 3.7576 Accuracy 0.3777\n",
            "Epoch 2 Batch 3950 Loss 3.7577 Accuracy 0.3777\n",
            "Epoch 2 Batch 4000 Loss 3.7580 Accuracy 0.3776\n",
            "Epoch 2 Batch 4050 Loss 3.7585 Accuracy 0.3776\n",
            "Epoch 2 Batch 4100 Loss 3.7589 Accuracy 0.3775\n",
            "Epoch 2 Batch 4150 Loss 3.7596 Accuracy 0.3774\n",
            "Epoch 2 Batch 4200 Loss 3.7599 Accuracy 0.3774\n",
            "Epoch 2 Batch 4250 Loss 3.7602 Accuracy 0.3773\n",
            "Epoch 2 Batch 4300 Loss 3.7609 Accuracy 0.3772\n",
            "Epoch 2 Batch 4350 Loss 3.7614 Accuracy 0.3772\n",
            "Epoch 2 Batch 4400 Loss 3.7620 Accuracy 0.3771\n",
            "Epoch 2 Batch 4450 Loss 3.7622 Accuracy 0.3770\n",
            "Epoch 2 Batch 4500 Loss 3.7624 Accuracy 0.3769\n",
            "Epoch 2 Batch 4550 Loss 3.7633 Accuracy 0.3767\n",
            "Epoch 2 Batch 4600 Loss 3.7644 Accuracy 0.3766\n",
            "Epoch 2 Batch 4650 Loss 3.7652 Accuracy 0.3765\n",
            "Epoch 2 Batch 4700 Loss 3.7656 Accuracy 0.3764\n",
            "Epoch 2 Batch 4750 Loss 3.7664 Accuracy 0.3763\n",
            "Epoch 2 Batch 4800 Loss 3.7665 Accuracy 0.3763\n",
            "Epoch 2 Batch 4850 Loss 3.7666 Accuracy 0.3763\n",
            "Epoch 2 Batch 4900 Loss 3.7671 Accuracy 0.3762\n",
            "Epoch 2 Batch 4950 Loss 3.7670 Accuracy 0.3762\n",
            "Epoch 2 Batch 5000 Loss 3.7673 Accuracy 0.3761\n",
            "Epoch 2 Batch 5050 Loss 3.7672 Accuracy 0.3761\n",
            "Epoch 2 Batch 5100 Loss 3.7675 Accuracy 0.3759\n",
            "Epoch 2 Batch 5150 Loss 3.7670 Accuracy 0.3759\n",
            "Epoch 2 Batch 5200 Loss 3.7670 Accuracy 0.3759\n",
            "Epoch 2 Batch 5250 Loss 3.7671 Accuracy 0.3759\n",
            "Epoch 2 Batch 5300 Loss 3.7671 Accuracy 0.3758\n",
            "Epoch 2 Batch 5350 Loss 3.7674 Accuracy 0.3758\n",
            "Epoch 2 Batch 5400 Loss 3.7672 Accuracy 0.3758\n",
            "Epoch 2 Batch 5450 Loss 3.7676 Accuracy 0.3757\n",
            "Epoch 2 Batch 5500 Loss 3.7683 Accuracy 0.3756\n",
            "Epoch 2 Batch 5550 Loss 3.7690 Accuracy 0.3755\n",
            "Epoch 2 Batch 5600 Loss 3.7692 Accuracy 0.3755\n",
            "Epoch 2 Batch 5650 Loss 3.7693 Accuracy 0.3754\n",
            "Epoch 2 Batch 5700 Loss 3.7695 Accuracy 0.3754\n",
            "Epoch 2 Batch 5750 Loss 3.7701 Accuracy 0.3753\n",
            "Epoch 2 Batch 5800 Loss 3.7709 Accuracy 0.3752\n",
            "Epoch 2 Batch 5850 Loss 3.7713 Accuracy 0.3751\n",
            "Epoch 2 Batch 5900 Loss 3.7721 Accuracy 0.3750\n",
            "Epoch 2 Batch 5950 Loss 3.7727 Accuracy 0.3749\n",
            "Epoch 2 Batch 6000 Loss 3.7735 Accuracy 0.3748\n",
            "Epoch 2 Batch 6050 Loss 3.7742 Accuracy 0.3747\n",
            "Epoch 2 Batch 6100 Loss 3.7752 Accuracy 0.3746\n",
            "Epoch 2 Batch 6150 Loss 3.7761 Accuracy 0.3745\n",
            "Epoch 2 Batch 6200 Loss 3.7767 Accuracy 0.3744\n",
            "Epoch 2 Batch 6250 Loss 3.7771 Accuracy 0.3743\n",
            "Epoch 2 Batch 6300 Loss 3.7774 Accuracy 0.3742\n",
            "Epoch 2 Batch 6350 Loss 3.7779 Accuracy 0.3741\n",
            "Epoch 2 Batch 6400 Loss 3.7784 Accuracy 0.3741\n",
            "Epoch 2 Batch 6450 Loss 3.7788 Accuracy 0.3740\n",
            "Epoch 2 Batch 6500 Loss 3.7789 Accuracy 0.3739\n",
            "Epoch 2 Batch 6550 Loss 3.7789 Accuracy 0.3739\n",
            "Epoch 2 Batch 6600 Loss 3.7793 Accuracy 0.3738\n",
            "Epoch 2 Batch 6650 Loss 3.7794 Accuracy 0.3737\n",
            "Epoch 2 Batch 6700 Loss 3.7796 Accuracy 0.3737\n",
            "Epoch 2 Batch 6750 Loss 3.7797 Accuracy 0.3736\n",
            "Epoch 2 Batch 6800 Loss 3.7799 Accuracy 0.3735\n",
            "Epoch 2 Batch 6850 Loss 3.7801 Accuracy 0.3735\n",
            "Epoch 2 Batch 6900 Loss 3.7805 Accuracy 0.3734\n",
            "Epoch 2 Batch 6950 Loss 3.7809 Accuracy 0.3733\n",
            "Epoch 2 Batch 7000 Loss 3.7812 Accuracy 0.3733\n",
            "Epoch 2 Batch 7050 Loss 3.7813 Accuracy 0.3732\n",
            "Epoch 2 Batch 7100 Loss 3.7818 Accuracy 0.3731\n",
            "Epoch 2 Batch 7150 Loss 3.7820 Accuracy 0.3731\n",
            "Epoch 2 Batch 7200 Loss 3.7824 Accuracy 0.3730\n",
            "Epoch 2 Batch 7250 Loss 3.7827 Accuracy 0.3729\n",
            "Epoch 2 Batch 7300 Loss 3.7833 Accuracy 0.3729\n",
            "Epoch 2 Batch 7350 Loss 3.7835 Accuracy 0.3728\n",
            "Epoch 2 Batch 7400 Loss 3.7835 Accuracy 0.3728\n",
            "Epoch 2 Batch 7450 Loss 3.7837 Accuracy 0.3727\n",
            "Epoch 2 Batch 7500 Loss 3.7841 Accuracy 0.3726\n",
            "Epoch 2 Batch 7550 Loss 3.7846 Accuracy 0.3726\n",
            "Epoch 2 Batch 7600 Loss 3.7852 Accuracy 0.3725\n",
            "Epoch 2 Batch 7650 Loss 3.7854 Accuracy 0.3725\n",
            "Epoch 2 Batch 7700 Loss 3.7857 Accuracy 0.3724\n",
            "Epoch 2 Batch 7750 Loss 3.7859 Accuracy 0.3724\n",
            "Epoch 2 Batch 7800 Loss 3.7859 Accuracy 0.3723\n",
            "Epoch 2 Batch 7850 Loss 3.7859 Accuracy 0.3723\n",
            "Epoch 2 Batch 7900 Loss 3.7861 Accuracy 0.3723\n",
            "Epoch 2 Batch 7950 Loss 3.7862 Accuracy 0.3723\n",
            "Epoch 2 Batch 8000 Loss 3.7862 Accuracy 0.3723\n",
            "Epoch 2 Batch 8050 Loss 3.7863 Accuracy 0.3723\n",
            "Epoch 2 Batch 8100 Loss 3.7863 Accuracy 0.3722\n",
            "Epoch 2 Batch 8150 Loss 3.7864 Accuracy 0.3722\n",
            "Epoch 2 Batch 8200 Loss 3.7864 Accuracy 0.3722\n",
            "Epoch 2 Batch 8250 Loss 3.7865 Accuracy 0.3722\n",
            "Epoch 2 Batch 8300 Loss 3.7868 Accuracy 0.3721\n",
            "Epoch 2 Batch 8350 Loss 3.7868 Accuracy 0.3721\n",
            "Epoch 2 Batch 8400 Loss 3.7871 Accuracy 0.3721\n",
            "Epoch 2 Batch 8450 Loss 3.7872 Accuracy 0.3720\n",
            "Epoch 2 Batch 8500 Loss 3.7873 Accuracy 0.3720\n",
            "Epoch 2 Batch 8550 Loss 3.7872 Accuracy 0.3720\n",
            "Epoch 2 Batch 8600 Loss 3.7873 Accuracy 0.3719\n",
            "Epoch 2 Batch 8650 Loss 3.7870 Accuracy 0.3720\n",
            "Epoch 2 Batch 8700 Loss 3.7872 Accuracy 0.3719\n",
            "Epoch 2 Batch 8750 Loss 3.7871 Accuracy 0.3719\n",
            "Epoch 2 Batch 8800 Loss 3.7870 Accuracy 0.3719\n",
            "Epoch 2 Batch 8850 Loss 3.7870 Accuracy 0.3719\n",
            "Epoch 2 Batch 8900 Loss 3.7869 Accuracy 0.3719\n",
            "Epoch 2 Batch 8950 Loss 3.7869 Accuracy 0.3719\n",
            "Epoch 2 Batch 9000 Loss 3.7869 Accuracy 0.3719\n",
            "Epoch 2 Batch 9050 Loss 3.7866 Accuracy 0.3719\n",
            "Epoch 2 Batch 9100 Loss 3.7866 Accuracy 0.3718\n",
            "Epoch 2 Batch 9150 Loss 3.7867 Accuracy 0.3718\n",
            "Epoch 2 Batch 9200 Loss 3.7866 Accuracy 0.3718\n",
            "Epoch 2 Batch 9250 Loss 3.7867 Accuracy 0.3718\n",
            "Epoch 2 Batch 9300 Loss 3.7869 Accuracy 0.3718\n",
            "Epoch 2 Batch 9350 Loss 3.7870 Accuracy 0.3718\n",
            "Epoch 2 Batch 9400 Loss 3.7871 Accuracy 0.3717\n",
            "Epoch 2 Batch 9450 Loss 3.7871 Accuracy 0.3717\n",
            "Epoch 2 Batch 9500 Loss 3.7871 Accuracy 0.3717\n",
            "Epoch 2 Batch 9550 Loss 3.7871 Accuracy 0.3717\n",
            "Epoch 2 Batch 9600 Loss 3.7872 Accuracy 0.3717\n",
            "Epoch 2 Batch 9650 Loss 3.7872 Accuracy 0.3716\n",
            "Epoch 2 Batch 9700 Loss 3.7870 Accuracy 0.3716\n",
            "Epoch 2 Batch 9750 Loss 3.7866 Accuracy 0.3716\n",
            "Epoch 2 Batch 9800 Loss 3.7864 Accuracy 0.3716\n",
            "Epoch 2 Batch 9850 Loss 3.7864 Accuracy 0.3716\n",
            "Epoch 2 Batch 9900 Loss 3.7860 Accuracy 0.3717\n",
            "Epoch 2 Batch 9950 Loss 3.7857 Accuracy 0.3717\n",
            "Epoch 2 Batch 10000 Loss 3.7855 Accuracy 0.3717\n",
            "Epoch 2 Batch 10050 Loss 3.7852 Accuracy 0.3717\n",
            "Epoch 2 Batch 10100 Loss 3.7847 Accuracy 0.3718\n",
            "Epoch 2 Batch 10150 Loss 3.7844 Accuracy 0.3718\n",
            "Epoch 2 Batch 10200 Loss 3.7842 Accuracy 0.3718\n",
            "Epoch 2 Batch 10250 Loss 3.7839 Accuracy 0.3718\n",
            "Epoch 2 Batch 10300 Loss 3.7838 Accuracy 0.3719\n",
            "Epoch 2 Batch 10350 Loss 3.7832 Accuracy 0.3719\n",
            "Epoch 2 Batch 10400 Loss 3.7828 Accuracy 0.3720\n",
            "Epoch 2 Batch 10450 Loss 3.7828 Accuracy 0.3720\n",
            "Epoch 2 Batch 10500 Loss 3.7827 Accuracy 0.3720\n",
            "Epoch 2 Batch 10550 Loss 3.7824 Accuracy 0.3720\n",
            "Epoch 2 Batch 10600 Loss 3.7821 Accuracy 0.3720\n",
            "Epoch 2 Batch 10650 Loss 3.7818 Accuracy 0.3720\n",
            "Epoch 2 Batch 10700 Loss 3.7814 Accuracy 0.3721\n",
            "Epoch 2 Batch 10750 Loss 3.7813 Accuracy 0.3721\n",
            "Epoch 2 Batch 10800 Loss 3.7810 Accuracy 0.3721\n",
            "Epoch 2 Batch 10850 Loss 3.7803 Accuracy 0.3722\n",
            "Epoch 2 Batch 10900 Loss 3.7796 Accuracy 0.3722\n",
            "Epoch 2 Batch 10950 Loss 3.7790 Accuracy 0.3723\n",
            "Epoch 2 Batch 11000 Loss 3.7785 Accuracy 0.3723\n",
            "Epoch 2 Batch 11050 Loss 3.7780 Accuracy 0.3724\n",
            "Epoch 2 Batch 11100 Loss 3.7777 Accuracy 0.3724\n",
            "Epoch 2 Batch 11150 Loss 3.7772 Accuracy 0.3725\n",
            "Epoch 2 Batch 11200 Loss 3.7769 Accuracy 0.3725\n",
            "Epoch 2 Batch 11250 Loss 3.7765 Accuracy 0.3726\n",
            "Epoch 2 Batch 11300 Loss 3.7758 Accuracy 0.3727\n",
            "Epoch 2 Batch 11350 Loss 3.7752 Accuracy 0.3727\n",
            "Epoch 2 Batch 11400 Loss 3.7747 Accuracy 0.3727\n",
            "Epoch 2 Batch 11450 Loss 3.7743 Accuracy 0.3727\n",
            "Epoch 2 Batch 11500 Loss 3.7741 Accuracy 0.3728\n",
            "Epoch 2 Batch 11550 Loss 3.7737 Accuracy 0.3728\n",
            "Epoch 2 Batch 11600 Loss 3.7731 Accuracy 0.3728\n",
            "Epoch 2 Batch 11650 Loss 3.7730 Accuracy 0.3728\n",
            "Epoch 2 Batch 11700 Loss 3.7727 Accuracy 0.3729\n",
            "Epoch 2 Batch 11750 Loss 3.7727 Accuracy 0.3729\n",
            "Epoch 2 Batch 11800 Loss 3.7726 Accuracy 0.3729\n",
            "Epoch 2 Batch 11850 Loss 3.7723 Accuracy 0.3729\n",
            "Epoch 2 Batch 11900 Loss 3.7718 Accuracy 0.3730\n",
            "Epoch 2 Batch 11950 Loss 3.7716 Accuracy 0.3730\n",
            "Epoch 2 Batch 12000 Loss 3.7711 Accuracy 0.3731\n",
            "Epoch 2 Batch 12050 Loss 3.7706 Accuracy 0.3731\n",
            "Epoch 2 Batch 12100 Loss 3.7701 Accuracy 0.3731\n",
            "Epoch 2 Batch 12150 Loss 3.7696 Accuracy 0.3732\n",
            "Epoch 2 Batch 12200 Loss 3.7694 Accuracy 0.3732\n",
            "Epoch 2 Batch 12250 Loss 3.7688 Accuracy 0.3733\n",
            "Epoch 2 Batch 12300 Loss 3.7684 Accuracy 0.3733\n",
            "Epoch 2 Batch 12350 Loss 3.7679 Accuracy 0.3733\n",
            "Epoch 2 Batch 12400 Loss 3.7676 Accuracy 0.3734\n",
            "Epoch 2 Batch 12450 Loss 3.7673 Accuracy 0.3734\n",
            "Epoch 2 Batch 12500 Loss 3.7669 Accuracy 0.3735\n",
            "Epoch 2 Batch 12550 Loss 3.7665 Accuracy 0.3735\n",
            "Epoch 2 Batch 12600 Loss 3.7662 Accuracy 0.3735\n",
            "Epoch 2 Batch 12650 Loss 3.7655 Accuracy 0.3736\n",
            "Epoch 2 Batch 12700 Loss 3.7648 Accuracy 0.3737\n",
            "Epoch 2 Batch 12750 Loss 3.7639 Accuracy 0.3738\n",
            "Epoch 2 Batch 12800 Loss 3.7631 Accuracy 0.3738\n",
            "Epoch 2 Batch 12850 Loss 3.7622 Accuracy 0.3740\n",
            "Epoch 2 Batch 12900 Loss 3.7613 Accuracy 0.3741\n",
            "Epoch 2 Batch 12950 Loss 3.7606 Accuracy 0.3741\n",
            "Epoch 2 Batch 13000 Loss 3.7600 Accuracy 0.3742\n",
            "Epoch 2 Batch 13050 Loss 3.7595 Accuracy 0.3743\n",
            "Epoch 2 Batch 13100 Loss 3.7589 Accuracy 0.3743\n",
            "Epoch 2 Batch 13150 Loss 3.7582 Accuracy 0.3744\n",
            "Epoch 2 Batch 13200 Loss 3.7578 Accuracy 0.3744\n",
            "Epoch 2 Batch 13250 Loss 3.7575 Accuracy 0.3745\n",
            "Epoch 2 Batch 13300 Loss 3.7572 Accuracy 0.3745\n",
            "Epoch 2 Batch 13350 Loss 3.7567 Accuracy 0.3746\n",
            "Epoch 2 Batch 13400 Loss 3.7568 Accuracy 0.3746\n",
            "Epoch 2 Batch 13450 Loss 3.7567 Accuracy 0.3746\n",
            "Epoch 2 Batch 13500 Loss 3.7565 Accuracy 0.3746\n",
            "Epoch 2 Batch 13550 Loss 3.7566 Accuracy 0.3746\n",
            "Epoch 2 Batch 13600 Loss 3.7564 Accuracy 0.3746\n",
            "Epoch 2 Batch 13650 Loss 3.7560 Accuracy 0.3747\n",
            "Epoch 2 Batch 13700 Loss 3.7555 Accuracy 0.3747\n",
            "Epoch 2 Batch 13750 Loss 3.7549 Accuracy 0.3748\n",
            "Epoch 2 Batch 13800 Loss 3.7543 Accuracy 0.3748\n",
            "Epoch 2 Batch 13850 Loss 3.7537 Accuracy 0.3749\n",
            "Epoch 2 Batch 13900 Loss 3.7533 Accuracy 0.3749\n",
            "Epoch 2 Batch 13950 Loss 3.7527 Accuracy 0.3750\n",
            "Epoch 2 Batch 14000 Loss 3.7521 Accuracy 0.3751\n",
            "Epoch 2 Batch 14050 Loss 3.7514 Accuracy 0.3751\n",
            "Epoch 2 Batch 14100 Loss 3.7508 Accuracy 0.3752\n",
            "Epoch 2 Batch 14150 Loss 3.7503 Accuracy 0.3752\n",
            "Epoch 2 Batch 14200 Loss 3.7498 Accuracy 0.3753\n",
            "Epoch 2 Batch 14250 Loss 3.7496 Accuracy 0.3753\n",
            "Epoch 2 Batch 14300 Loss 3.7490 Accuracy 0.3754\n",
            "Epoch 2 Batch 14350 Loss 3.7486 Accuracy 0.3754\n",
            "Epoch 2 Batch 14400 Loss 3.7481 Accuracy 0.3754\n",
            "Epoch 2 Batch 14450 Loss 3.7478 Accuracy 0.3755\n",
            "Epoch 2 Batch 14500 Loss 3.7474 Accuracy 0.3755\n",
            "Epoch 2 Batch 14550 Loss 3.7470 Accuracy 0.3755\n",
            "Epoch 2 Batch 14600 Loss 3.7466 Accuracy 0.3756\n",
            "Epoch 2 Batch 14650 Loss 3.7463 Accuracy 0.3756\n",
            "Epoch 2 Batch 14700 Loss 3.7459 Accuracy 0.3756\n",
            "Epoch 2 Batch 14750 Loss 3.7456 Accuracy 0.3757\n",
            "Epoch 2 Batch 14800 Loss 3.7454 Accuracy 0.3757\n",
            "Epoch 2 Batch 14850 Loss 3.7452 Accuracy 0.3757\n",
            "Epoch 2 Batch 14900 Loss 3.7452 Accuracy 0.3757\n",
            "Epoch 2 Batch 14950 Loss 3.7450 Accuracy 0.3757\n",
            "Epoch 2 Batch 15000 Loss 3.7448 Accuracy 0.3757\n",
            "Epoch 2 Batch 15050 Loss 3.7446 Accuracy 0.3757\n",
            "Epoch 2 Batch 15100 Loss 3.7444 Accuracy 0.3757\n",
            "Epoch 2 Batch 15150 Loss 3.7442 Accuracy 0.3757\n",
            "Epoch 2 Batch 15200 Loss 3.7439 Accuracy 0.3757\n",
            "Epoch 2 Batch 15250 Loss 3.7437 Accuracy 0.3757\n",
            "Epoch 2 Batch 15300 Loss 3.7434 Accuracy 0.3758\n",
            "Epoch 2 Batch 15350 Loss 3.7433 Accuracy 0.3757\n",
            "Epoch 2 Batch 15400 Loss 3.7430 Accuracy 0.3758\n",
            "Epoch 2 Batch 15450 Loss 3.7426 Accuracy 0.3758\n",
            "Epoch 2 Batch 15500 Loss 3.7423 Accuracy 0.3758\n",
            "Epoch 2 Batch 15550 Loss 3.7418 Accuracy 0.3758\n",
            "Epoch 2 Loss 3.7418 Accuracy 0.3758\n",
            "Time taken for 1 epoch: 3515.996914625168 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.3015 Accuracy 0.3209\n",
            "Epoch 3 Batch 50 Loss 4.0382 Accuracy 0.3604\n",
            "Epoch 3 Batch 100 Loss 3.9968 Accuracy 0.3661\n",
            "Epoch 3 Batch 150 Loss 3.9799 Accuracy 0.3669\n",
            "Epoch 3 Batch 200 Loss 3.9499 Accuracy 0.3677\n",
            "Epoch 3 Batch 250 Loss 3.9273 Accuracy 0.3688\n",
            "Epoch 3 Batch 300 Loss 3.9051 Accuracy 0.3702\n",
            "Epoch 3 Batch 350 Loss 3.8832 Accuracy 0.3715\n",
            "Epoch 3 Batch 400 Loss 3.8711 Accuracy 0.3719\n",
            "Epoch 3 Batch 450 Loss 3.8589 Accuracy 0.3728\n",
            "Epoch 3 Batch 500 Loss 3.8427 Accuracy 0.3737\n",
            "Epoch 3 Batch 550 Loss 3.8303 Accuracy 0.3746\n",
            "Epoch 3 Batch 600 Loss 3.8195 Accuracy 0.3754\n",
            "Epoch 3 Batch 650 Loss 3.8112 Accuracy 0.3762\n",
            "Epoch 3 Batch 700 Loss 3.8007 Accuracy 0.3769\n",
            "Epoch 3 Batch 750 Loss 3.7946 Accuracy 0.3772\n",
            "Epoch 3 Batch 800 Loss 3.7911 Accuracy 0.3772\n",
            "Epoch 3 Batch 850 Loss 3.7840 Accuracy 0.3776\n",
            "Epoch 3 Batch 900 Loss 3.7744 Accuracy 0.3785\n",
            "Epoch 3 Batch 950 Loss 3.7693 Accuracy 0.3789\n",
            "Epoch 3 Batch 1000 Loss 3.7669 Accuracy 0.3793\n",
            "Epoch 3 Batch 1050 Loss 3.7624 Accuracy 0.3797\n",
            "Epoch 3 Batch 1100 Loss 3.7598 Accuracy 0.3799\n",
            "Epoch 3 Batch 1150 Loss 3.7577 Accuracy 0.3802\n",
            "Epoch 3 Batch 1200 Loss 3.7564 Accuracy 0.3803\n",
            "Epoch 3 Batch 1250 Loss 3.7564 Accuracy 0.3801\n",
            "Epoch 3 Batch 1300 Loss 3.7556 Accuracy 0.3803\n",
            "Epoch 3 Batch 1350 Loss 3.7566 Accuracy 0.3802\n",
            "Epoch 3 Batch 1400 Loss 3.7574 Accuracy 0.3800\n",
            "Epoch 3 Batch 1450 Loss 3.7597 Accuracy 0.3798\n",
            "Epoch 3 Batch 1500 Loss 3.7607 Accuracy 0.3797\n",
            "Epoch 3 Batch 1550 Loss 3.7606 Accuracy 0.3795\n",
            "Epoch 3 Batch 1600 Loss 3.7611 Accuracy 0.3794\n",
            "Epoch 3 Batch 1650 Loss 3.7614 Accuracy 0.3792\n",
            "Epoch 3 Batch 1700 Loss 3.7598 Accuracy 0.3793\n",
            "Epoch 3 Batch 1750 Loss 3.7585 Accuracy 0.3792\n",
            "Epoch 3 Batch 1800 Loss 3.7576 Accuracy 0.3792\n",
            "Epoch 3 Batch 1850 Loss 3.7564 Accuracy 0.3793\n",
            "Epoch 3 Batch 1900 Loss 3.7574 Accuracy 0.3791\n",
            "Epoch 3 Batch 1950 Loss 3.7560 Accuracy 0.3792\n",
            "Epoch 3 Batch 2000 Loss 3.7544 Accuracy 0.3793\n",
            "Epoch 3 Batch 2050 Loss 3.7540 Accuracy 0.3792\n",
            "Epoch 3 Batch 2100 Loss 3.7551 Accuracy 0.3790\n",
            "Epoch 3 Batch 2150 Loss 3.7548 Accuracy 0.3791\n",
            "Epoch 3 Batch 2200 Loss 3.7548 Accuracy 0.3791\n",
            "Epoch 3 Batch 2250 Loss 3.7534 Accuracy 0.3792\n",
            "Epoch 3 Batch 2300 Loss 3.7537 Accuracy 0.3791\n",
            "Epoch 3 Batch 2350 Loss 3.7539 Accuracy 0.3790\n",
            "Epoch 3 Batch 2400 Loss 3.7538 Accuracy 0.3790\n",
            "Epoch 3 Batch 2450 Loss 3.7535 Accuracy 0.3790\n",
            "Epoch 3 Batch 2500 Loss 3.7537 Accuracy 0.3789\n",
            "Epoch 3 Batch 2550 Loss 3.7530 Accuracy 0.3789\n",
            "Epoch 3 Batch 2600 Loss 3.7529 Accuracy 0.3789\n",
            "Epoch 3 Batch 2650 Loss 3.7534 Accuracy 0.3789\n",
            "Epoch 3 Batch 2700 Loss 3.7531 Accuracy 0.3790\n",
            "Epoch 3 Batch 2750 Loss 3.7532 Accuracy 0.3790\n",
            "Epoch 3 Batch 2800 Loss 3.7539 Accuracy 0.3789\n",
            "Epoch 3 Batch 2850 Loss 3.7548 Accuracy 0.3788\n",
            "Epoch 3 Batch 2900 Loss 3.7553 Accuracy 0.3787\n",
            "Epoch 3 Batch 2950 Loss 3.7552 Accuracy 0.3786\n",
            "Epoch 3 Batch 3000 Loss 3.7561 Accuracy 0.3786\n",
            "Epoch 3 Batch 3050 Loss 3.7560 Accuracy 0.3785\n",
            "Epoch 3 Batch 3100 Loss 3.7567 Accuracy 0.3784\n",
            "Epoch 3 Batch 3150 Loss 3.7574 Accuracy 0.3784\n",
            "Epoch 3 Batch 3200 Loss 3.7565 Accuracy 0.3785\n",
            "Epoch 3 Batch 3250 Loss 3.7572 Accuracy 0.3784\n",
            "Epoch 3 Batch 3300 Loss 3.7566 Accuracy 0.3783\n",
            "Epoch 3 Batch 3350 Loss 3.7559 Accuracy 0.3784\n",
            "Epoch 3 Batch 3400 Loss 3.7557 Accuracy 0.3784\n",
            "Epoch 3 Batch 3450 Loss 3.7553 Accuracy 0.3784\n",
            "Epoch 3 Batch 3500 Loss 3.7559 Accuracy 0.3783\n",
            "Epoch 3 Batch 3550 Loss 3.7552 Accuracy 0.3784\n",
            "Epoch 3 Batch 3600 Loss 3.7557 Accuracy 0.3783\n",
            "Epoch 3 Batch 3650 Loss 3.7559 Accuracy 0.3782\n",
            "Epoch 3 Batch 3700 Loss 3.7553 Accuracy 0.3783\n",
            "Epoch 3 Batch 3750 Loss 3.7560 Accuracy 0.3782\n",
            "Epoch 3 Batch 3800 Loss 3.7552 Accuracy 0.3782\n",
            "Epoch 3 Batch 3850 Loss 3.7559 Accuracy 0.3780\n",
            "Epoch 3 Batch 3900 Loss 3.7570 Accuracy 0.3779\n",
            "Epoch 3 Batch 3950 Loss 3.7576 Accuracy 0.3778\n",
            "Epoch 3 Batch 4000 Loss 3.7573 Accuracy 0.3777\n",
            "Epoch 3 Batch 4050 Loss 3.7570 Accuracy 0.3778\n",
            "Epoch 3 Batch 4100 Loss 3.7576 Accuracy 0.3776\n",
            "Epoch 3 Batch 4150 Loss 3.7586 Accuracy 0.3775\n",
            "Epoch 3 Batch 4200 Loss 3.7594 Accuracy 0.3774\n",
            "Epoch 3 Batch 4250 Loss 3.7597 Accuracy 0.3774\n",
            "Epoch 3 Batch 4300 Loss 3.7605 Accuracy 0.3772\n",
            "Epoch 3 Batch 4350 Loss 3.7607 Accuracy 0.3771\n",
            "Epoch 3 Batch 4400 Loss 3.7609 Accuracy 0.3770\n",
            "Epoch 3 Batch 4450 Loss 3.7615 Accuracy 0.3769\n",
            "Epoch 3 Batch 4500 Loss 3.7624 Accuracy 0.3768\n",
            "Epoch 3 Batch 4550 Loss 3.7626 Accuracy 0.3767\n",
            "Epoch 3 Batch 4600 Loss 3.7632 Accuracy 0.3766\n",
            "Epoch 3 Batch 4650 Loss 3.7643 Accuracy 0.3765\n",
            "Epoch 3 Batch 4700 Loss 3.7653 Accuracy 0.3764\n",
            "Epoch 3 Batch 4750 Loss 3.7656 Accuracy 0.3764\n",
            "Epoch 3 Batch 4800 Loss 3.7661 Accuracy 0.3763\n",
            "Epoch 3 Batch 4850 Loss 3.7662 Accuracy 0.3763\n",
            "Epoch 3 Batch 4900 Loss 3.7663 Accuracy 0.3763\n",
            "Epoch 3 Batch 4950 Loss 3.7661 Accuracy 0.3763\n",
            "Epoch 3 Batch 5000 Loss 3.7666 Accuracy 0.3762\n",
            "Epoch 3 Batch 5050 Loss 3.7665 Accuracy 0.3762\n",
            "Epoch 3 Batch 5100 Loss 3.7661 Accuracy 0.3762\n",
            "Epoch 3 Batch 5150 Loss 3.7662 Accuracy 0.3762\n",
            "Epoch 3 Batch 5200 Loss 3.7658 Accuracy 0.3761\n",
            "Epoch 3 Batch 5250 Loss 3.7657 Accuracy 0.3761\n",
            "Epoch 3 Batch 5300 Loss 3.7661 Accuracy 0.3761\n",
            "Epoch 3 Batch 5350 Loss 3.7662 Accuracy 0.3760\n",
            "Epoch 3 Batch 5400 Loss 3.7660 Accuracy 0.3760\n",
            "Epoch 3 Batch 5450 Loss 3.7666 Accuracy 0.3759\n",
            "Epoch 3 Batch 5500 Loss 3.7673 Accuracy 0.3758\n",
            "Epoch 3 Batch 5550 Loss 3.7677 Accuracy 0.3758\n",
            "Epoch 3 Batch 5600 Loss 3.7684 Accuracy 0.3757\n",
            "Epoch 3 Batch 5650 Loss 3.7690 Accuracy 0.3755\n",
            "Epoch 3 Batch 5700 Loss 3.7690 Accuracy 0.3755\n",
            "Epoch 3 Batch 5750 Loss 3.7694 Accuracy 0.3754\n",
            "Epoch 3 Batch 5800 Loss 3.7700 Accuracy 0.3754\n",
            "Epoch 3 Batch 5850 Loss 3.7705 Accuracy 0.3752\n",
            "Epoch 3 Batch 5900 Loss 3.7715 Accuracy 0.3751\n",
            "Epoch 3 Batch 5950 Loss 3.7720 Accuracy 0.3750\n",
            "Epoch 3 Batch 6000 Loss 3.7730 Accuracy 0.3749\n",
            "Epoch 3 Batch 6050 Loss 3.7734 Accuracy 0.3748\n",
            "Epoch 3 Batch 6100 Loss 3.7740 Accuracy 0.3747\n",
            "Epoch 3 Batch 6150 Loss 3.7751 Accuracy 0.3746\n",
            "Epoch 3 Batch 6200 Loss 3.7754 Accuracy 0.3746\n",
            "Epoch 3 Batch 6250 Loss 3.7757 Accuracy 0.3745\n",
            "Epoch 3 Batch 6300 Loss 3.7758 Accuracy 0.3744\n",
            "Epoch 3 Batch 6350 Loss 3.7765 Accuracy 0.3743\n",
            "Epoch 3 Batch 6400 Loss 3.7769 Accuracy 0.3742\n",
            "Epoch 3 Batch 6450 Loss 3.7774 Accuracy 0.3742\n",
            "Epoch 3 Batch 6500 Loss 3.7777 Accuracy 0.3741\n",
            "Epoch 3 Batch 6550 Loss 3.7781 Accuracy 0.3740\n",
            "Epoch 3 Batch 6600 Loss 3.7781 Accuracy 0.3740\n",
            "Epoch 3 Batch 6650 Loss 3.7784 Accuracy 0.3739\n",
            "Epoch 3 Batch 6700 Loss 3.7786 Accuracy 0.3739\n",
            "Epoch 3 Batch 6750 Loss 3.7787 Accuracy 0.3738\n",
            "Epoch 3 Batch 6800 Loss 3.7790 Accuracy 0.3737\n",
            "Epoch 3 Batch 6850 Loss 3.7792 Accuracy 0.3736\n",
            "Epoch 3 Batch 6900 Loss 3.7796 Accuracy 0.3736\n",
            "Epoch 3 Batch 6950 Loss 3.7798 Accuracy 0.3735\n",
            "Epoch 3 Batch 7000 Loss 3.7802 Accuracy 0.3734\n",
            "Epoch 3 Batch 7050 Loss 3.7805 Accuracy 0.3733\n",
            "Epoch 3 Batch 7100 Loss 3.7808 Accuracy 0.3733\n",
            "Epoch 3 Batch 7150 Loss 3.7814 Accuracy 0.3732\n",
            "Epoch 3 Batch 7200 Loss 3.7815 Accuracy 0.3731\n",
            "Epoch 3 Batch 7250 Loss 3.7821 Accuracy 0.3731\n",
            "Epoch 3 Batch 7300 Loss 3.7822 Accuracy 0.3730\n",
            "Epoch 3 Batch 7350 Loss 3.7825 Accuracy 0.3730\n",
            "Epoch 3 Batch 7400 Loss 3.7827 Accuracy 0.3729\n",
            "Epoch 3 Batch 7450 Loss 3.7831 Accuracy 0.3729\n",
            "Epoch 3 Batch 7500 Loss 3.7834 Accuracy 0.3728\n",
            "Epoch 3 Batch 7550 Loss 3.7836 Accuracy 0.3727\n",
            "Epoch 3 Batch 7600 Loss 3.7839 Accuracy 0.3727\n",
            "Epoch 3 Batch 7650 Loss 3.7840 Accuracy 0.3726\n",
            "Epoch 3 Batch 7700 Loss 3.7840 Accuracy 0.3726\n",
            "Epoch 3 Batch 7750 Loss 3.7841 Accuracy 0.3726\n",
            "Epoch 3 Batch 7800 Loss 3.7842 Accuracy 0.3725\n",
            "Epoch 3 Batch 7850 Loss 3.7843 Accuracy 0.3725\n",
            "Epoch 3 Batch 7900 Loss 3.7845 Accuracy 0.3725\n",
            "Epoch 3 Batch 7950 Loss 3.7848 Accuracy 0.3724\n",
            "Epoch 3 Batch 8000 Loss 3.7851 Accuracy 0.3724\n",
            "Epoch 3 Batch 8050 Loss 3.7853 Accuracy 0.3723\n",
            "Epoch 3 Batch 8100 Loss 3.7856 Accuracy 0.3723\n",
            "Epoch 3 Batch 8150 Loss 3.7857 Accuracy 0.3723\n",
            "Epoch 3 Batch 8200 Loss 3.7858 Accuracy 0.3723\n",
            "Epoch 3 Batch 8250 Loss 3.7857 Accuracy 0.3723\n",
            "Epoch 3 Batch 8300 Loss 3.7855 Accuracy 0.3723\n",
            "Epoch 3 Batch 8350 Loss 3.7855 Accuracy 0.3722\n",
            "Epoch 3 Batch 8400 Loss 3.7857 Accuracy 0.3722\n",
            "Epoch 3 Batch 8450 Loss 3.7860 Accuracy 0.3722\n",
            "Epoch 3 Batch 8500 Loss 3.7861 Accuracy 0.3722\n",
            "Epoch 3 Batch 8550 Loss 3.7862 Accuracy 0.3721\n",
            "Epoch 3 Batch 8600 Loss 3.7861 Accuracy 0.3721\n",
            "Epoch 3 Batch 8650 Loss 3.7862 Accuracy 0.3721\n",
            "Epoch 3 Batch 8700 Loss 3.7860 Accuracy 0.3721\n",
            "Epoch 3 Batch 8750 Loss 3.7859 Accuracy 0.3721\n",
            "Epoch 3 Batch 8800 Loss 3.7858 Accuracy 0.3720\n",
            "Epoch 3 Batch 8850 Loss 3.7860 Accuracy 0.3720\n",
            "Epoch 3 Batch 8900 Loss 3.7858 Accuracy 0.3720\n",
            "Epoch 3 Batch 8950 Loss 3.7858 Accuracy 0.3720\n",
            "Epoch 3 Batch 9000 Loss 3.7855 Accuracy 0.3720\n",
            "Epoch 3 Batch 9050 Loss 3.7854 Accuracy 0.3720\n",
            "Epoch 3 Batch 9100 Loss 3.7855 Accuracy 0.3720\n",
            "Epoch 3 Batch 9150 Loss 3.7852 Accuracy 0.3720\n",
            "Epoch 3 Batch 9200 Loss 3.7851 Accuracy 0.3720\n",
            "Epoch 3 Batch 9250 Loss 3.7853 Accuracy 0.3719\n",
            "Epoch 3 Batch 9300 Loss 3.7855 Accuracy 0.3719\n",
            "Epoch 3 Batch 9350 Loss 3.7856 Accuracy 0.3719\n",
            "Epoch 3 Batch 9400 Loss 3.7854 Accuracy 0.3719\n",
            "Epoch 3 Batch 9450 Loss 3.7854 Accuracy 0.3719\n",
            "Epoch 3 Batch 9500 Loss 3.7855 Accuracy 0.3718\n",
            "Epoch 3 Batch 9550 Loss 3.7854 Accuracy 0.3718\n",
            "Epoch 3 Batch 9600 Loss 3.7856 Accuracy 0.3718\n",
            "Epoch 3 Batch 9650 Loss 3.7855 Accuracy 0.3718\n",
            "Epoch 3 Batch 9700 Loss 3.7855 Accuracy 0.3718\n",
            "Epoch 3 Batch 9750 Loss 3.7852 Accuracy 0.3718\n",
            "Epoch 3 Batch 9800 Loss 3.7853 Accuracy 0.3718\n",
            "Epoch 3 Batch 9850 Loss 3.7852 Accuracy 0.3718\n",
            "Epoch 3 Batch 9900 Loss 3.7847 Accuracy 0.3718\n",
            "Epoch 3 Batch 9950 Loss 3.7845 Accuracy 0.3718\n",
            "Epoch 3 Batch 10000 Loss 3.7845 Accuracy 0.3718\n",
            "Epoch 3 Batch 10050 Loss 3.7843 Accuracy 0.3718\n",
            "Epoch 3 Batch 10100 Loss 3.7842 Accuracy 0.3719\n",
            "Epoch 3 Batch 10150 Loss 3.7839 Accuracy 0.3719\n",
            "Epoch 3 Batch 10200 Loss 3.7834 Accuracy 0.3719\n",
            "Epoch 3 Batch 10250 Loss 3.7830 Accuracy 0.3720\n",
            "Epoch 3 Batch 10300 Loss 3.7826 Accuracy 0.3720\n",
            "Epoch 3 Batch 10350 Loss 3.7821 Accuracy 0.3720\n",
            "Epoch 3 Batch 10400 Loss 3.7817 Accuracy 0.3721\n",
            "Epoch 3 Batch 10450 Loss 3.7813 Accuracy 0.3721\n",
            "Epoch 3 Batch 10500 Loss 3.7809 Accuracy 0.3722\n",
            "Epoch 3 Batch 10550 Loss 3.7807 Accuracy 0.3722\n",
            "Epoch 3 Batch 10600 Loss 3.7804 Accuracy 0.3722\n",
            "Epoch 3 Batch 10650 Loss 3.7803 Accuracy 0.3722\n",
            "Epoch 3 Batch 10700 Loss 3.7804 Accuracy 0.3722\n",
            "Epoch 3 Batch 10750 Loss 3.7799 Accuracy 0.3722\n",
            "Epoch 3 Batch 10800 Loss 3.7793 Accuracy 0.3722\n",
            "Epoch 3 Batch 10850 Loss 3.7787 Accuracy 0.3723\n",
            "Epoch 3 Batch 10900 Loss 3.7780 Accuracy 0.3724\n",
            "Epoch 3 Batch 10950 Loss 3.7778 Accuracy 0.3724\n",
            "Epoch 3 Batch 11000 Loss 3.7773 Accuracy 0.3724\n",
            "Epoch 3 Batch 11050 Loss 3.7768 Accuracy 0.3725\n",
            "Epoch 3 Batch 11100 Loss 3.7764 Accuracy 0.3725\n",
            "Epoch 3 Batch 11150 Loss 3.7762 Accuracy 0.3726\n",
            "Epoch 3 Batch 11200 Loss 3.7756 Accuracy 0.3726\n",
            "Epoch 3 Batch 11250 Loss 3.7750 Accuracy 0.3727\n",
            "Epoch 3 Batch 11300 Loss 3.7745 Accuracy 0.3727\n",
            "Epoch 3 Batch 11350 Loss 3.7741 Accuracy 0.3728\n",
            "Epoch 3 Batch 11400 Loss 3.7734 Accuracy 0.3728\n",
            "Epoch 3 Batch 11450 Loss 3.7731 Accuracy 0.3729\n",
            "Epoch 3 Batch 11500 Loss 3.7726 Accuracy 0.3729\n",
            "Epoch 3 Batch 11550 Loss 3.7722 Accuracy 0.3730\n",
            "Epoch 3 Batch 11600 Loss 3.7717 Accuracy 0.3730\n",
            "Epoch 3 Batch 11650 Loss 3.7715 Accuracy 0.3730\n",
            "Epoch 3 Batch 11700 Loss 3.7713 Accuracy 0.3730\n",
            "Epoch 3 Batch 11750 Loss 3.7711 Accuracy 0.3731\n",
            "Epoch 3 Batch 11800 Loss 3.7709 Accuracy 0.3731\n",
            "Epoch 3 Batch 11850 Loss 3.7706 Accuracy 0.3731\n",
            "Epoch 3 Batch 11900 Loss 3.7704 Accuracy 0.3731\n",
            "Epoch 3 Batch 11950 Loss 3.7700 Accuracy 0.3732\n",
            "Epoch 3 Batch 12000 Loss 3.7697 Accuracy 0.3732\n",
            "Epoch 3 Batch 12050 Loss 3.7694 Accuracy 0.3733\n",
            "Epoch 3 Batch 12100 Loss 3.7687 Accuracy 0.3733\n",
            "Epoch 3 Batch 12150 Loss 3.7682 Accuracy 0.3734\n",
            "Epoch 3 Batch 12200 Loss 3.7678 Accuracy 0.3734\n",
            "Epoch 3 Batch 12250 Loss 3.7675 Accuracy 0.3735\n",
            "Epoch 3 Batch 12300 Loss 3.7670 Accuracy 0.3735\n",
            "Epoch 3 Batch 12350 Loss 3.7666 Accuracy 0.3736\n",
            "Epoch 3 Batch 12400 Loss 3.7663 Accuracy 0.3736\n",
            "Epoch 3 Batch 12450 Loss 3.7658 Accuracy 0.3737\n",
            "Epoch 3 Batch 12500 Loss 3.7656 Accuracy 0.3737\n",
            "Epoch 3 Batch 12550 Loss 3.7651 Accuracy 0.3738\n",
            "Epoch 3 Batch 12600 Loss 3.7647 Accuracy 0.3738\n",
            "Epoch 3 Batch 12650 Loss 3.7640 Accuracy 0.3739\n",
            "Epoch 3 Batch 12700 Loss 3.7634 Accuracy 0.3740\n",
            "Epoch 3 Batch 12750 Loss 3.7627 Accuracy 0.3740\n",
            "Epoch 3 Batch 12800 Loss 3.7619 Accuracy 0.3741\n",
            "Epoch 3 Batch 12850 Loss 3.7608 Accuracy 0.3742\n",
            "Epoch 3 Batch 12900 Loss 3.7599 Accuracy 0.3743\n",
            "Epoch 3 Batch 12950 Loss 3.7592 Accuracy 0.3744\n",
            "Epoch 3 Batch 13000 Loss 3.7585 Accuracy 0.3744\n",
            "Epoch 3 Batch 13050 Loss 3.7579 Accuracy 0.3745\n",
            "Epoch 3 Batch 13100 Loss 3.7572 Accuracy 0.3746\n",
            "Epoch 3 Batch 13150 Loss 3.7568 Accuracy 0.3746\n",
            "Epoch 3 Batch 13200 Loss 3.7565 Accuracy 0.3746\n",
            "Epoch 3 Batch 13250 Loss 3.7560 Accuracy 0.3747\n",
            "Epoch 3 Batch 13300 Loss 3.7559 Accuracy 0.3747\n",
            "Epoch 3 Batch 13350 Loss 3.7557 Accuracy 0.3747\n",
            "Epoch 3 Batch 13400 Loss 3.7557 Accuracy 0.3747\n",
            "Epoch 3 Batch 13450 Loss 3.7557 Accuracy 0.3747\n",
            "Epoch 3 Batch 13500 Loss 3.7556 Accuracy 0.3748\n",
            "Epoch 3 Batch 13550 Loss 3.7555 Accuracy 0.3748\n",
            "Epoch 3 Batch 13600 Loss 3.7554 Accuracy 0.3748\n",
            "Epoch 3 Batch 13650 Loss 3.7550 Accuracy 0.3749\n",
            "Epoch 3 Batch 13700 Loss 3.7544 Accuracy 0.3749\n",
            "Epoch 3 Batch 13750 Loss 3.7538 Accuracy 0.3750\n",
            "Epoch 3 Batch 13800 Loss 3.7532 Accuracy 0.3750\n",
            "Epoch 3 Batch 13850 Loss 3.7526 Accuracy 0.3751\n",
            "Epoch 3 Batch 13900 Loss 3.7520 Accuracy 0.3752\n",
            "Epoch 3 Batch 13950 Loss 3.7515 Accuracy 0.3752\n",
            "Epoch 3 Batch 14000 Loss 3.7509 Accuracy 0.3753\n",
            "Epoch 3 Batch 14050 Loss 3.7502 Accuracy 0.3753\n",
            "Epoch 3 Batch 14100 Loss 3.7494 Accuracy 0.3754\n",
            "Epoch 3 Batch 14150 Loss 3.7489 Accuracy 0.3755\n",
            "Epoch 3 Batch 14200 Loss 3.7485 Accuracy 0.3755\n",
            "Epoch 3 Batch 14250 Loss 3.7479 Accuracy 0.3756\n",
            "Epoch 3 Batch 14300 Loss 3.7474 Accuracy 0.3756\n",
            "Epoch 3 Batch 14350 Loss 3.7470 Accuracy 0.3757\n",
            "Epoch 3 Batch 14400 Loss 3.7466 Accuracy 0.3757\n",
            "Epoch 3 Batch 14450 Loss 3.7462 Accuracy 0.3758\n",
            "Epoch 3 Batch 14500 Loss 3.7457 Accuracy 0.3758\n",
            "Epoch 3 Batch 14550 Loss 3.7453 Accuracy 0.3758\n",
            "Epoch 3 Batch 14600 Loss 3.7449 Accuracy 0.3759\n",
            "Epoch 3 Batch 14650 Loss 3.7446 Accuracy 0.3759\n",
            "Epoch 3 Batch 14700 Loss 3.7443 Accuracy 0.3759\n",
            "Epoch 3 Batch 14750 Loss 3.7441 Accuracy 0.3759\n",
            "Epoch 3 Batch 14800 Loss 3.7437 Accuracy 0.3760\n",
            "Epoch 3 Batch 14850 Loss 3.7433 Accuracy 0.3760\n",
            "Epoch 3 Batch 14900 Loss 3.7431 Accuracy 0.3760\n",
            "Epoch 3 Batch 14950 Loss 3.7430 Accuracy 0.3760\n",
            "Epoch 3 Batch 15000 Loss 3.7429 Accuracy 0.3760\n",
            "Epoch 3 Batch 15050 Loss 3.7427 Accuracy 0.3760\n",
            "Epoch 3 Batch 15100 Loss 3.7426 Accuracy 0.3760\n",
            "Epoch 3 Batch 15150 Loss 3.7422 Accuracy 0.3760\n",
            "Epoch 3 Batch 15200 Loss 3.7420 Accuracy 0.3761\n",
            "Epoch 3 Batch 15250 Loss 3.7418 Accuracy 0.3761\n",
            "Epoch 3 Batch 15300 Loss 3.7415 Accuracy 0.3761\n",
            "Epoch 3 Batch 15350 Loss 3.7414 Accuracy 0.3761\n",
            "Epoch 3 Batch 15400 Loss 3.7411 Accuracy 0.3761\n",
            "Epoch 3 Batch 15450 Loss 3.7410 Accuracy 0.3761\n",
            "Epoch 3 Batch 15500 Loss 3.7408 Accuracy 0.3761\n",
            "Epoch 3 Batch 15550 Loss 3.7405 Accuracy 0.3761\n",
            "Saving checkpoint for epoch 3 at /content/gdrive/My Drive/checkpoints/ckpt-17\n",
            "Epoch 3 Loss 3.7405 Accuracy 0.3761\n",
            "Time taken for 1 epoch: 3517.6514914035797 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 4.2726 Accuracy 0.3541\n",
            "Epoch 4 Batch 50 Loss 4.0566 Accuracy 0.3563\n",
            "Epoch 4 Batch 100 Loss 4.0091 Accuracy 0.3628\n",
            "Epoch 4 Batch 150 Loss 3.9763 Accuracy 0.3649\n",
            "Epoch 4 Batch 200 Loss 3.9389 Accuracy 0.3673\n",
            "Epoch 4 Batch 250 Loss 3.9139 Accuracy 0.3701\n",
            "Epoch 4 Batch 300 Loss 3.8949 Accuracy 0.3716\n",
            "Epoch 4 Batch 350 Loss 3.8776 Accuracy 0.3720\n",
            "Epoch 4 Batch 400 Loss 3.8642 Accuracy 0.3729\n",
            "Epoch 4 Batch 450 Loss 3.8535 Accuracy 0.3732\n",
            "Epoch 4 Batch 500 Loss 3.8402 Accuracy 0.3738\n",
            "Epoch 4 Batch 550 Loss 3.8260 Accuracy 0.3749\n",
            "Epoch 4 Batch 600 Loss 3.8188 Accuracy 0.3755\n",
            "Epoch 4 Batch 650 Loss 3.8099 Accuracy 0.3765\n",
            "Epoch 4 Batch 700 Loss 3.8039 Accuracy 0.3771\n",
            "Epoch 4 Batch 750 Loss 3.7966 Accuracy 0.3775\n",
            "Epoch 4 Batch 800 Loss 3.7917 Accuracy 0.3776\n",
            "Epoch 4 Batch 850 Loss 3.7857 Accuracy 0.3781\n",
            "Epoch 4 Batch 900 Loss 3.7805 Accuracy 0.3787\n",
            "Epoch 4 Batch 950 Loss 3.7745 Accuracy 0.3791\n",
            "Epoch 4 Batch 1000 Loss 3.7719 Accuracy 0.3792\n",
            "Epoch 4 Batch 1050 Loss 3.7692 Accuracy 0.3794\n",
            "Epoch 4 Batch 1100 Loss 3.7661 Accuracy 0.3795\n",
            "Epoch 4 Batch 1150 Loss 3.7643 Accuracy 0.3797\n",
            "Epoch 4 Batch 1200 Loss 3.7625 Accuracy 0.3799\n",
            "Epoch 4 Batch 1250 Loss 3.7613 Accuracy 0.3799\n",
            "Epoch 4 Batch 1300 Loss 3.7632 Accuracy 0.3798\n",
            "Epoch 4 Batch 1350 Loss 3.7619 Accuracy 0.3800\n",
            "Epoch 4 Batch 1400 Loss 3.7604 Accuracy 0.3801\n",
            "Epoch 4 Batch 1450 Loss 3.7594 Accuracy 0.3800\n",
            "Epoch 4 Batch 1500 Loss 3.7597 Accuracy 0.3800\n",
            "Epoch 4 Batch 1550 Loss 3.7602 Accuracy 0.3799\n",
            "Epoch 4 Batch 1600 Loss 3.7599 Accuracy 0.3797\n",
            "Epoch 4 Batch 1650 Loss 3.7604 Accuracy 0.3797\n",
            "Epoch 4 Batch 1700 Loss 3.7597 Accuracy 0.3796\n",
            "Epoch 4 Batch 1750 Loss 3.7587 Accuracy 0.3795\n",
            "Epoch 4 Batch 1800 Loss 3.7567 Accuracy 0.3795\n",
            "Epoch 4 Batch 1850 Loss 3.7568 Accuracy 0.3796\n",
            "Epoch 4 Batch 1900 Loss 3.7556 Accuracy 0.3797\n",
            "Epoch 4 Batch 1950 Loss 3.7564 Accuracy 0.3797\n",
            "Epoch 4 Batch 2000 Loss 3.7559 Accuracy 0.3797\n",
            "Epoch 4 Batch 2050 Loss 3.7559 Accuracy 0.3795\n",
            "Epoch 4 Batch 2100 Loss 3.7552 Accuracy 0.3796\n",
            "Epoch 4 Batch 2150 Loss 3.7550 Accuracy 0.3796\n",
            "Epoch 4 Batch 2200 Loss 3.7543 Accuracy 0.3797\n",
            "Epoch 4 Batch 2250 Loss 3.7546 Accuracy 0.3797\n",
            "Epoch 4 Batch 2300 Loss 3.7549 Accuracy 0.3796\n",
            "Epoch 4 Batch 2350 Loss 3.7554 Accuracy 0.3795\n",
            "Epoch 4 Batch 2400 Loss 3.7553 Accuracy 0.3795\n",
            "Epoch 4 Batch 2450 Loss 3.7549 Accuracy 0.3795\n",
            "Epoch 4 Batch 2500 Loss 3.7556 Accuracy 0.3795\n",
            "Epoch 4 Batch 2550 Loss 3.7556 Accuracy 0.3795\n",
            "Epoch 4 Batch 2600 Loss 3.7562 Accuracy 0.3795\n",
            "Epoch 4 Batch 2650 Loss 3.7558 Accuracy 0.3794\n",
            "Epoch 4 Batch 2700 Loss 3.7557 Accuracy 0.3794\n",
            "Epoch 4 Batch 2750 Loss 3.7568 Accuracy 0.3794\n",
            "Epoch 4 Batch 2800 Loss 3.7570 Accuracy 0.3793\n",
            "Epoch 4 Batch 2850 Loss 3.7573 Accuracy 0.3793\n",
            "Epoch 4 Batch 2900 Loss 3.7570 Accuracy 0.3792\n",
            "Epoch 4 Batch 2950 Loss 3.7576 Accuracy 0.3792\n",
            "Epoch 4 Batch 3000 Loss 3.7573 Accuracy 0.3791\n",
            "Epoch 4 Batch 3050 Loss 3.7575 Accuracy 0.3791\n",
            "Epoch 4 Batch 3100 Loss 3.7579 Accuracy 0.3791\n",
            "Epoch 4 Batch 3150 Loss 3.7578 Accuracy 0.3791\n",
            "Epoch 4 Batch 3200 Loss 3.7576 Accuracy 0.3790\n",
            "Epoch 4 Batch 3250 Loss 3.7575 Accuracy 0.3789\n",
            "Epoch 4 Batch 3300 Loss 3.7574 Accuracy 0.3789\n",
            "Epoch 4 Batch 3350 Loss 3.7570 Accuracy 0.3789\n",
            "Epoch 4 Batch 3400 Loss 3.7568 Accuracy 0.3789\n",
            "Epoch 4 Batch 3450 Loss 3.7561 Accuracy 0.3789\n",
            "Epoch 4 Batch 3500 Loss 3.7563 Accuracy 0.3788\n",
            "Epoch 4 Batch 3550 Loss 3.7568 Accuracy 0.3787\n",
            "Epoch 4 Batch 3600 Loss 3.7566 Accuracy 0.3786\n",
            "Epoch 4 Batch 3650 Loss 3.7567 Accuracy 0.3785\n",
            "Epoch 4 Batch 3700 Loss 3.7563 Accuracy 0.3785\n",
            "Epoch 4 Batch 3750 Loss 3.7558 Accuracy 0.3785\n",
            "Epoch 4 Batch 3800 Loss 3.7554 Accuracy 0.3785\n",
            "Epoch 4 Batch 3850 Loss 3.7561 Accuracy 0.3783\n",
            "Epoch 4 Batch 3900 Loss 3.7565 Accuracy 0.3782\n",
            "Epoch 4 Batch 3950 Loss 3.7562 Accuracy 0.3782\n",
            "Epoch 4 Batch 4000 Loss 3.7566 Accuracy 0.3781\n",
            "Epoch 4 Batch 4050 Loss 3.7575 Accuracy 0.3779\n",
            "Epoch 4 Batch 4100 Loss 3.7586 Accuracy 0.3778\n",
            "Epoch 4 Batch 4150 Loss 3.7594 Accuracy 0.3777\n",
            "Epoch 4 Batch 4200 Loss 3.7602 Accuracy 0.3776\n",
            "Epoch 4 Batch 4250 Loss 3.7605 Accuracy 0.3776\n",
            "Epoch 4 Batch 4300 Loss 3.7612 Accuracy 0.3775\n",
            "Epoch 4 Batch 4350 Loss 3.7611 Accuracy 0.3774\n",
            "Epoch 4 Batch 4400 Loss 3.7612 Accuracy 0.3773\n",
            "Epoch 4 Batch 4450 Loss 3.7616 Accuracy 0.3772\n",
            "Epoch 4 Batch 4500 Loss 3.7623 Accuracy 0.3771\n",
            "Epoch 4 Batch 4550 Loss 3.7635 Accuracy 0.3770\n",
            "Epoch 4 Batch 4600 Loss 3.7643 Accuracy 0.3770\n",
            "Epoch 4 Batch 4650 Loss 3.7647 Accuracy 0.3769\n",
            "Epoch 4 Batch 4700 Loss 3.7655 Accuracy 0.3768\n",
            "Epoch 4 Batch 4750 Loss 3.7662 Accuracy 0.3767\n",
            "Epoch 4 Batch 4800 Loss 3.7663 Accuracy 0.3766\n",
            "Epoch 4 Batch 4850 Loss 3.7663 Accuracy 0.3766\n",
            "Epoch 4 Batch 4900 Loss 3.7666 Accuracy 0.3766\n",
            "Epoch 4 Batch 4950 Loss 3.7668 Accuracy 0.3765\n",
            "Epoch 4 Batch 5000 Loss 3.7674 Accuracy 0.3764\n",
            "Epoch 4 Batch 5050 Loss 3.7674 Accuracy 0.3763\n",
            "Epoch 4 Batch 5100 Loss 3.7672 Accuracy 0.3763\n",
            "Epoch 4 Batch 5150 Loss 3.7667 Accuracy 0.3764\n",
            "Epoch 4 Batch 5200 Loss 3.7663 Accuracy 0.3764\n",
            "Epoch 4 Batch 5250 Loss 3.7663 Accuracy 0.3764\n",
            "Epoch 4 Batch 5300 Loss 3.7664 Accuracy 0.3763\n",
            "Epoch 4 Batch 5350 Loss 3.7661 Accuracy 0.3763\n",
            "Epoch 4 Batch 5400 Loss 3.7666 Accuracy 0.3763\n",
            "Epoch 4 Batch 5450 Loss 3.7672 Accuracy 0.3761\n",
            "Epoch 4 Batch 5500 Loss 3.7674 Accuracy 0.3761\n",
            "Epoch 4 Batch 5550 Loss 3.7681 Accuracy 0.3760\n",
            "Epoch 4 Batch 5600 Loss 3.7687 Accuracy 0.3760\n",
            "Epoch 4 Batch 5650 Loss 3.7692 Accuracy 0.3759\n",
            "Epoch 4 Batch 5700 Loss 3.7697 Accuracy 0.3758\n",
            "Epoch 4 Batch 5750 Loss 3.7705 Accuracy 0.3756\n",
            "Epoch 4 Batch 5800 Loss 3.7710 Accuracy 0.3755\n",
            "Epoch 4 Batch 5850 Loss 3.7713 Accuracy 0.3755\n",
            "Epoch 4 Batch 5900 Loss 3.7718 Accuracy 0.3754\n",
            "Epoch 4 Batch 5950 Loss 3.7723 Accuracy 0.3753\n",
            "Epoch 4 Batch 6000 Loss 3.7732 Accuracy 0.3752\n",
            "Epoch 4 Batch 6050 Loss 3.7737 Accuracy 0.3752\n",
            "Epoch 4 Batch 6100 Loss 3.7743 Accuracy 0.3751\n",
            "Epoch 4 Batch 6150 Loss 3.7748 Accuracy 0.3750\n",
            "Epoch 4 Batch 6200 Loss 3.7753 Accuracy 0.3749\n",
            "Epoch 4 Batch 6250 Loss 3.7762 Accuracy 0.3748\n",
            "Epoch 4 Batch 6300 Loss 3.7767 Accuracy 0.3747\n",
            "Epoch 4 Batch 6350 Loss 3.7770 Accuracy 0.3746\n",
            "Epoch 4 Batch 6400 Loss 3.7774 Accuracy 0.3746\n",
            "Epoch 4 Batch 6450 Loss 3.7782 Accuracy 0.3745\n",
            "Epoch 4 Batch 6500 Loss 3.7788 Accuracy 0.3744\n",
            "Epoch 4 Batch 6550 Loss 3.7793 Accuracy 0.3743\n",
            "Epoch 4 Batch 6600 Loss 3.7792 Accuracy 0.3742\n",
            "Epoch 4 Batch 6650 Loss 3.7792 Accuracy 0.3742\n",
            "Epoch 4 Batch 6700 Loss 3.7793 Accuracy 0.3741\n",
            "Epoch 4 Batch 6750 Loss 3.7793 Accuracy 0.3741\n",
            "Epoch 4 Batch 6800 Loss 3.7796 Accuracy 0.3740\n",
            "Epoch 4 Batch 6850 Loss 3.7800 Accuracy 0.3739\n",
            "Epoch 4 Batch 6900 Loss 3.7802 Accuracy 0.3739\n",
            "Epoch 4 Batch 6950 Loss 3.7806 Accuracy 0.3737\n",
            "Epoch 4 Batch 7000 Loss 3.7808 Accuracy 0.3737\n",
            "Epoch 4 Batch 7050 Loss 3.7810 Accuracy 0.3736\n",
            "Epoch 4 Batch 7100 Loss 3.7816 Accuracy 0.3735\n",
            "Epoch 4 Batch 7150 Loss 3.7818 Accuracy 0.3735\n",
            "Epoch 4 Batch 7200 Loss 3.7818 Accuracy 0.3735\n",
            "Epoch 4 Batch 7250 Loss 3.7823 Accuracy 0.3734\n",
            "Epoch 4 Batch 7300 Loss 3.7825 Accuracy 0.3733\n",
            "Epoch 4 Batch 7350 Loss 3.7827 Accuracy 0.3733\n",
            "Epoch 4 Batch 7400 Loss 3.7829 Accuracy 0.3732\n",
            "Epoch 4 Batch 7450 Loss 3.7830 Accuracy 0.3732\n",
            "Epoch 4 Batch 7500 Loss 3.7830 Accuracy 0.3731\n",
            "Epoch 4 Batch 7550 Loss 3.7831 Accuracy 0.3731\n",
            "Epoch 4 Batch 7600 Loss 3.7836 Accuracy 0.3730\n",
            "Epoch 4 Batch 7650 Loss 3.7839 Accuracy 0.3730\n",
            "Epoch 4 Batch 7700 Loss 3.7840 Accuracy 0.3729\n",
            "Epoch 4 Batch 7750 Loss 3.7839 Accuracy 0.3729\n",
            "Epoch 4 Batch 7800 Loss 3.7841 Accuracy 0.3729\n",
            "Epoch 4 Batch 7850 Loss 3.7842 Accuracy 0.3728\n",
            "Epoch 4 Batch 7900 Loss 3.7847 Accuracy 0.3728\n",
            "Epoch 4 Batch 7950 Loss 3.7848 Accuracy 0.3727\n",
            "Epoch 4 Batch 8000 Loss 3.7849 Accuracy 0.3727\n",
            "Epoch 4 Batch 8050 Loss 3.7853 Accuracy 0.3726\n",
            "Epoch 4 Batch 8100 Loss 3.7852 Accuracy 0.3727\n",
            "Epoch 4 Batch 8150 Loss 3.7852 Accuracy 0.3726\n",
            "Epoch 4 Batch 8200 Loss 3.7851 Accuracy 0.3726\n",
            "Epoch 4 Batch 8250 Loss 3.7851 Accuracy 0.3726\n",
            "Epoch 4 Batch 8300 Loss 3.7853 Accuracy 0.3726\n",
            "Epoch 4 Batch 8350 Loss 3.7854 Accuracy 0.3725\n",
            "Epoch 4 Batch 8400 Loss 3.7857 Accuracy 0.3725\n",
            "Epoch 4 Batch 8450 Loss 3.7860 Accuracy 0.3725\n",
            "Epoch 4 Batch 8500 Loss 3.7859 Accuracy 0.3724\n",
            "Epoch 4 Batch 8550 Loss 3.7859 Accuracy 0.3724\n",
            "Epoch 4 Batch 8600 Loss 3.7859 Accuracy 0.3724\n",
            "Epoch 4 Batch 8650 Loss 3.7860 Accuracy 0.3724\n",
            "Epoch 4 Batch 8700 Loss 3.7860 Accuracy 0.3723\n",
            "Epoch 4 Batch 8750 Loss 3.7863 Accuracy 0.3723\n",
            "Epoch 4 Batch 8800 Loss 3.7862 Accuracy 0.3723\n",
            "Epoch 4 Batch 8850 Loss 3.7861 Accuracy 0.3723\n",
            "Epoch 4 Batch 8900 Loss 3.7861 Accuracy 0.3723\n",
            "Epoch 4 Batch 8950 Loss 3.7860 Accuracy 0.3723\n",
            "Epoch 4 Batch 9000 Loss 3.7861 Accuracy 0.3722\n",
            "Epoch 4 Batch 9050 Loss 3.7862 Accuracy 0.3722\n",
            "Epoch 4 Batch 9100 Loss 3.7861 Accuracy 0.3722\n",
            "Epoch 4 Batch 9150 Loss 3.7860 Accuracy 0.3722\n",
            "Epoch 4 Batch 9200 Loss 3.7859 Accuracy 0.3722\n",
            "Epoch 4 Batch 9250 Loss 3.7858 Accuracy 0.3722\n",
            "Epoch 4 Batch 9300 Loss 3.7858 Accuracy 0.3722\n",
            "Epoch 4 Batch 9350 Loss 3.7860 Accuracy 0.3722\n",
            "Epoch 4 Batch 9400 Loss 3.7862 Accuracy 0.3721\n",
            "Epoch 4 Batch 9450 Loss 3.7863 Accuracy 0.3721\n",
            "Epoch 4 Batch 9500 Loss 3.7861 Accuracy 0.3722\n",
            "Epoch 4 Batch 9550 Loss 3.7860 Accuracy 0.3721\n",
            "Epoch 4 Batch 9600 Loss 3.7859 Accuracy 0.3721\n",
            "Epoch 4 Batch 9650 Loss 3.7860 Accuracy 0.3721\n",
            "Epoch 4 Batch 9700 Loss 3.7858 Accuracy 0.3721\n",
            "Epoch 4 Batch 9750 Loss 3.7857 Accuracy 0.3722\n",
            "Epoch 4 Batch 9800 Loss 3.7856 Accuracy 0.3721\n",
            "Epoch 4 Batch 9850 Loss 3.7855 Accuracy 0.3721\n",
            "Epoch 4 Batch 9900 Loss 3.7852 Accuracy 0.3721\n",
            "Epoch 4 Batch 9950 Loss 3.7848 Accuracy 0.3722\n",
            "Epoch 4 Batch 10000 Loss 3.7849 Accuracy 0.3722\n",
            "Epoch 4 Batch 10050 Loss 3.7844 Accuracy 0.3722\n",
            "Epoch 4 Batch 10100 Loss 3.7841 Accuracy 0.3722\n",
            "Epoch 4 Batch 10150 Loss 3.7840 Accuracy 0.3722\n",
            "Epoch 4 Batch 10200 Loss 3.7836 Accuracy 0.3723\n",
            "Epoch 4 Batch 10250 Loss 3.7832 Accuracy 0.3723\n",
            "Epoch 4 Batch 10300 Loss 3.7826 Accuracy 0.3724\n",
            "Epoch 4 Batch 10350 Loss 3.7823 Accuracy 0.3724\n",
            "Epoch 4 Batch 10400 Loss 3.7822 Accuracy 0.3724\n",
            "Epoch 4 Batch 10450 Loss 3.7819 Accuracy 0.3724\n",
            "Epoch 4 Batch 10500 Loss 3.7818 Accuracy 0.3724\n",
            "Epoch 4 Batch 10550 Loss 3.7814 Accuracy 0.3725\n",
            "Epoch 4 Batch 10600 Loss 3.7811 Accuracy 0.3725\n",
            "Epoch 4 Batch 10650 Loss 3.7810 Accuracy 0.3725\n",
            "Epoch 4 Batch 10700 Loss 3.7806 Accuracy 0.3725\n",
            "Epoch 4 Batch 10750 Loss 3.7806 Accuracy 0.3725\n",
            "Epoch 4 Batch 10800 Loss 3.7803 Accuracy 0.3725\n",
            "Epoch 4 Batch 10850 Loss 3.7796 Accuracy 0.3726\n",
            "Epoch 4 Batch 10900 Loss 3.7791 Accuracy 0.3726\n",
            "Epoch 4 Batch 10950 Loss 3.7786 Accuracy 0.3727\n",
            "Epoch 4 Batch 11000 Loss 3.7779 Accuracy 0.3727\n",
            "Epoch 4 Batch 11050 Loss 3.7773 Accuracy 0.3728\n",
            "Epoch 4 Batch 11100 Loss 3.7768 Accuracy 0.3729\n",
            "Epoch 4 Batch 11150 Loss 3.7765 Accuracy 0.3729\n",
            "Epoch 4 Batch 11200 Loss 3.7761 Accuracy 0.3730\n",
            "Epoch 4 Batch 11250 Loss 3.7757 Accuracy 0.3730\n",
            "Epoch 4 Batch 11300 Loss 3.7751 Accuracy 0.3731\n",
            "Epoch 4 Batch 11350 Loss 3.7743 Accuracy 0.3731\n",
            "Epoch 4 Batch 11400 Loss 3.7739 Accuracy 0.3732\n",
            "Epoch 4 Batch 11450 Loss 3.7733 Accuracy 0.3732\n",
            "Epoch 4 Batch 11500 Loss 3.7728 Accuracy 0.3732\n",
            "Epoch 4 Batch 11550 Loss 3.7724 Accuracy 0.3733\n",
            "Epoch 4 Batch 11600 Loss 3.7723 Accuracy 0.3733\n",
            "Epoch 4 Batch 11650 Loss 3.7721 Accuracy 0.3733\n",
            "Epoch 4 Batch 11700 Loss 3.7719 Accuracy 0.3733\n",
            "Epoch 4 Batch 11750 Loss 3.7716 Accuracy 0.3733\n",
            "Epoch 4 Batch 11800 Loss 3.7713 Accuracy 0.3734\n",
            "Epoch 4 Batch 11850 Loss 3.7708 Accuracy 0.3734\n",
            "Epoch 4 Batch 11900 Loss 3.7704 Accuracy 0.3735\n",
            "Epoch 4 Batch 11950 Loss 3.7699 Accuracy 0.3735\n",
            "Epoch 4 Batch 12000 Loss 3.7695 Accuracy 0.3736\n",
            "Epoch 4 Batch 12050 Loss 3.7689 Accuracy 0.3736\n",
            "Epoch 4 Batch 12100 Loss 3.7686 Accuracy 0.3737\n",
            "Epoch 4 Batch 12150 Loss 3.7683 Accuracy 0.3737\n",
            "Epoch 4 Batch 12200 Loss 3.7679 Accuracy 0.3737\n",
            "Epoch 4 Batch 12250 Loss 3.7676 Accuracy 0.3737\n",
            "Epoch 4 Batch 12300 Loss 3.7671 Accuracy 0.3738\n",
            "Epoch 4 Batch 12350 Loss 3.7665 Accuracy 0.3739\n",
            "Epoch 4 Batch 12400 Loss 3.7663 Accuracy 0.3739\n",
            "Epoch 4 Batch 12450 Loss 3.7661 Accuracy 0.3739\n",
            "Epoch 4 Batch 12500 Loss 3.7658 Accuracy 0.3740\n",
            "Epoch 4 Batch 12550 Loss 3.7653 Accuracy 0.3740\n",
            "Epoch 4 Batch 12600 Loss 3.7646 Accuracy 0.3741\n",
            "Epoch 4 Batch 12650 Loss 3.7640 Accuracy 0.3741\n",
            "Epoch 4 Batch 12700 Loss 3.7634 Accuracy 0.3742\n",
            "Epoch 4 Batch 12750 Loss 3.7624 Accuracy 0.3743\n",
            "Epoch 4 Batch 12800 Loss 3.7616 Accuracy 0.3744\n",
            "Epoch 4 Batch 12850 Loss 3.7607 Accuracy 0.3745\n",
            "Epoch 4 Batch 12900 Loss 3.7597 Accuracy 0.3746\n",
            "Epoch 4 Batch 12950 Loss 3.7593 Accuracy 0.3746\n",
            "Epoch 4 Batch 13000 Loss 3.7587 Accuracy 0.3747\n",
            "Epoch 4 Batch 13050 Loss 3.7579 Accuracy 0.3748\n",
            "Epoch 4 Batch 13100 Loss 3.7573 Accuracy 0.3749\n",
            "Epoch 4 Batch 13150 Loss 3.7570 Accuracy 0.3749\n",
            "Epoch 4 Batch 13200 Loss 3.7567 Accuracy 0.3749\n",
            "Epoch 4 Batch 13250 Loss 3.7565 Accuracy 0.3750\n",
            "Epoch 4 Batch 13300 Loss 3.7561 Accuracy 0.3750\n",
            "Epoch 4 Batch 13350 Loss 3.7557 Accuracy 0.3751\n",
            "Epoch 4 Batch 13400 Loss 3.7557 Accuracy 0.3751\n",
            "Epoch 4 Batch 13450 Loss 3.7554 Accuracy 0.3751\n",
            "Epoch 4 Batch 13500 Loss 3.7553 Accuracy 0.3751\n",
            "Epoch 4 Batch 13550 Loss 3.7552 Accuracy 0.3751\n",
            "Epoch 4 Batch 13600 Loss 3.7551 Accuracy 0.3751\n",
            "Epoch 4 Batch 13650 Loss 3.7548 Accuracy 0.3752\n",
            "Epoch 4 Batch 13700 Loss 3.7544 Accuracy 0.3752\n",
            "Epoch 4 Batch 13750 Loss 3.7536 Accuracy 0.3753\n",
            "Epoch 4 Batch 13800 Loss 3.7532 Accuracy 0.3753\n",
            "Epoch 4 Batch 13850 Loss 3.7525 Accuracy 0.3754\n",
            "Epoch 4 Batch 13900 Loss 3.7518 Accuracy 0.3755\n",
            "Epoch 4 Batch 13950 Loss 3.7512 Accuracy 0.3755\n",
            "Epoch 4 Batch 14000 Loss 3.7505 Accuracy 0.3756\n",
            "Epoch 4 Batch 14050 Loss 3.7498 Accuracy 0.3757\n",
            "Epoch 4 Batch 14100 Loss 3.7492 Accuracy 0.3757\n",
            "Epoch 4 Batch 14150 Loss 3.7487 Accuracy 0.3758\n",
            "Epoch 4 Batch 14200 Loss 3.7483 Accuracy 0.3758\n",
            "Epoch 4 Batch 14250 Loss 3.7478 Accuracy 0.3759\n",
            "Epoch 4 Batch 14300 Loss 3.7477 Accuracy 0.3759\n",
            "Epoch 4 Batch 14350 Loss 3.7473 Accuracy 0.3759\n",
            "Epoch 4 Batch 14400 Loss 3.7469 Accuracy 0.3760\n",
            "Epoch 4 Batch 14450 Loss 3.7465 Accuracy 0.3760\n",
            "Epoch 4 Batch 14500 Loss 3.7462 Accuracy 0.3760\n",
            "Epoch 4 Batch 14550 Loss 3.7457 Accuracy 0.3760\n",
            "Epoch 4 Batch 14600 Loss 3.7454 Accuracy 0.3761\n",
            "Epoch 4 Batch 14650 Loss 3.7450 Accuracy 0.3761\n",
            "Epoch 4 Batch 14700 Loss 3.7447 Accuracy 0.3761\n",
            "Epoch 4 Batch 14750 Loss 3.7444 Accuracy 0.3761\n",
            "Epoch 4 Batch 14800 Loss 3.7440 Accuracy 0.3761\n",
            "Epoch 4 Batch 14850 Loss 3.7438 Accuracy 0.3762\n",
            "Epoch 4 Batch 14900 Loss 3.7435 Accuracy 0.3762\n",
            "Epoch 4 Batch 14950 Loss 3.7434 Accuracy 0.3762\n",
            "Epoch 4 Batch 15000 Loss 3.7433 Accuracy 0.3762\n",
            "Epoch 4 Batch 15050 Loss 3.7430 Accuracy 0.3762\n",
            "Epoch 4 Batch 15100 Loss 3.7427 Accuracy 0.3762\n",
            "Epoch 4 Batch 15150 Loss 3.7426 Accuracy 0.3762\n",
            "Epoch 4 Batch 15200 Loss 3.7424 Accuracy 0.3763\n",
            "Epoch 4 Batch 15250 Loss 3.7422 Accuracy 0.3763\n",
            "Epoch 4 Batch 15300 Loss 3.7419 Accuracy 0.3763\n",
            "Epoch 4 Batch 15350 Loss 3.7417 Accuracy 0.3763\n",
            "Epoch 4 Batch 15400 Loss 3.7414 Accuracy 0.3763\n",
            "Epoch 4 Batch 15450 Loss 3.7411 Accuracy 0.3763\n",
            "Epoch 4 Batch 15500 Loss 3.7407 Accuracy 0.3764\n",
            "Epoch 4 Batch 15550 Loss 3.7405 Accuracy 0.3764\n",
            "Epoch 4 Loss 3.7404 Accuracy 0.3764\n",
            "Time taken for 1 epoch: 3518.8828723430634 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 4.3929 Accuracy 0.3317\n",
            "Epoch 5 Batch 50 Loss 4.0321 Accuracy 0.3667\n",
            "Epoch 5 Batch 100 Loss 4.0099 Accuracy 0.3663\n",
            "Epoch 5 Batch 150 Loss 3.9881 Accuracy 0.3658\n",
            "Epoch 5 Batch 200 Loss 3.9483 Accuracy 0.3682\n",
            "Epoch 5 Batch 250 Loss 3.9230 Accuracy 0.3703\n",
            "Epoch 5 Batch 300 Loss 3.9028 Accuracy 0.3715\n",
            "Epoch 5 Batch 350 Loss 3.8884 Accuracy 0.3723\n",
            "Epoch 5 Batch 400 Loss 3.8701 Accuracy 0.3737\n",
            "Epoch 5 Batch 450 Loss 3.8534 Accuracy 0.3749\n",
            "Epoch 5 Batch 500 Loss 3.8409 Accuracy 0.3756\n",
            "Epoch 5 Batch 550 Loss 3.8265 Accuracy 0.3767\n",
            "Epoch 5 Batch 600 Loss 3.8184 Accuracy 0.3770\n",
            "Epoch 5 Batch 650 Loss 3.8065 Accuracy 0.3782\n",
            "Epoch 5 Batch 700 Loss 3.7976 Accuracy 0.3784\n",
            "Epoch 5 Batch 750 Loss 3.7928 Accuracy 0.3784\n",
            "Epoch 5 Batch 800 Loss 3.7887 Accuracy 0.3785\n",
            "Epoch 5 Batch 850 Loss 3.7820 Accuracy 0.3790\n",
            "Epoch 5 Batch 900 Loss 3.7744 Accuracy 0.3795\n",
            "Epoch 5 Batch 950 Loss 3.7683 Accuracy 0.3801\n",
            "Epoch 5 Batch 1000 Loss 3.7622 Accuracy 0.3804\n",
            "Epoch 5 Batch 1050 Loss 3.7585 Accuracy 0.3808\n",
            "Epoch 5 Batch 1100 Loss 3.7602 Accuracy 0.3807\n",
            "Epoch 5 Batch 1150 Loss 3.7583 Accuracy 0.3808\n",
            "Epoch 5 Batch 1200 Loss 3.7556 Accuracy 0.3810\n",
            "Epoch 5 Batch 1250 Loss 3.7548 Accuracy 0.3812\n",
            "Epoch 5 Batch 1300 Loss 3.7551 Accuracy 0.3810\n",
            "Epoch 5 Batch 1350 Loss 3.7577 Accuracy 0.3808\n",
            "Epoch 5 Batch 1400 Loss 3.7600 Accuracy 0.3805\n",
            "Epoch 5 Batch 1450 Loss 3.7609 Accuracy 0.3803\n",
            "Epoch 5 Batch 1500 Loss 3.7608 Accuracy 0.3804\n",
            "Epoch 5 Batch 1550 Loss 3.7607 Accuracy 0.3804\n",
            "Epoch 5 Batch 1600 Loss 3.7595 Accuracy 0.3804\n",
            "Epoch 5 Batch 1650 Loss 3.7588 Accuracy 0.3804\n",
            "Epoch 5 Batch 1700 Loss 3.7555 Accuracy 0.3805\n",
            "Epoch 5 Batch 1750 Loss 3.7536 Accuracy 0.3805\n",
            "Epoch 5 Batch 1800 Loss 3.7529 Accuracy 0.3804\n",
            "Epoch 5 Batch 1850 Loss 3.7522 Accuracy 0.3804\n",
            "Epoch 5 Batch 1900 Loss 3.7513 Accuracy 0.3804\n",
            "Epoch 5 Batch 1950 Loss 3.7502 Accuracy 0.3805\n",
            "Epoch 5 Batch 2000 Loss 3.7503 Accuracy 0.3805\n",
            "Epoch 5 Batch 2050 Loss 3.7504 Accuracy 0.3804\n",
            "Epoch 5 Batch 2100 Loss 3.7504 Accuracy 0.3805\n",
            "Epoch 5 Batch 2150 Loss 3.7518 Accuracy 0.3803\n",
            "Epoch 5 Batch 2200 Loss 3.7510 Accuracy 0.3804\n",
            "Epoch 5 Batch 2250 Loss 3.7514 Accuracy 0.3802\n",
            "Epoch 5 Batch 2300 Loss 3.7508 Accuracy 0.3802\n",
            "Epoch 5 Batch 2350 Loss 3.7517 Accuracy 0.3802\n",
            "Epoch 5 Batch 2400 Loss 3.7515 Accuracy 0.3803\n",
            "Epoch 5 Batch 2450 Loss 3.7524 Accuracy 0.3802\n",
            "Epoch 5 Batch 2500 Loss 3.7527 Accuracy 0.3801\n",
            "Epoch 5 Batch 2550 Loss 3.7527 Accuracy 0.3802\n",
            "Epoch 5 Batch 2600 Loss 3.7532 Accuracy 0.3800\n",
            "Epoch 5 Batch 2650 Loss 3.7528 Accuracy 0.3800\n",
            "Epoch 5 Batch 2700 Loss 3.7518 Accuracy 0.3801\n",
            "Epoch 5 Batch 2750 Loss 3.7524 Accuracy 0.3800\n",
            "Epoch 5 Batch 2800 Loss 3.7524 Accuracy 0.3798\n",
            "Epoch 5 Batch 2850 Loss 3.7523 Accuracy 0.3798\n",
            "Epoch 5 Batch 2900 Loss 3.7529 Accuracy 0.3797\n",
            "Epoch 5 Batch 2950 Loss 3.7532 Accuracy 0.3796\n",
            "Epoch 5 Batch 3000 Loss 3.7537 Accuracy 0.3796\n",
            "Epoch 5 Batch 3050 Loss 3.7538 Accuracy 0.3795\n",
            "Epoch 5 Batch 3100 Loss 3.7539 Accuracy 0.3795\n",
            "Epoch 5 Batch 3150 Loss 3.7541 Accuracy 0.3794\n",
            "Epoch 5 Batch 3200 Loss 3.7541 Accuracy 0.3793\n",
            "Epoch 5 Batch 3250 Loss 3.7542 Accuracy 0.3793\n",
            "Epoch 5 Batch 3300 Loss 3.7536 Accuracy 0.3793\n",
            "Epoch 5 Batch 3350 Loss 3.7539 Accuracy 0.3792\n",
            "Epoch 5 Batch 3400 Loss 3.7535 Accuracy 0.3792\n",
            "Epoch 5 Batch 3450 Loss 3.7530 Accuracy 0.3791\n",
            "Epoch 5 Batch 3500 Loss 3.7529 Accuracy 0.3791\n",
            "Epoch 5 Batch 3550 Loss 3.7524 Accuracy 0.3791\n",
            "Epoch 5 Batch 3600 Loss 3.7523 Accuracy 0.3791\n",
            "Epoch 5 Batch 3650 Loss 3.7524 Accuracy 0.3791\n",
            "Epoch 5 Batch 3700 Loss 3.7524 Accuracy 0.3790\n",
            "Epoch 5 Batch 3750 Loss 3.7529 Accuracy 0.3789\n",
            "Epoch 5 Batch 3800 Loss 3.7533 Accuracy 0.3788\n",
            "Epoch 5 Batch 3850 Loss 3.7538 Accuracy 0.3786\n",
            "Epoch 5 Batch 3900 Loss 3.7542 Accuracy 0.3785\n",
            "Epoch 5 Batch 3950 Loss 3.7539 Accuracy 0.3785\n",
            "Epoch 5 Batch 4000 Loss 3.7545 Accuracy 0.3783\n",
            "Epoch 5 Batch 4050 Loss 3.7549 Accuracy 0.3783\n",
            "Epoch 5 Batch 4100 Loss 3.7557 Accuracy 0.3781\n",
            "Epoch 5 Batch 4150 Loss 3.7562 Accuracy 0.3781\n",
            "Epoch 5 Batch 4200 Loss 3.7570 Accuracy 0.3780\n",
            "Epoch 5 Batch 4250 Loss 3.7574 Accuracy 0.3779\n",
            "Epoch 5 Batch 4300 Loss 3.7577 Accuracy 0.3778\n",
            "Epoch 5 Batch 4350 Loss 3.7576 Accuracy 0.3778\n",
            "Epoch 5 Batch 4400 Loss 3.7577 Accuracy 0.3777\n",
            "Epoch 5 Batch 4450 Loss 3.7584 Accuracy 0.3776\n",
            "Epoch 5 Batch 4500 Loss 3.7589 Accuracy 0.3775\n",
            "Epoch 5 Batch 4550 Loss 3.7598 Accuracy 0.3774\n",
            "Epoch 5 Batch 4600 Loss 3.7610 Accuracy 0.3773\n",
            "Epoch 5 Batch 4650 Loss 3.7616 Accuracy 0.3772\n",
            "Epoch 5 Batch 4700 Loss 3.7617 Accuracy 0.3772\n",
            "Epoch 5 Batch 4750 Loss 3.7620 Accuracy 0.3771\n",
            "Epoch 5 Batch 4800 Loss 3.7621 Accuracy 0.3771\n",
            "Epoch 5 Batch 4850 Loss 3.7626 Accuracy 0.3770\n",
            "Epoch 5 Batch 4900 Loss 3.7629 Accuracy 0.3769\n",
            "Epoch 5 Batch 4950 Loss 3.7631 Accuracy 0.3769\n",
            "Epoch 5 Batch 5000 Loss 3.7634 Accuracy 0.3768\n",
            "Epoch 5 Batch 5050 Loss 3.7634 Accuracy 0.3768\n",
            "Epoch 5 Batch 5100 Loss 3.7634 Accuracy 0.3767\n",
            "Epoch 5 Batch 5150 Loss 3.7632 Accuracy 0.3767\n",
            "Epoch 5 Batch 5200 Loss 3.7630 Accuracy 0.3767\n",
            "Epoch 5 Batch 5250 Loss 3.7630 Accuracy 0.3766\n",
            "Epoch 5 Batch 5300 Loss 3.7628 Accuracy 0.3766\n",
            "Epoch 5 Batch 5350 Loss 3.7629 Accuracy 0.3766\n",
            "Epoch 5 Batch 5400 Loss 3.7634 Accuracy 0.3766\n",
            "Epoch 5 Batch 5450 Loss 3.7635 Accuracy 0.3765\n",
            "Epoch 5 Batch 5500 Loss 3.7637 Accuracy 0.3765\n",
            "Epoch 5 Batch 5550 Loss 3.7644 Accuracy 0.3764\n",
            "Epoch 5 Batch 5600 Loss 3.7648 Accuracy 0.3763\n",
            "Epoch 5 Batch 5650 Loss 3.7650 Accuracy 0.3763\n",
            "Epoch 5 Batch 5700 Loss 3.7654 Accuracy 0.3762\n",
            "Epoch 5 Batch 5750 Loss 3.7657 Accuracy 0.3761\n",
            "Epoch 5 Batch 5800 Loss 3.7663 Accuracy 0.3761\n",
            "Epoch 5 Batch 5850 Loss 3.7671 Accuracy 0.3760\n",
            "Epoch 5 Batch 5900 Loss 3.7679 Accuracy 0.3758\n",
            "Epoch 5 Batch 5950 Loss 3.7690 Accuracy 0.3757\n",
            "Epoch 5 Batch 6000 Loss 3.7695 Accuracy 0.3756\n",
            "Epoch 5 Batch 6050 Loss 3.7700 Accuracy 0.3755\n",
            "Epoch 5 Batch 6100 Loss 3.7705 Accuracy 0.3754\n",
            "Epoch 5 Batch 6150 Loss 3.7712 Accuracy 0.3753\n",
            "Epoch 5 Batch 6200 Loss 3.7714 Accuracy 0.3752\n",
            "Epoch 5 Batch 6250 Loss 3.7721 Accuracy 0.3751\n",
            "Epoch 5 Batch 6300 Loss 3.7725 Accuracy 0.3751\n",
            "Epoch 5 Batch 6350 Loss 3.7730 Accuracy 0.3750\n",
            "Epoch 5 Batch 6400 Loss 3.7737 Accuracy 0.3748\n",
            "Epoch 5 Batch 6450 Loss 3.7745 Accuracy 0.3747\n",
            "Epoch 5 Batch 6500 Loss 3.7748 Accuracy 0.3747\n",
            "Epoch 5 Batch 6550 Loss 3.7752 Accuracy 0.3746\n",
            "Epoch 5 Batch 6600 Loss 3.7753 Accuracy 0.3746\n",
            "Epoch 5 Batch 6650 Loss 3.7754 Accuracy 0.3746\n",
            "Epoch 5 Batch 6700 Loss 3.7758 Accuracy 0.3745\n",
            "Epoch 5 Batch 6750 Loss 3.7759 Accuracy 0.3744\n",
            "Epoch 5 Batch 6800 Loss 3.7760 Accuracy 0.3743\n",
            "Epoch 5 Batch 6850 Loss 3.7765 Accuracy 0.3743\n",
            "Epoch 5 Batch 6900 Loss 3.7767 Accuracy 0.3742\n",
            "Epoch 5 Batch 6950 Loss 3.7770 Accuracy 0.3742\n",
            "Epoch 5 Batch 7000 Loss 3.7775 Accuracy 0.3741\n",
            "Epoch 5 Batch 7050 Loss 3.7777 Accuracy 0.3740\n",
            "Epoch 5 Batch 7100 Loss 3.7780 Accuracy 0.3739\n",
            "Epoch 5 Batch 7150 Loss 3.7782 Accuracy 0.3739\n",
            "Epoch 5 Batch 7200 Loss 3.7786 Accuracy 0.3738\n",
            "Epoch 5 Batch 7250 Loss 3.7786 Accuracy 0.3738\n",
            "Epoch 5 Batch 7300 Loss 3.7792 Accuracy 0.3737\n",
            "Epoch 5 Batch 7350 Loss 3.7795 Accuracy 0.3736\n",
            "Epoch 5 Batch 7400 Loss 3.7798 Accuracy 0.3736\n",
            "Epoch 5 Batch 7450 Loss 3.7803 Accuracy 0.3735\n",
            "Epoch 5 Batch 7500 Loss 3.7805 Accuracy 0.3734\n",
            "Epoch 5 Batch 7550 Loss 3.7808 Accuracy 0.3734\n",
            "Epoch 5 Batch 7600 Loss 3.7812 Accuracy 0.3733\n",
            "Epoch 5 Batch 7650 Loss 3.7817 Accuracy 0.3732\n",
            "Epoch 5 Batch 7700 Loss 3.7819 Accuracy 0.3732\n",
            "Epoch 5 Batch 7750 Loss 3.7821 Accuracy 0.3731\n",
            "Epoch 5 Batch 7800 Loss 3.7824 Accuracy 0.3731\n",
            "Epoch 5 Batch 7850 Loss 3.7824 Accuracy 0.3730\n",
            "Epoch 5 Batch 7900 Loss 3.7825 Accuracy 0.3730\n",
            "Epoch 5 Batch 7950 Loss 3.7825 Accuracy 0.3730\n",
            "Epoch 5 Batch 8000 Loss 3.7827 Accuracy 0.3730\n",
            "Epoch 5 Batch 8050 Loss 3.7827 Accuracy 0.3730\n",
            "Epoch 5 Batch 8100 Loss 3.7829 Accuracy 0.3729\n",
            "Epoch 5 Batch 8150 Loss 3.7830 Accuracy 0.3729\n",
            "Epoch 5 Batch 8200 Loss 3.7830 Accuracy 0.3729\n",
            "Epoch 5 Batch 8250 Loss 3.7831 Accuracy 0.3729\n",
            "Epoch 5 Batch 8300 Loss 3.7833 Accuracy 0.3728\n",
            "Epoch 5 Batch 8350 Loss 3.7836 Accuracy 0.3728\n",
            "Epoch 5 Batch 8400 Loss 3.7837 Accuracy 0.3727\n",
            "Epoch 5 Batch 8450 Loss 3.7839 Accuracy 0.3727\n",
            "Epoch 5 Batch 8500 Loss 3.7840 Accuracy 0.3726\n",
            "Epoch 5 Batch 8550 Loss 3.7841 Accuracy 0.3726\n",
            "Epoch 5 Batch 8600 Loss 3.7841 Accuracy 0.3726\n",
            "Epoch 5 Batch 8650 Loss 3.7840 Accuracy 0.3726\n",
            "Epoch 5 Batch 8700 Loss 3.7838 Accuracy 0.3726\n",
            "Epoch 5 Batch 8750 Loss 3.7839 Accuracy 0.3725\n",
            "Epoch 5 Batch 8800 Loss 3.7838 Accuracy 0.3725\n",
            "Epoch 5 Batch 8850 Loss 3.7838 Accuracy 0.3725\n",
            "Epoch 5 Batch 8900 Loss 3.7835 Accuracy 0.3725\n",
            "Epoch 5 Batch 8950 Loss 3.7834 Accuracy 0.3725\n",
            "Epoch 5 Batch 9000 Loss 3.7833 Accuracy 0.3725\n",
            "Epoch 5 Batch 9050 Loss 3.7835 Accuracy 0.3725\n",
            "Epoch 5 Batch 9100 Loss 3.7836 Accuracy 0.3724\n",
            "Epoch 5 Batch 9150 Loss 3.7838 Accuracy 0.3724\n",
            "Epoch 5 Batch 9200 Loss 3.7838 Accuracy 0.3724\n",
            "Epoch 5 Batch 9250 Loss 3.7840 Accuracy 0.3724\n",
            "Epoch 5 Batch 9300 Loss 3.7839 Accuracy 0.3724\n",
            "Epoch 5 Batch 9350 Loss 3.7837 Accuracy 0.3724\n",
            "Epoch 5 Batch 9400 Loss 3.7838 Accuracy 0.3723\n",
            "Epoch 5 Batch 9450 Loss 3.7835 Accuracy 0.3724\n",
            "Epoch 5 Batch 9500 Loss 3.7834 Accuracy 0.3724\n",
            "Epoch 5 Batch 9550 Loss 3.7832 Accuracy 0.3724\n",
            "Epoch 5 Batch 9600 Loss 3.7831 Accuracy 0.3724\n",
            "Epoch 5 Batch 9650 Loss 3.7832 Accuracy 0.3724\n",
            "Epoch 5 Batch 9700 Loss 3.7830 Accuracy 0.3724\n",
            "Epoch 5 Batch 9750 Loss 3.7829 Accuracy 0.3724\n",
            "Epoch 5 Batch 9800 Loss 3.7830 Accuracy 0.3724\n",
            "Epoch 5 Batch 9850 Loss 3.7830 Accuracy 0.3723\n",
            "Epoch 5 Batch 9900 Loss 3.7824 Accuracy 0.3724\n",
            "Epoch 5 Batch 9950 Loss 3.7821 Accuracy 0.3724\n",
            "Epoch 5 Batch 10000 Loss 3.7818 Accuracy 0.3724\n",
            "Epoch 5 Batch 10050 Loss 3.7818 Accuracy 0.3724\n",
            "Epoch 5 Batch 10100 Loss 3.7815 Accuracy 0.3724\n",
            "Epoch 5 Batch 10150 Loss 3.7813 Accuracy 0.3724\n",
            "Epoch 5 Batch 10200 Loss 3.7809 Accuracy 0.3724\n",
            "Epoch 5 Batch 10250 Loss 3.7806 Accuracy 0.3725\n",
            "Epoch 5 Batch 10300 Loss 3.7800 Accuracy 0.3725\n",
            "Epoch 5 Batch 10350 Loss 3.7798 Accuracy 0.3725\n",
            "Epoch 5 Batch 10400 Loss 3.7796 Accuracy 0.3726\n",
            "Epoch 5 Batch 10450 Loss 3.7792 Accuracy 0.3726\n",
            "Epoch 5 Batch 10500 Loss 3.7788 Accuracy 0.3726\n",
            "Epoch 5 Batch 10550 Loss 3.7785 Accuracy 0.3727\n",
            "Epoch 5 Batch 10600 Loss 3.7784 Accuracy 0.3727\n",
            "Epoch 5 Batch 10650 Loss 3.7783 Accuracy 0.3727\n",
            "Epoch 5 Batch 10700 Loss 3.7782 Accuracy 0.3727\n",
            "Epoch 5 Batch 10750 Loss 3.7778 Accuracy 0.3727\n",
            "Epoch 5 Batch 10800 Loss 3.7774 Accuracy 0.3727\n",
            "Epoch 5 Batch 10850 Loss 3.7768 Accuracy 0.3728\n",
            "Epoch 5 Batch 10900 Loss 3.7760 Accuracy 0.3728\n",
            "Epoch 5 Batch 10950 Loss 3.7755 Accuracy 0.3729\n",
            "Epoch 5 Batch 11000 Loss 3.7751 Accuracy 0.3729\n",
            "Epoch 5 Batch 11050 Loss 3.7746 Accuracy 0.3730\n",
            "Epoch 5 Batch 11100 Loss 3.7743 Accuracy 0.3730\n",
            "Epoch 5 Batch 11150 Loss 3.7742 Accuracy 0.3730\n",
            "Epoch 5 Batch 11200 Loss 3.7737 Accuracy 0.3731\n",
            "Epoch 5 Batch 11250 Loss 3.7732 Accuracy 0.3731\n",
            "Epoch 5 Batch 11300 Loss 3.7727 Accuracy 0.3732\n",
            "Epoch 5 Batch 11350 Loss 3.7722 Accuracy 0.3732\n",
            "Epoch 5 Batch 11400 Loss 3.7718 Accuracy 0.3733\n",
            "Epoch 5 Batch 11450 Loss 3.7714 Accuracy 0.3733\n",
            "Epoch 5 Batch 11500 Loss 3.7711 Accuracy 0.3733\n",
            "Epoch 5 Batch 11550 Loss 3.7707 Accuracy 0.3734\n",
            "Epoch 5 Batch 11600 Loss 3.7704 Accuracy 0.3734\n",
            "Epoch 5 Batch 11650 Loss 3.7701 Accuracy 0.3734\n",
            "Epoch 5 Batch 11700 Loss 3.7698 Accuracy 0.3734\n",
            "Epoch 5 Batch 11750 Loss 3.7697 Accuracy 0.3735\n",
            "Epoch 5 Batch 11800 Loss 3.7696 Accuracy 0.3735\n",
            "Epoch 5 Batch 11850 Loss 3.7695 Accuracy 0.3735\n",
            "Epoch 5 Batch 11900 Loss 3.7691 Accuracy 0.3735\n",
            "Epoch 5 Batch 11950 Loss 3.7687 Accuracy 0.3736\n",
            "Epoch 5 Batch 12000 Loss 3.7682 Accuracy 0.3736\n",
            "Epoch 5 Batch 12050 Loss 3.7677 Accuracy 0.3737\n",
            "Epoch 5 Batch 12100 Loss 3.7674 Accuracy 0.3737\n",
            "Epoch 5 Batch 12150 Loss 3.7671 Accuracy 0.3738\n",
            "Epoch 5 Batch 12200 Loss 3.7669 Accuracy 0.3738\n",
            "Epoch 5 Batch 12250 Loss 3.7664 Accuracy 0.3738\n",
            "Epoch 5 Batch 12300 Loss 3.7657 Accuracy 0.3739\n",
            "Epoch 5 Batch 12350 Loss 3.7653 Accuracy 0.3739\n",
            "Epoch 5 Batch 12400 Loss 3.7648 Accuracy 0.3740\n",
            "Epoch 5 Batch 12450 Loss 3.7646 Accuracy 0.3740\n",
            "Epoch 5 Batch 12500 Loss 3.7644 Accuracy 0.3740\n",
            "Epoch 5 Batch 12550 Loss 3.7640 Accuracy 0.3741\n",
            "Epoch 5 Batch 12600 Loss 3.7635 Accuracy 0.3741\n",
            "Epoch 5 Batch 12650 Loss 3.7627 Accuracy 0.3742\n",
            "Epoch 5 Batch 12700 Loss 3.7619 Accuracy 0.3743\n",
            "Epoch 5 Batch 12750 Loss 3.7612 Accuracy 0.3744\n",
            "Epoch 5 Batch 12800 Loss 3.7605 Accuracy 0.3745\n",
            "Epoch 5 Batch 12850 Loss 3.7596 Accuracy 0.3745\n",
            "Epoch 5 Batch 12900 Loss 3.7589 Accuracy 0.3746\n",
            "Epoch 5 Batch 12950 Loss 3.7582 Accuracy 0.3747\n",
            "Epoch 5 Batch 13000 Loss 3.7576 Accuracy 0.3747\n",
            "Epoch 5 Batch 13050 Loss 3.7569 Accuracy 0.3748\n",
            "Epoch 5 Batch 13100 Loss 3.7564 Accuracy 0.3749\n",
            "Epoch 5 Batch 13150 Loss 3.7559 Accuracy 0.3749\n",
            "Epoch 5 Batch 13200 Loss 3.7557 Accuracy 0.3750\n",
            "Epoch 5 Batch 13250 Loss 3.7554 Accuracy 0.3750\n",
            "Epoch 5 Batch 13300 Loss 3.7552 Accuracy 0.3750\n",
            "Epoch 5 Batch 13350 Loss 3.7547 Accuracy 0.3751\n",
            "Epoch 5 Batch 13400 Loss 3.7545 Accuracy 0.3751\n",
            "Epoch 5 Batch 13450 Loss 3.7544 Accuracy 0.3751\n",
            "Epoch 5 Batch 13500 Loss 3.7542 Accuracy 0.3751\n",
            "Epoch 5 Batch 13550 Loss 3.7542 Accuracy 0.3751\n",
            "Epoch 5 Batch 13600 Loss 3.7540 Accuracy 0.3751\n",
            "Epoch 5 Batch 13650 Loss 3.7536 Accuracy 0.3752\n",
            "Epoch 5 Batch 13700 Loss 3.7532 Accuracy 0.3752\n",
            "Epoch 5 Batch 13750 Loss 3.7525 Accuracy 0.3753\n",
            "Epoch 5 Batch 13800 Loss 3.7519 Accuracy 0.3753\n",
            "Epoch 5 Batch 13850 Loss 3.7511 Accuracy 0.3754\n",
            "Epoch 5 Batch 13900 Loss 3.7506 Accuracy 0.3754\n",
            "Epoch 5 Batch 13950 Loss 3.7499 Accuracy 0.3755\n",
            "Epoch 5 Batch 14000 Loss 3.7491 Accuracy 0.3756\n",
            "Epoch 5 Batch 14050 Loss 3.7483 Accuracy 0.3757\n",
            "Epoch 5 Batch 14100 Loss 3.7478 Accuracy 0.3757\n",
            "Epoch 5 Batch 14150 Loss 3.7472 Accuracy 0.3758\n",
            "Epoch 5 Batch 14200 Loss 3.7468 Accuracy 0.3758\n",
            "Epoch 5 Batch 14250 Loss 3.7466 Accuracy 0.3759\n",
            "Epoch 5 Batch 14300 Loss 3.7462 Accuracy 0.3759\n",
            "Epoch 5 Batch 14350 Loss 3.7457 Accuracy 0.3760\n",
            "Epoch 5 Batch 14400 Loss 3.7453 Accuracy 0.3760\n",
            "Epoch 5 Batch 14450 Loss 3.7450 Accuracy 0.3760\n",
            "Epoch 5 Batch 14500 Loss 3.7445 Accuracy 0.3761\n",
            "Epoch 5 Batch 14550 Loss 3.7442 Accuracy 0.3761\n",
            "Epoch 5 Batch 14600 Loss 3.7436 Accuracy 0.3762\n",
            "Epoch 5 Batch 14650 Loss 3.7434 Accuracy 0.3762\n",
            "Epoch 5 Batch 14700 Loss 3.7434 Accuracy 0.3762\n",
            "Epoch 5 Batch 14750 Loss 3.7432 Accuracy 0.3762\n",
            "Epoch 5 Batch 14800 Loss 3.7429 Accuracy 0.3762\n",
            "Epoch 5 Batch 14850 Loss 3.7428 Accuracy 0.3762\n",
            "Epoch 5 Batch 14900 Loss 3.7426 Accuracy 0.3762\n",
            "Epoch 5 Batch 14950 Loss 3.7422 Accuracy 0.3763\n",
            "Epoch 5 Batch 15000 Loss 3.7421 Accuracy 0.3762\n",
            "Epoch 5 Batch 15050 Loss 3.7418 Accuracy 0.3763\n",
            "Epoch 5 Batch 15100 Loss 3.7415 Accuracy 0.3763\n",
            "Epoch 5 Batch 15150 Loss 3.7412 Accuracy 0.3763\n",
            "Epoch 5 Batch 15200 Loss 3.7410 Accuracy 0.3763\n",
            "Epoch 5 Batch 15250 Loss 3.7410 Accuracy 0.3763\n",
            "Epoch 5 Batch 15300 Loss 3.7407 Accuracy 0.3763\n",
            "Epoch 5 Batch 15350 Loss 3.7404 Accuracy 0.3764\n",
            "Epoch 5 Batch 15400 Loss 3.7402 Accuracy 0.3764\n",
            "Epoch 5 Batch 15450 Loss 3.7399 Accuracy 0.3764\n",
            "Epoch 5 Batch 15500 Loss 3.7396 Accuracy 0.3764\n",
            "Epoch 5 Batch 15550 Loss 3.7392 Accuracy 0.3764\n",
            "Epoch 5 Loss 3.7391 Accuracy 0.3764\n",
            "Time taken for 1 epoch: 3539.0347821712494 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 4.2053 Accuracy 0.3477\n",
            "Epoch 6 Batch 50 Loss 4.0373 Accuracy 0.3678\n",
            "Epoch 6 Batch 100 Loss 3.9995 Accuracy 0.3669\n",
            "Epoch 6 Batch 150 Loss 3.9632 Accuracy 0.3683\n",
            "Epoch 6 Batch 200 Loss 3.9434 Accuracy 0.3685\n",
            "Epoch 6 Batch 250 Loss 3.9174 Accuracy 0.3695\n",
            "Epoch 6 Batch 300 Loss 3.8999 Accuracy 0.3702\n",
            "Epoch 6 Batch 350 Loss 3.8835 Accuracy 0.3707\n",
            "Epoch 6 Batch 400 Loss 3.8692 Accuracy 0.3721\n",
            "Epoch 6 Batch 450 Loss 3.8553 Accuracy 0.3735\n",
            "Epoch 6 Batch 500 Loss 3.8390 Accuracy 0.3748\n",
            "Epoch 6 Batch 550 Loss 3.8228 Accuracy 0.3759\n",
            "Epoch 6 Batch 600 Loss 3.8149 Accuracy 0.3763\n",
            "Epoch 6 Batch 650 Loss 3.8070 Accuracy 0.3774\n",
            "Epoch 6 Batch 700 Loss 3.7970 Accuracy 0.3783\n",
            "Epoch 6 Batch 750 Loss 3.7903 Accuracy 0.3785\n",
            "Epoch 6 Batch 800 Loss 3.7848 Accuracy 0.3788\n",
            "Epoch 6 Batch 850 Loss 3.7807 Accuracy 0.3788\n",
            "Epoch 6 Batch 900 Loss 3.7754 Accuracy 0.3789\n",
            "Epoch 6 Batch 950 Loss 3.7694 Accuracy 0.3795\n",
            "Epoch 6 Batch 1000 Loss 3.7636 Accuracy 0.3801\n",
            "Epoch 6 Batch 1050 Loss 3.7599 Accuracy 0.3803\n",
            "Epoch 6 Batch 1100 Loss 3.7569 Accuracy 0.3807\n",
            "Epoch 6 Batch 1150 Loss 3.7559 Accuracy 0.3808\n",
            "Epoch 6 Batch 1200 Loss 3.7556 Accuracy 0.3805\n",
            "Epoch 6 Batch 1250 Loss 3.7556 Accuracy 0.3806\n",
            "Epoch 6 Batch 1300 Loss 3.7558 Accuracy 0.3806\n",
            "Epoch 6 Batch 1350 Loss 3.7567 Accuracy 0.3805\n",
            "Epoch 6 Batch 1400 Loss 3.7572 Accuracy 0.3804\n",
            "Epoch 6 Batch 1450 Loss 3.7598 Accuracy 0.3802\n",
            "Epoch 6 Batch 1500 Loss 3.7584 Accuracy 0.3802\n",
            "Epoch 6 Batch 1550 Loss 3.7585 Accuracy 0.3800\n",
            "Epoch 6 Batch 1600 Loss 3.7583 Accuracy 0.3799\n",
            "Epoch 6 Batch 1650 Loss 3.7576 Accuracy 0.3797\n",
            "Epoch 6 Batch 1700 Loss 3.7566 Accuracy 0.3796\n",
            "Epoch 6 Batch 1750 Loss 3.7564 Accuracy 0.3796\n",
            "Epoch 6 Batch 1800 Loss 3.7548 Accuracy 0.3797\n",
            "Epoch 6 Batch 1850 Loss 3.7548 Accuracy 0.3797\n",
            "Epoch 6 Batch 1900 Loss 3.7529 Accuracy 0.3798\n",
            "Epoch 6 Batch 1950 Loss 3.7521 Accuracy 0.3798\n",
            "Epoch 6 Batch 2000 Loss 3.7523 Accuracy 0.3797\n",
            "Epoch 6 Batch 2050 Loss 3.7523 Accuracy 0.3797\n",
            "Epoch 6 Batch 2100 Loss 3.7521 Accuracy 0.3798\n",
            "Epoch 6 Batch 2150 Loss 3.7526 Accuracy 0.3797\n",
            "Epoch 6 Batch 2200 Loss 3.7516 Accuracy 0.3799\n",
            "Epoch 6 Batch 2250 Loss 3.7523 Accuracy 0.3797\n",
            "Epoch 6 Batch 2300 Loss 3.7520 Accuracy 0.3798\n",
            "Epoch 6 Batch 2350 Loss 3.7518 Accuracy 0.3799\n",
            "Epoch 6 Batch 2400 Loss 3.7512 Accuracy 0.3799\n",
            "Epoch 6 Batch 2450 Loss 3.7515 Accuracy 0.3799\n",
            "Epoch 6 Batch 2500 Loss 3.7513 Accuracy 0.3799\n",
            "Epoch 6 Batch 2550 Loss 3.7513 Accuracy 0.3799\n",
            "Epoch 6 Batch 2600 Loss 3.7514 Accuracy 0.3799\n",
            "Epoch 6 Batch 2650 Loss 3.7516 Accuracy 0.3799\n",
            "Epoch 6 Batch 2700 Loss 3.7511 Accuracy 0.3798\n",
            "Epoch 6 Batch 2750 Loss 3.7510 Accuracy 0.3798\n",
            "Epoch 6 Batch 2800 Loss 3.7508 Accuracy 0.3797\n",
            "Epoch 6 Batch 2850 Loss 3.7511 Accuracy 0.3797\n",
            "Epoch 6 Batch 2900 Loss 3.7516 Accuracy 0.3796\n",
            "Epoch 6 Batch 2950 Loss 3.7520 Accuracy 0.3796\n",
            "Epoch 6 Batch 3000 Loss 3.7524 Accuracy 0.3795\n",
            "Epoch 6 Batch 3050 Loss 3.7525 Accuracy 0.3793\n",
            "Epoch 6 Batch 3100 Loss 3.7530 Accuracy 0.3793\n",
            "Epoch 6 Batch 3150 Loss 3.7533 Accuracy 0.3793\n",
            "Epoch 6 Batch 3200 Loss 3.7536 Accuracy 0.3792\n",
            "Epoch 6 Batch 3250 Loss 3.7536 Accuracy 0.3791\n",
            "Epoch 6 Batch 3300 Loss 3.7534 Accuracy 0.3791\n",
            "Epoch 6 Batch 3350 Loss 3.7533 Accuracy 0.3790\n",
            "Epoch 6 Batch 3400 Loss 3.7537 Accuracy 0.3790\n",
            "Epoch 6 Batch 3450 Loss 3.7532 Accuracy 0.3790\n",
            "Epoch 6 Batch 3500 Loss 3.7530 Accuracy 0.3790\n",
            "Epoch 6 Batch 3550 Loss 3.7529 Accuracy 0.3789\n",
            "Epoch 6 Batch 3600 Loss 3.7528 Accuracy 0.3789\n",
            "Epoch 6 Batch 3650 Loss 3.7521 Accuracy 0.3788\n",
            "Epoch 6 Batch 3700 Loss 3.7522 Accuracy 0.3788\n",
            "Epoch 6 Batch 3750 Loss 3.7528 Accuracy 0.3787\n",
            "Epoch 6 Batch 3800 Loss 3.7527 Accuracy 0.3786\n",
            "Epoch 6 Batch 3850 Loss 3.7537 Accuracy 0.3785\n",
            "Epoch 6 Batch 3900 Loss 3.7537 Accuracy 0.3784\n",
            "Epoch 6 Batch 3950 Loss 3.7533 Accuracy 0.3784\n",
            "Epoch 6 Batch 4000 Loss 3.7537 Accuracy 0.3784\n",
            "Epoch 6 Batch 4050 Loss 3.7546 Accuracy 0.3782\n",
            "Epoch 6 Batch 4100 Loss 3.7550 Accuracy 0.3781\n",
            "Epoch 6 Batch 4150 Loss 3.7560 Accuracy 0.3780\n",
            "Epoch 6 Batch 4200 Loss 3.7563 Accuracy 0.3779\n",
            "Epoch 6 Batch 4250 Loss 3.7570 Accuracy 0.3777\n",
            "Epoch 6 Batch 4300 Loss 3.7569 Accuracy 0.3777\n",
            "Epoch 6 Batch 4350 Loss 3.7569 Accuracy 0.3776\n",
            "Epoch 6 Batch 4400 Loss 3.7578 Accuracy 0.3775\n",
            "Epoch 6 Batch 4450 Loss 3.7582 Accuracy 0.3774\n",
            "Epoch 6 Batch 4500 Loss 3.7587 Accuracy 0.3773\n",
            "Epoch 6 Batch 4550 Loss 3.7595 Accuracy 0.3772\n",
            "Epoch 6 Batch 4600 Loss 3.7599 Accuracy 0.3772\n",
            "Epoch 6 Batch 4650 Loss 3.7609 Accuracy 0.3771\n",
            "Epoch 6 Batch 4700 Loss 3.7614 Accuracy 0.3770\n",
            "Epoch 6 Batch 4750 Loss 3.7618 Accuracy 0.3769\n",
            "Epoch 6 Batch 4800 Loss 3.7621 Accuracy 0.3769\n",
            "Epoch 6 Batch 4850 Loss 3.7622 Accuracy 0.3769\n",
            "Epoch 6 Batch 4900 Loss 3.7627 Accuracy 0.3769\n",
            "Epoch 6 Batch 4950 Loss 3.7630 Accuracy 0.3768\n",
            "Epoch 6 Batch 5000 Loss 3.7629 Accuracy 0.3768\n",
            "Epoch 6 Batch 5050 Loss 3.7630 Accuracy 0.3767\n",
            "Epoch 6 Batch 5100 Loss 3.7625 Accuracy 0.3767\n",
            "Epoch 6 Batch 5150 Loss 3.7628 Accuracy 0.3766\n",
            "Epoch 6 Batch 5200 Loss 3.7622 Accuracy 0.3766\n",
            "Epoch 6 Batch 5250 Loss 3.7625 Accuracy 0.3766\n",
            "Epoch 6 Batch 5300 Loss 3.7628 Accuracy 0.3766\n",
            "Epoch 6 Batch 5350 Loss 3.7630 Accuracy 0.3765\n",
            "Epoch 6 Batch 5400 Loss 3.7632 Accuracy 0.3764\n",
            "Epoch 6 Batch 5450 Loss 3.7641 Accuracy 0.3764\n",
            "Epoch 6 Batch 5500 Loss 3.7644 Accuracy 0.3763\n",
            "Epoch 6 Batch 5550 Loss 3.7648 Accuracy 0.3763\n",
            "Epoch 6 Batch 5600 Loss 3.7652 Accuracy 0.3762\n",
            "Epoch 6 Batch 5650 Loss 3.7657 Accuracy 0.3761\n",
            "Epoch 6 Batch 5700 Loss 3.7663 Accuracy 0.3760\n",
            "Epoch 6 Batch 5750 Loss 3.7668 Accuracy 0.3760\n",
            "Epoch 6 Batch 5800 Loss 3.7675 Accuracy 0.3759\n",
            "Epoch 6 Batch 5850 Loss 3.7679 Accuracy 0.3758\n",
            "Epoch 6 Batch 5900 Loss 3.7686 Accuracy 0.3757\n",
            "Epoch 6 Batch 5950 Loss 3.7692 Accuracy 0.3757\n",
            "Epoch 6 Batch 6000 Loss 3.7700 Accuracy 0.3756\n",
            "Epoch 6 Batch 6050 Loss 3.7709 Accuracy 0.3755\n",
            "Epoch 6 Batch 6100 Loss 3.7711 Accuracy 0.3754\n",
            "Epoch 6 Batch 6150 Loss 3.7719 Accuracy 0.3753\n",
            "Epoch 6 Batch 6200 Loss 3.7728 Accuracy 0.3752\n",
            "Epoch 6 Batch 6250 Loss 3.7735 Accuracy 0.3750\n",
            "Epoch 6 Batch 6300 Loss 3.7741 Accuracy 0.3749\n",
            "Epoch 6 Batch 6350 Loss 3.7739 Accuracy 0.3749\n",
            "Epoch 6 Batch 6400 Loss 3.7743 Accuracy 0.3748\n",
            "Epoch 6 Batch 6450 Loss 3.7748 Accuracy 0.3747\n",
            "Epoch 6 Batch 6500 Loss 3.7750 Accuracy 0.3746\n",
            "Epoch 6 Batch 6550 Loss 3.7754 Accuracy 0.3746\n",
            "Epoch 6 Batch 6600 Loss 3.7758 Accuracy 0.3745\n",
            "Epoch 6 Batch 6650 Loss 3.7758 Accuracy 0.3744\n",
            "Epoch 6 Batch 6700 Loss 3.7760 Accuracy 0.3743\n",
            "Epoch 6 Batch 6750 Loss 3.7763 Accuracy 0.3742\n",
            "Epoch 6 Batch 6800 Loss 3.7762 Accuracy 0.3742\n",
            "Epoch 6 Batch 6850 Loss 3.7763 Accuracy 0.3741\n",
            "Epoch 6 Batch 6900 Loss 3.7767 Accuracy 0.3740\n",
            "Epoch 6 Batch 6950 Loss 3.7770 Accuracy 0.3739\n",
            "Epoch 6 Batch 7000 Loss 3.7777 Accuracy 0.3738\n",
            "Epoch 6 Batch 7050 Loss 3.7782 Accuracy 0.3738\n",
            "Epoch 6 Batch 7100 Loss 3.7785 Accuracy 0.3737\n",
            "Epoch 6 Batch 7150 Loss 3.7789 Accuracy 0.3737\n",
            "Epoch 6 Batch 7200 Loss 3.7794 Accuracy 0.3736\n",
            "Epoch 6 Batch 7250 Loss 3.7796 Accuracy 0.3735\n",
            "Epoch 6 Batch 7300 Loss 3.7799 Accuracy 0.3735\n",
            "Epoch 6 Batch 7350 Loss 3.7803 Accuracy 0.3734\n",
            "Epoch 6 Batch 7400 Loss 3.7801 Accuracy 0.3734\n",
            "Epoch 6 Batch 7450 Loss 3.7802 Accuracy 0.3734\n",
            "Epoch 6 Batch 7500 Loss 3.7806 Accuracy 0.3733\n",
            "Epoch 6 Batch 7550 Loss 3.7810 Accuracy 0.3732\n",
            "Epoch 6 Batch 7600 Loss 3.7810 Accuracy 0.3732\n",
            "Epoch 6 Batch 7650 Loss 3.7812 Accuracy 0.3731\n",
            "Epoch 6 Batch 7700 Loss 3.7815 Accuracy 0.3731\n",
            "Epoch 6 Batch 7750 Loss 3.7817 Accuracy 0.3731\n",
            "Epoch 6 Batch 7800 Loss 3.7817 Accuracy 0.3730\n",
            "Epoch 6 Batch 7850 Loss 3.7821 Accuracy 0.3730\n",
            "Epoch 6 Batch 7900 Loss 3.7821 Accuracy 0.3730\n",
            "Epoch 6 Batch 7950 Loss 3.7822 Accuracy 0.3729\n",
            "Epoch 6 Batch 8000 Loss 3.7820 Accuracy 0.3729\n",
            "Epoch 6 Batch 8050 Loss 3.7823 Accuracy 0.3729\n",
            "Epoch 6 Batch 8100 Loss 3.7824 Accuracy 0.3729\n",
            "Epoch 6 Batch 8150 Loss 3.7824 Accuracy 0.3728\n",
            "Epoch 6 Batch 8200 Loss 3.7825 Accuracy 0.3728\n",
            "Epoch 6 Batch 8250 Loss 3.7823 Accuracy 0.3728\n",
            "Epoch 6 Batch 8300 Loss 3.7825 Accuracy 0.3727\n",
            "Epoch 6 Batch 8350 Loss 3.7827 Accuracy 0.3727\n",
            "Epoch 6 Batch 8400 Loss 3.7828 Accuracy 0.3727\n",
            "Epoch 6 Batch 8450 Loss 3.7830 Accuracy 0.3726\n",
            "Epoch 6 Batch 8500 Loss 3.7830 Accuracy 0.3726\n",
            "Epoch 6 Batch 8550 Loss 3.7830 Accuracy 0.3726\n",
            "Epoch 6 Batch 8600 Loss 3.7830 Accuracy 0.3726\n",
            "Epoch 6 Batch 8650 Loss 3.7829 Accuracy 0.3725\n",
            "Epoch 6 Batch 8700 Loss 3.7829 Accuracy 0.3726\n",
            "Epoch 6 Batch 8750 Loss 3.7828 Accuracy 0.3725\n",
            "Epoch 6 Batch 8800 Loss 3.7828 Accuracy 0.3725\n",
            "Epoch 6 Batch 8850 Loss 3.7827 Accuracy 0.3725\n",
            "Epoch 6 Batch 8900 Loss 3.7827 Accuracy 0.3725\n",
            "Epoch 6 Batch 8950 Loss 3.7826 Accuracy 0.3725\n",
            "Epoch 6 Batch 9000 Loss 3.7826 Accuracy 0.3725\n",
            "Epoch 6 Batch 9050 Loss 3.7828 Accuracy 0.3724\n",
            "Epoch 6 Batch 9100 Loss 3.7827 Accuracy 0.3724\n",
            "Epoch 6 Batch 9150 Loss 3.7828 Accuracy 0.3724\n",
            "Epoch 6 Batch 9200 Loss 3.7827 Accuracy 0.3724\n",
            "Epoch 6 Batch 9250 Loss 3.7827 Accuracy 0.3723\n",
            "Epoch 6 Batch 9300 Loss 3.7827 Accuracy 0.3723\n",
            "Epoch 6 Batch 9350 Loss 3.7829 Accuracy 0.3723\n",
            "Epoch 6 Batch 9400 Loss 3.7831 Accuracy 0.3723\n",
            "Epoch 6 Batch 9450 Loss 3.7832 Accuracy 0.3723\n",
            "Epoch 6 Batch 9500 Loss 3.7832 Accuracy 0.3722\n",
            "Epoch 6 Batch 9550 Loss 3.7830 Accuracy 0.3722\n",
            "Epoch 6 Batch 9600 Loss 3.7832 Accuracy 0.3722\n",
            "Epoch 6 Batch 9650 Loss 3.7835 Accuracy 0.3721\n",
            "Epoch 6 Batch 9700 Loss 3.7833 Accuracy 0.3721\n",
            "Epoch 6 Batch 9750 Loss 3.7830 Accuracy 0.3722\n",
            "Epoch 6 Batch 9800 Loss 3.7828 Accuracy 0.3721\n",
            "Epoch 6 Batch 9850 Loss 3.7827 Accuracy 0.3721\n",
            "Epoch 6 Batch 9900 Loss 3.7824 Accuracy 0.3722\n",
            "Epoch 6 Batch 9950 Loss 3.7820 Accuracy 0.3722\n",
            "Epoch 6 Batch 10000 Loss 3.7816 Accuracy 0.3722\n",
            "Epoch 6 Batch 10050 Loss 3.7816 Accuracy 0.3722\n",
            "Epoch 6 Batch 10100 Loss 3.7811 Accuracy 0.3722\n",
            "Epoch 6 Batch 10150 Loss 3.7809 Accuracy 0.3723\n",
            "Epoch 6 Batch 10200 Loss 3.7806 Accuracy 0.3723\n",
            "Epoch 6 Batch 10250 Loss 3.7798 Accuracy 0.3724\n",
            "Epoch 6 Batch 10300 Loss 3.7795 Accuracy 0.3724\n",
            "Epoch 6 Batch 10350 Loss 3.7790 Accuracy 0.3725\n",
            "Epoch 6 Batch 10400 Loss 3.7789 Accuracy 0.3725\n",
            "Epoch 6 Batch 10450 Loss 3.7788 Accuracy 0.3725\n",
            "Epoch 6 Batch 10500 Loss 3.7784 Accuracy 0.3725\n",
            "Epoch 6 Batch 10550 Loss 3.7781 Accuracy 0.3726\n",
            "Epoch 6 Batch 10600 Loss 3.7779 Accuracy 0.3726\n",
            "Epoch 6 Batch 10650 Loss 3.7777 Accuracy 0.3726\n",
            "Epoch 6 Batch 10700 Loss 3.7774 Accuracy 0.3726\n",
            "Epoch 6 Batch 10750 Loss 3.7773 Accuracy 0.3726\n",
            "Epoch 6 Batch 10800 Loss 3.7769 Accuracy 0.3727\n",
            "Epoch 6 Batch 10850 Loss 3.7763 Accuracy 0.3727\n",
            "Epoch 6 Batch 10900 Loss 3.7756 Accuracy 0.3727\n",
            "Epoch 6 Batch 10950 Loss 3.7750 Accuracy 0.3728\n",
            "Epoch 6 Batch 11000 Loss 3.7745 Accuracy 0.3728\n",
            "Epoch 6 Batch 11050 Loss 3.7739 Accuracy 0.3729\n",
            "Epoch 6 Batch 11100 Loss 3.7733 Accuracy 0.3730\n",
            "Epoch 6 Batch 11150 Loss 3.7729 Accuracy 0.3730\n",
            "Epoch 6 Batch 11200 Loss 3.7727 Accuracy 0.3730\n",
            "Epoch 6 Batch 11250 Loss 3.7723 Accuracy 0.3731\n",
            "Epoch 6 Batch 11300 Loss 3.7718 Accuracy 0.3731\n",
            "Epoch 6 Batch 11350 Loss 3.7712 Accuracy 0.3732\n",
            "Epoch 6 Batch 11400 Loss 3.7707 Accuracy 0.3732\n",
            "Epoch 6 Batch 11450 Loss 3.7702 Accuracy 0.3732\n",
            "Epoch 6 Batch 11500 Loss 3.7694 Accuracy 0.3733\n",
            "Epoch 6 Batch 11550 Loss 3.7688 Accuracy 0.3734\n",
            "Epoch 6 Batch 11600 Loss 3.7684 Accuracy 0.3734\n",
            "Epoch 6 Batch 11650 Loss 3.7681 Accuracy 0.3734\n",
            "Epoch 6 Batch 11700 Loss 3.7680 Accuracy 0.3734\n",
            "Epoch 6 Batch 11750 Loss 3.7677 Accuracy 0.3735\n",
            "Epoch 6 Batch 11800 Loss 3.7675 Accuracy 0.3735\n",
            "Epoch 6 Batch 11850 Loss 3.7674 Accuracy 0.3735\n",
            "Epoch 6 Batch 11900 Loss 3.7671 Accuracy 0.3736\n",
            "Epoch 6 Batch 11950 Loss 3.7666 Accuracy 0.3736\n",
            "Epoch 6 Batch 12000 Loss 3.7664 Accuracy 0.3737\n",
            "Epoch 6 Batch 12050 Loss 3.7662 Accuracy 0.3737\n",
            "Epoch 6 Batch 12100 Loss 3.7656 Accuracy 0.3737\n",
            "Epoch 6 Batch 12150 Loss 3.7651 Accuracy 0.3738\n",
            "Epoch 6 Batch 12200 Loss 3.7646 Accuracy 0.3738\n",
            "Epoch 6 Batch 12250 Loss 3.7642 Accuracy 0.3738\n",
            "Epoch 6 Batch 12300 Loss 3.7639 Accuracy 0.3739\n",
            "Epoch 6 Batch 12350 Loss 3.7634 Accuracy 0.3739\n",
            "Epoch 6 Batch 12400 Loss 3.7632 Accuracy 0.3739\n",
            "Epoch 6 Batch 12450 Loss 3.7627 Accuracy 0.3740\n",
            "Epoch 6 Batch 12500 Loss 3.7625 Accuracy 0.3740\n",
            "Epoch 6 Batch 12550 Loss 3.7621 Accuracy 0.3740\n",
            "Epoch 6 Batch 12600 Loss 3.7617 Accuracy 0.3741\n",
            "Epoch 6 Batch 12650 Loss 3.7611 Accuracy 0.3741\n",
            "Epoch 6 Batch 12700 Loss 3.7605 Accuracy 0.3742\n",
            "Epoch 6 Batch 12750 Loss 3.7598 Accuracy 0.3743\n",
            "Epoch 6 Batch 12800 Loss 3.7591 Accuracy 0.3744\n",
            "Epoch 6 Batch 12850 Loss 3.7582 Accuracy 0.3745\n",
            "Epoch 6 Batch 12900 Loss 3.7571 Accuracy 0.3746\n",
            "Epoch 6 Batch 12950 Loss 3.7564 Accuracy 0.3746\n",
            "Epoch 6 Batch 13000 Loss 3.7557 Accuracy 0.3747\n",
            "Epoch 6 Batch 13050 Loss 3.7552 Accuracy 0.3748\n",
            "Epoch 6 Batch 13100 Loss 3.7545 Accuracy 0.3748\n",
            "Epoch 6 Batch 13150 Loss 3.7540 Accuracy 0.3749\n",
            "Epoch 6 Batch 13200 Loss 3.7537 Accuracy 0.3749\n",
            "Epoch 6 Batch 13250 Loss 3.7534 Accuracy 0.3750\n",
            "Epoch 6 Batch 13300 Loss 3.7531 Accuracy 0.3750\n",
            "Epoch 6 Batch 13350 Loss 3.7528 Accuracy 0.3750\n",
            "Epoch 6 Batch 13400 Loss 3.7527 Accuracy 0.3751\n",
            "Epoch 6 Batch 13450 Loss 3.7525 Accuracy 0.3751\n",
            "Epoch 6 Batch 13500 Loss 3.7524 Accuracy 0.3751\n",
            "Epoch 6 Batch 13550 Loss 3.7523 Accuracy 0.3751\n",
            "Epoch 6 Batch 13600 Loss 3.7521 Accuracy 0.3752\n",
            "Epoch 6 Batch 13650 Loss 3.7518 Accuracy 0.3752\n",
            "Epoch 6 Batch 13700 Loss 3.7513 Accuracy 0.3752\n",
            "Epoch 6 Batch 13750 Loss 3.7507 Accuracy 0.3753\n",
            "Epoch 6 Batch 13800 Loss 3.7501 Accuracy 0.3754\n",
            "Epoch 6 Batch 13850 Loss 3.7495 Accuracy 0.3754\n",
            "Epoch 6 Batch 13900 Loss 3.7488 Accuracy 0.3755\n",
            "Epoch 6 Batch 13950 Loss 3.7483 Accuracy 0.3756\n",
            "Epoch 6 Batch 14000 Loss 3.7477 Accuracy 0.3756\n",
            "Epoch 6 Batch 14050 Loss 3.7470 Accuracy 0.3757\n",
            "Epoch 6 Batch 14100 Loss 3.7463 Accuracy 0.3758\n",
            "Epoch 6 Batch 14150 Loss 3.7458 Accuracy 0.3758\n",
            "Epoch 6 Batch 14200 Loss 3.7454 Accuracy 0.3759\n",
            "Epoch 6 Batch 14250 Loss 3.7450 Accuracy 0.3759\n",
            "Epoch 6 Batch 14300 Loss 3.7447 Accuracy 0.3759\n",
            "Epoch 6 Batch 14350 Loss 3.7443 Accuracy 0.3760\n",
            "Epoch 6 Batch 14400 Loss 3.7440 Accuracy 0.3760\n",
            "Epoch 6 Batch 14450 Loss 3.7437 Accuracy 0.3760\n",
            "Epoch 6 Batch 14500 Loss 3.7434 Accuracy 0.3760\n",
            "Epoch 6 Batch 14550 Loss 3.7430 Accuracy 0.3761\n",
            "Epoch 6 Batch 14600 Loss 3.7427 Accuracy 0.3761\n",
            "Epoch 6 Batch 14650 Loss 3.7423 Accuracy 0.3761\n",
            "Epoch 6 Batch 14700 Loss 3.7419 Accuracy 0.3762\n",
            "Epoch 6 Batch 14750 Loss 3.7413 Accuracy 0.3762\n",
            "Epoch 6 Batch 14800 Loss 3.7411 Accuracy 0.3762\n",
            "Epoch 6 Batch 14850 Loss 3.7410 Accuracy 0.3762\n",
            "Epoch 6 Batch 14900 Loss 3.7407 Accuracy 0.3762\n",
            "Epoch 6 Batch 14950 Loss 3.7406 Accuracy 0.3763\n",
            "Epoch 6 Batch 15000 Loss 3.7404 Accuracy 0.3763\n",
            "Epoch 6 Batch 15050 Loss 3.7402 Accuracy 0.3763\n",
            "Epoch 6 Batch 15100 Loss 3.7399 Accuracy 0.3763\n",
            "Epoch 6 Batch 15150 Loss 3.7397 Accuracy 0.3763\n",
            "Epoch 6 Batch 15200 Loss 3.7396 Accuracy 0.3763\n",
            "Epoch 6 Batch 15250 Loss 3.7395 Accuracy 0.3763\n",
            "Epoch 6 Batch 15300 Loss 3.7393 Accuracy 0.3763\n",
            "Epoch 6 Batch 15350 Loss 3.7392 Accuracy 0.3763\n",
            "Epoch 6 Batch 15400 Loss 3.7390 Accuracy 0.3764\n",
            "Epoch 6 Batch 15450 Loss 3.7388 Accuracy 0.3764\n",
            "Epoch 6 Batch 15500 Loss 3.7384 Accuracy 0.3764\n",
            "Epoch 6 Batch 15550 Loss 3.7380 Accuracy 0.3764\n",
            "Saving checkpoint for epoch 6 at /content/gdrive/My Drive/checkpoints/ckpt-18\n",
            "Epoch 6 Loss 3.7379 Accuracy 0.3764\n",
            "Time taken for 1 epoch: 3521.8306362628937 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 4.3331 Accuracy 0.3516\n",
            "Epoch 7 Batch 50 Loss 4.0148 Accuracy 0.3650\n",
            "Epoch 7 Batch 100 Loss 4.0188 Accuracy 0.3644\n",
            "Epoch 7 Batch 150 Loss 3.9717 Accuracy 0.3675\n",
            "Epoch 7 Batch 200 Loss 3.9394 Accuracy 0.3695\n",
            "Epoch 7 Batch 250 Loss 3.9172 Accuracy 0.3701\n",
            "Epoch 7 Batch 300 Loss 3.8928 Accuracy 0.3719\n",
            "Epoch 7 Batch 350 Loss 3.8728 Accuracy 0.3729\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-ee79102a1a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# inp -> portuguese, tar -> english\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4ojhkiRfXFh"
      },
      "source": [
        "def evaluate(inp_sentence):\r\n",
        "  start_token = [tokenizer_text.vocab_size]\r\n",
        "  end_token = [tokenizer_text.vocab_size + 1]\r\n",
        "\r\n",
        "  # inp sentence is portuguese, hence adding the start and end token\r\n",
        "  inp_sentence = start_token + tokenizer_text.encode(inp_sentence) + end_token\r\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\r\n",
        "\r\n",
        "  # as the target is english, the first word to the transformer should be the\r\n",
        "  # english start token.\r\n",
        "  decoder_input = [tokenizer_summary.vocab_size]\r\n",
        "  output = tf.expand_dims(decoder_input, 0)\r\n",
        "\r\n",
        "  for i in range(MAX_LENGTH_SUM):\r\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\r\n",
        "        encoder_input, output)\r\n",
        "\r\n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\r\n",
        "    predictions, attention_weights = transformer(encoder_input, \r\n",
        "                                                 output,\r\n",
        "                                                 False,\r\n",
        "                                                 enc_padding_mask,\r\n",
        "                                                 combined_mask,\r\n",
        "                                                 dec_padding_mask)\r\n",
        "\r\n",
        "    # select the last word from the seq_len dimension\r\n",
        "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\r\n",
        "\r\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\r\n",
        "\r\n",
        "    # return the result if the predicted_id is equal to the end token\r\n",
        "    if predicted_id == tokenizer_summary.vocab_size+1:\r\n",
        "      return tf.squeeze(output, axis=0), attention_weights\r\n",
        "\r\n",
        "    # concatentate the predicted_id to the output which is given to the decoder\r\n",
        "    # as its input.\r\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\r\n",
        "\r\n",
        "  return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP8pAJhvfxeU"
      },
      "source": [
        "def translate(sentence, plot=''):\r\n",
        "  result, attention_weights = evaluate(sentence)\r\n",
        "\r\n",
        "  predicted_sentence = tokenizer_summary.decode([i for i in result \r\n",
        "                                            if i < tokenizer_summary.vocab_size])  \r\n",
        "\r\n",
        "  return predicted_sentence\r\n",
        "\r\n",
        "  if plot:\r\n",
        "    plot_attention_weights(attention_weights, sentence, result, plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QMIPp-RfyXV"
      },
      "source": [
        "load_data = pd.read_csv('/content/gdrive/My Drive/proc_reviews.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uIcTX357SH9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(load_data['text'], load_data['summary'], test_size=0.0012, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9IdqfYP-8zH"
      },
      "source": [
        "result = list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcZ_2pK8eih7",
        "outputId": "49de69f0-c88d-4595-d587-c56a044db958"
      },
      "source": [
        "i=0\r\n",
        "for sent in X_test:\r\n",
        "  result.append(translate(sent))\r\n",
        "  i+=1\r\n",
        "  if i%10 == 0:\r\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "110\n",
            "120\n",
            "130\n",
            "140\n",
            "150\n",
            "160\n",
            "170\n",
            "180\n",
            "190\n",
            "200\n",
            "210\n",
            "220\n",
            "230\n",
            "240\n",
            "250\n",
            "260\n",
            "270\n",
            "280\n",
            "290\n",
            "300\n",
            "310\n",
            "320\n",
            "330\n",
            "340\n",
            "350\n",
            "360\n",
            "370\n",
            "380\n",
            "390\n",
            "400\n",
            "410\n",
            "420\n",
            "430\n",
            "440\n",
            "450\n",
            "460\n",
            "470\n",
            "480\n",
            "490\n",
            "500\n",
            "510\n",
            "520\n",
            "530\n",
            "540\n",
            "550\n",
            "560\n",
            "570\n",
            "580\n",
            "590\n",
            "600\n",
            "610\n",
            "620\n",
            "630\n",
            "640\n",
            "650\n",
            "660\n",
            "670\n",
            "680\n",
            "690\n",
            "700\n",
            "710\n",
            "720\n",
            "730\n",
            "740\n",
            "750\n",
            "760\n",
            "770\n",
            "780\n",
            "790\n",
            "800\n",
            "810\n",
            "820\n",
            "830\n",
            "840\n",
            "850\n",
            "860\n",
            "870\n",
            "880\n",
            "890\n",
            "900\n",
            "910\n",
            "920\n",
            "930\n",
            "940\n",
            "950\n",
            "960\n",
            "970\n",
            "980\n",
            "990\n",
            "1000\n",
            "1010\n",
            "1020\n",
            "1030\n",
            "1040\n",
            "1050\n",
            "1060\n",
            "1070\n",
            "1080\n",
            "1090\n",
            "1100\n",
            "1110\n",
            "1120\n",
            "1130\n",
            "1140\n",
            "1150\n",
            "1160\n",
            "1170\n",
            "1180\n",
            "1190\n",
            "1200\n",
            "1210\n",
            "1220\n",
            "1230\n",
            "1240\n",
            "1250\n",
            "1260\n",
            "1270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8pTNddRPDXr"
      },
      "source": [
        "sentence = list() \r\n",
        "original_summary = list()\r\n",
        "for sen in X_test:\r\n",
        "  sentence.append(sen)\r\n",
        "\r\n",
        "for sen in y_test:\r\n",
        "  original_summary.append(sen)\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRXmDJdfPnFR",
        "outputId": "2bfd3199-d0c8-49ad-e364-e27dad62beb5"
      },
      "source": [
        "print(sentence[12])\r\n",
        "print(original_summary[12])\r\n",
        "print(result[12])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sostoken> i have seen most of these short films , as one appears at the beginning of just about every pixar film . having them all in one place is a treat , plus i can never remember which pixar film has which short , so if i want to see one now i can just pop this dvd in . the progression through the years is interesting , too watching how pixar got better with their art was very informative . <eostoken>\n",
            "<sostoken> great shorts <eostoken>\n",
            "not the best animated film of the year\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "Jtw7hfSxRKVH",
        "outputId": "a8830cea-083b-4658-e1b5-c7bce5d10c98"
      },
      "source": [
        "dataf = pd.DataFrame({'sentence':sentence , 'original_summary' : original_summary , 'result' : result})\r\n",
        "dataf.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>original_summary</th>\n",
              "      <th>result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;sostoken&gt; a incredible well scripted movie th...</td>\n",
              "      <td>&lt;sostoken&gt; cause and effect &lt;eostoken&gt;</td>\n",
              "      <td>best animated movie ever</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;sostoken&gt; . starsthis really is a classic fil...</td>\n",
              "      <td>&lt;sostoken&gt; bill at his best &lt;eostoken&gt;</td>\n",
              "      <td>a great film for the whole family</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;sostoken&gt; i liked the first one and liked thi...</td>\n",
              "      <td>&lt;sostoken&gt; very funny &lt;eostoken&gt;</td>\n",
              "      <td>not as good as the first one</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;sostoken&gt; cannot wait for the nd season to co...</td>\n",
              "      <td>&lt;sostoken&gt; spent my sunday watching was not a ...</td>\n",
              "      <td>great show</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;sostoken&gt; if you like delta force like action...</td>\n",
              "      <td>&lt;sostoken&gt; unit watchers , unite ! &lt;eostoken&gt;</td>\n",
              "      <td>great series</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  ...                             result\n",
              "0  <sostoken> a incredible well scripted movie th...  ...           best animated movie ever\n",
              "1  <sostoken> . starsthis really is a classic fil...  ...  a great film for the whole family\n",
              "2  <sostoken> i liked the first one and liked thi...  ...       not as good as the first one\n",
              "3  <sostoken> cannot wait for the nd season to co...  ...                         great show\n",
              "4  <sostoken> if you like delta force like action...  ...                       great series\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDZbD06lR8Kb"
      },
      "source": [
        "dataf.to_csv('/content/gdrive/My Drive/result.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}